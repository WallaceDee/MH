# ğŸŒ ä»£ç†IPè½®æ¢ç³»ç»Ÿä½¿ç”¨æŒ‡å—

## ğŸ“– æ¦‚è¿°

ä»£ç†IPè½®æ¢ç³»ç»Ÿæ˜¯ä¸€å¥—å®Œæ•´çš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºæå‡çˆ¬è™«çš„æ€§èƒ½å’Œç¨³å®šæ€§ï¼Œé¿å…IPè¢«å°é”ã€‚ç³»ç»ŸåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ï¼š

1. **`proxy_rotation_system.py`** - ä»£ç†IPè½®æ¢æ ¸å¿ƒç³»ç»Ÿ
2. **`proxy_source_manager.py`** - å…è´¹ä»£ç†IPæºç®¡ç†å™¨  
3. **`cbg_crawler_with_proxy.py`** - å®Œæ•´çš„CBGçˆ¬è™«é›†æˆç¤ºä¾‹

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ³•ä¸€ï¼šä½¿ç”¨å…è´¹ä»£ç†ï¼ˆæ¨èæ–°æ‰‹ï¼‰

```bash
# 1. è‡ªåŠ¨è·å–å…è´¹ä»£ç†IP
python3 proxy_source_manager.py

# 2. æµ‹è¯•ä»£ç†å¯ç”¨æ€§
python3 proxy_rotation_system.py

# 3. è¿è¡Œé›†æˆçˆ¬è™«
python3 cbg_crawler_with_proxy.py
```

### æ–¹æ³•äºŒï¼šä½¿ç”¨ä»˜è´¹ä»£ç†

1. åˆ›å»º `proxy_list.txt` æ–‡ä»¶ï¼Œæ·»åŠ ä½ çš„ä»£ç†IPï¼š
```
# æ”¯æŒå¤šç§æ ¼å¼
http://127.0.0.1:8080
https://username:password@proxy.example.com:3128
socks5://127.0.0.1:1080
192.168.1.100:8080:username:password
```

2. è¿è¡Œæµ‹è¯•ï¼š
```bash
python3 proxy_rotation_system.py
```

## ğŸ“‹ è¯¦ç»†åŠŸèƒ½ä»‹ç»

### 1. ä»£ç†IPè½®æ¢ç®¡ç†å™¨ (`ProxyRotationManager`)

#### æ ¸å¿ƒåŠŸèƒ½
- âœ… **å¤šæ ¼å¼ä»£ç†æ”¯æŒ**: HTTP/HTTPS/SOCKS5
- âœ… **æ™ºèƒ½è½®æ¢ç­–ç•¥**: åŸºäºæˆåŠŸç‡å’Œå“åº”æ—¶é—´
- âœ… **è‡ªåŠ¨æ•…éšœæ£€æµ‹**: è‡ªåŠ¨æ ‡è®°å¤±è´¥ä»£ç†
- âœ… **ç»Ÿè®¡åˆ†æ**: è¯¦ç»†çš„ä½¿ç”¨ç»Ÿè®¡å’Œæ€§èƒ½ç›‘æ§
- âœ… **çº¿ç¨‹å®‰å…¨**: æ”¯æŒå¤šçº¿ç¨‹å¹¶å‘ä½¿ç”¨

#### æ”¯æŒçš„ä»£ç†æ ¼å¼
```python
# ç®€å•æ ¼å¼
"127.0.0.1:8080"

# å¸¦åè®®
"http://127.0.0.1:8080"
"https://proxy.example.com:3128" 
"socks5://127.0.0.1:1080"

# å¸¦è®¤è¯
"http://username:password@proxy.example.com:8080"

# å››æ®µæ ¼å¼
"127.0.0.1:8080:username:password"
```

#### ä»£ç†é€‰æ‹©ç­–ç•¥
```python
# ç­–ç•¥1: æ–°ä»£ç†éšæœºé€‰æ‹©
# ç­–ç•¥2: åŸºäºæˆåŠŸç‡(70%) + å“åº”æ—¶é—´(30%)çš„æƒé‡é€‰æ‹©
score = success_rate * 0.7 + time_weight * 0.3
```

### 2. å…è´¹ä»£ç†æºç®¡ç†å™¨ (`ProxySourceManager`)

#### æ”¯æŒçš„å…è´¹ä»£ç†æº
- ğŸŒ **free-proxy-list.net** - é«˜åŒ¿åä»£ç†
- ğŸŒ **proxy-list.download** - HTTP/HTTPSä»£ç†
- ğŸŒ **pubproxy.com** - APIæ¥å£ä»£ç†
- ğŸŒ **gimmeproxy.com** - å•ä¸ªä»£ç†API

#### è‡ªåŠ¨è·å–æµç¨‹
```python
# 1. ä»å¤šä¸ªæºå¹¶è¡Œè·å–
# 2. å»é‡å¤„ç†
# 3. ä¿å­˜åˆ°proxy_list.txt
# 4. æŒ‰æ¥æºç»Ÿè®¡
```

### 3. CBGçˆ¬è™«é›†æˆ (`CBGProxyCrawler`)

#### æ ¸å¿ƒç‰¹æ€§
- ğŸ”„ **è‡ªåŠ¨é‡è¯•æœºåˆ¶**: ä»£ç†å¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢
- ğŸ“Š **æ€§èƒ½ç›‘æ§**: è®°å½•æ¯ä¸ªä»£ç†çš„å“åº”æ—¶é—´
- ğŸš« **åçˆ¬è™«æ£€æµ‹**: æ™ºèƒ½è¯†åˆ«è¢«å°é”çš„ä»£ç†
- âš¡ **å¹¶è¡Œçˆ¬å–**: æ”¯æŒå¤šçº¿ç¨‹å¹¶å‘

## ğŸ›  é«˜çº§é…ç½®

### è‡ªå®šä¹‰ä»£ç†é€‰æ‹©ç­–ç•¥

```python
class CustomProxyManager(ProxyRotationManager):
    def _select_best_proxy(self, proxies):
        # è‡ªå®šä¹‰é€‰æ‹©é€»è¾‘
        # ä¾‹å¦‚ï¼šä¼˜å…ˆé€‰æ‹©ç‰¹å®šåœ°åŒºçš„ä»£ç†
        china_proxies = [p for p in proxies if p.get('country') == 'China']
        if china_proxies:
            return random.choice(china_proxies)
        return super()._select_best_proxy(proxies)
```

### é…ç½®å¹¶å‘çˆ¬å–

```python
# è°ƒæ•´æœ€å¤§å¹¶å‘æ•°
proxy_manager = ProxyRotationManager(max_workers=5)

# è®¾ç½®è¯·æ±‚é—´éš”
crawler.parallel_crawl_with_proxy(1, 50, delay_range=(2, 5))
```

### ä»£ç†æ± å¥åº·ç›‘æ§

```python
# å®šæœŸæ¸…ç†å¤±è´¥ä»£ç†
def cleanup_failed_proxies():
    active_count = len([p for p in proxy_manager.proxy_pool if p['status'] == 'active'])
    if active_count < 3:  # å¯ç”¨ä»£ç†å°‘äº3ä¸ªæ—¶
        # é‡æ–°è·å–ä»£ç†
        source_manager = ProxySourceManager()
        new_proxies = source_manager.get_proxies_from_all_sources()
        # æ›´æ–°ä»£ç†æ± ...
```

## âš¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. ä»£ç†æ± ç®¡ç†
```python
# å®šæœŸæ›´æ–°ä»£ç†æ± ï¼ˆæ¯4-6å°æ—¶ï¼‰
# ä¿æŒè‡³å°‘10-20ä¸ªå¯ç”¨ä»£ç†
# åŠæ—¶æ¸…ç†å¤±è´¥ç‡>50%çš„ä»£ç†
```

### 2. è¯·æ±‚ç­–ç•¥
```python
# åˆç†è®¾ç½®è¯·æ±‚é—´éš”
delay_range = (1, 3)  # 1-3ç§’éšæœºé—´éš”

# é”™å³°çˆ¬å–ï¼ˆé¿å¼€ç½‘ç«™é«˜å³°æœŸï¼‰
best_hours = [2, 3, 4, 5, 6]  # å‡Œæ™¨æ—¶æ®µ

# åˆ†æ‰¹å¤„ç†å¤§é‡æ•°æ®
batch_size = 10  # æ¯æ‰¹10é¡µ
```

### 3. ååçˆ¬è™«
```python
# éšæœºUser-Agent
user_agents = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) ...',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) ...',
    # ... æ›´å¤šUA
]

# æ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸º
time.sleep(random.uniform(1, 5))  # éšæœºåœç•™æ—¶é—´
```

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. æ‰€æœ‰ä»£ç†éƒ½ä¸å¯ç”¨
```bash
# è§£å†³æ–¹æ¡ˆï¼š
python3 proxy_source_manager.py  # é‡æ–°è·å–å…è´¹ä»£ç†
# æˆ–æ‰‹åŠ¨æ·»åŠ ä»˜è´¹ä»£ç†åˆ°proxy_list.txt
```

#### 2. ä»£ç†å“åº”æ…¢
```python
# è°ƒæ•´è¶…æ—¶è®¾ç½®
proxy_manager.test_proxy(proxy, timeout=15)  # å¢åŠ åˆ°15ç§’

# è¿‡æ»¤æ…¢é€Ÿä»£ç†
fast_proxies = [p for p in proxies if p['response_time'] < 5.0]
```

#### 3. è¢«ç›®æ ‡ç½‘ç«™æ£€æµ‹
```python
# å¢åŠ è¯·æ±‚é—´éš”
time.sleep(random.uniform(3, 8))

# å‡å°‘å¹¶å‘æ•°
max_workers = 2

# ä½¿ç”¨é«˜åŒ¿åä»£ç†
anonymous_proxies = [p for p in proxies if p.get('anonymity') == 'high anonymity']
```

### è°ƒè¯•æŠ€å·§

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
logging.getLogger().setLevel(logging.DEBUG)

# æŸ¥çœ‹ä»£ç†ç»Ÿè®¡
proxy_manager.show_proxy_status()

# æµ‹è¯•å•ä¸ªä»£ç†
proxy_manager.test_proxy(proxy, test_url="http://httpbin.org/ip")
```

## ğŸ“Š ç›‘æ§ä¸ç»Ÿè®¡

### å®æ—¶ç›‘æ§æŒ‡æ ‡
- ğŸ“ˆ **æˆåŠŸç‡**: æ¯ä¸ªä»£ç†çš„è¯·æ±‚æˆåŠŸç‡
- â±ï¸ **å“åº”æ—¶é—´**: å¹³å‡å“åº”æ—¶é—´ç»Ÿè®¡
- ğŸ”„ **è½®æ¢é¢‘æ¬¡**: ä»£ç†ä½¿ç”¨é¢‘ç‡åˆ†å¸ƒ
- âŒ **å¤±è´¥åŸå› **: è¶…æ—¶ã€è¿æ¥é”™è¯¯ã€HTTPçŠ¶æ€ç ç»Ÿè®¡

### æ€§èƒ½æŠ¥å‘Š
```python
# ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
stats = proxy_manager.get_proxy_stats()
print(f"æ€»ä»£ç†æ•°: {stats['total_proxies']}")
print(f"å¯ç”¨ä»£ç†: {stats['active_proxies']}")
print(f"å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}ç§’")
```

## ğŸš¨ å®‰å…¨æ³¨æ„äº‹é¡¹

### 1. ä»£ç†å®‰å…¨
- âŒ é¿å…ä½¿ç”¨æ¥æºä¸æ˜çš„ä»£ç†
- âœ… å®šæœŸæ›´æ¢ä»£ç†IP
- âœ… ç›‘æ§ä»£ç†çš„ç½‘ç»œæµé‡
- âœ… ä½¿ç”¨HTTPSä»£ç†å¤„ç†æ•æ„Ÿæ•°æ®

### 2. çˆ¬è™«åˆè§„
- âœ… éµå®ˆrobots.txtè§„åˆ™
- âœ… è®¾ç½®åˆç†çš„è¯·æ±‚é¢‘ç‡
- âœ… é¿å…å¯¹ç›®æ ‡æœåŠ¡å™¨é€ æˆè´Ÿè½½å‹åŠ›
- âœ… å°Šé‡ç½‘ç«™çš„æœåŠ¡æ¡æ¬¾

### 3. æ•°æ®ä¿æŠ¤
- âœ… åŠ å¯†å­˜å‚¨ä»£ç†å‡­æ®
- âœ… å®šæœŸæ¸…ç†æ—¥å¿—æ–‡ä»¶
- âœ… é¿å…åœ¨ä»£ç ä¸­ç¡¬ç¼–ç æ•æ„Ÿä¿¡æ¯

## ğŸ“ˆ æ‰©å±•åº”ç”¨

### 1. å…¶ä»–ç½‘ç«™çˆ¬è™«
```python
# è½»æ¾é€‚é…åˆ°å…¶ä»–ç½‘ç«™
class CustomWebCrawler(CBGProxyCrawler):
    def __init__(self, proxy_manager, target_url):
        super().__init__(proxy_manager)
        self.target_url = target_url
    
    def fetch_page_data(self, page):
        # è‡ªå®šä¹‰é¡µé¢è·å–é€»è¾‘
        pass
```

### 2. APIè¯·æ±‚ä»£ç†
```python
# ä¸ºAPIè¯·æ±‚æ·»åŠ ä»£ç†æ”¯æŒ
def api_request_with_proxy(url, data, proxy_manager):
    proxy = proxy_manager.get_next_proxy()
    session = create_session_with_proxy(proxy)
    return session.post(url, json=data)
```

### 3. å®šæ—¶ä»»åŠ¡é›†æˆ
```python
# ç»“åˆcrontabå®ç°å®šæ—¶çˆ¬å–
# 0 */6 * * * cd /path/to/project && python3 cbg_crawler_with_proxy.py
```

## ğŸ¯ æœ€ä½³å®è·µæ€»ç»“

1. **ä»£ç†æ± ç»´æŠ¤**ï¼šä¿æŒè¶³å¤Ÿçš„ä»£ç†æ•°é‡ï¼Œå®šæœŸæ›´æ–°
2. **æ™ºèƒ½é‡è¯•**ï¼šå¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢ä»£ç†ï¼Œé¿å…æ— è°“çš„é‡è¯•
3. **æ€§èƒ½ç›‘æ§**ï¼šå¯†åˆ‡å…³æ³¨æˆåŠŸç‡å’Œå“åº”æ—¶é—´
4. **è¯·æ±‚æ§åˆ¶**ï¼šåˆç†æ§åˆ¶å¹¶å‘æ•°å’Œè¯·æ±‚é¢‘ç‡
5. **é”™è¯¯å¤„ç†**ï¼šå®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œæ—¥å¿—è®°å½•
6. **å®‰å…¨åˆè§„**ï¼šéµå®ˆç›¸å…³æ³•å¾‹æ³•è§„å’Œç½‘ç«™è§„åˆ™

---

## ğŸ†˜ æŠ€æœ¯æ”¯æŒ

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š
1. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸
2. ä»£ç†IPæ˜¯å¦æœ‰æ•ˆ
3. ç›®æ ‡ç½‘ç«™æ˜¯å¦æœ‰åçˆ¬è™«æªæ–½
4. Pythonä¾èµ–åŒ…æ˜¯å¦å®Œæ•´å®‰è£…

ç³»ç»Ÿè®¾è®¡çµæ´»ï¼Œå¯æ ¹æ®å…·ä½“éœ€æ±‚è¿›è¡Œå®šåˆ¶å’Œæ‰©å±•ã€‚ 