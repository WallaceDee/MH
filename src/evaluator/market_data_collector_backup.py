import sys
import os
import threading
import concurrent.futures

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œè§£å†³æ¨¡å—å¯¼å…¥é—®é¢˜
from src.utils.project_path import get_project_root
project_root = get_project_root()
sys.path.insert(0, project_root)

import pandas as pd
import numpy as np
import json
import logging
import time
import hashlib
from typing import Dict, Any, List, Optional, Union
from datetime import datetime, timedelta
from flask import current_app

try:
    from .feature_extractor.feature_extractor import FeatureExtractor
except ImportError:
    from src.evaluator.feature_extractor.feature_extractor import FeatureExtractor


class MarketDataCollector:
    """å¸‚åœºæ•°æ®é‡‡é›†å™¨ - ä»MySQLæ•°æ®åº“è·å–ç©ºè§’è‰²æ•°æ®ä½œä¸ºé”šç‚¹ï¼Œæ”¯æŒå•ä¾‹æ¨¡å¼çš„æ•°æ®ç¼“å­˜å…±äº«"""
    
    _instance = None  # å•ä¾‹å®ä¾‹
    _lock = threading.Lock()  # çº¿ç¨‹é”ï¼Œç¡®ä¿çº¿ç¨‹å®‰å…¨
    
    def __new__(cls):
        """å•ä¾‹æ¨¡å¼å®ç°"""
        with cls._lock:
            if cls._instance is None:
                instance = super(MarketDataCollector, cls).__new__(cls)
                cls._instance = instance
                # æ ‡è®°å®ä¾‹æ˜¯å¦å·²åˆå§‹åŒ–ï¼Œé¿å…é‡å¤åˆå§‹åŒ–
                instance._initialized = False
                print("åˆ›å»ºæ–°çš„ MarketDataCollector å•ä¾‹å®ä¾‹")
            else:
                print("ä½¿ç”¨ç°æœ‰çš„ MarketDataCollector å•ä¾‹å®ä¾‹")
            
            return cls._instance
    
    def __init__(self):
        """åˆå§‹åŒ–å¸‚åœºæ•°æ®é‡‡é›†å™¨ï¼ˆå•ä¾‹æ¨¡å¼ä¸‹åªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰"""
        # é¿å…é‡å¤åˆå§‹åŒ–
        if hasattr(self, '_initialized') and self._initialized:
            return
        
        self.logger = logging.getLogger(__name__)
        self.feature_extractor = FeatureExtractor()
        self.market_data = pd.DataFrame()
        
        # æ•°æ®ç¼“å­˜ç›¸å…³å±æ€§
        self._data_loaded = False
        self._last_refresh_time = None
        self._cache_expiry_hours = 24  # ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆå°æ—¶ï¼‰
        
        # Flask-Caching ç¼“å­˜å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self._cache = None
        
        # è¿›åº¦è·Ÿè¸ªç›¸å…³å±æ€§
        self._refresh_status = "idle"  # idle, running, completed, error
        self._refresh_progress = 0  # 0-100
        self._refresh_message = ""
        self._refresh_start_time = None
        self._refresh_total_records = 0
        self._refresh_processed_records = 0
        self._refresh_current_batch = 0
        self._refresh_total_batches = 0
        
        self._initialized = True
        print("å¸‚åœºæ•°æ®é‡‡é›†å™¨å•ä¾‹åˆå§‹åŒ–å®Œæˆï¼Œé»˜è®¤è·å–ç©ºè§’è‰²æ•°æ®ä½œä¸ºé”šç‚¹")
    
    def _get_cache(self):
        """è·å–Flask-Cachingå®ä¾‹"""
        if self._cache is None:
            try:
                # è·å–å½“å‰åº”ç”¨çš„ç¼“å­˜å®ä¾‹
                from flask_caching import Cache
                
                # æ£€æŸ¥æ˜¯å¦åœ¨åº”ç”¨ä¸Šä¸‹æ–‡ä¸­
                if current_app:
                    # ä»åº”ç”¨æ‰©å±•ä¸­è·å–ç¼“å­˜å®ä¾‹
                    extensions = getattr(current_app, 'extensions', {})
                    
                    # æŸ¥æ‰¾Cacheå®ä¾‹
                    cache_instance = None
                    for ext_name, ext_instance in extensions.items():
                        if isinstance(ext_instance, Cache):
                            cache_instance = ext_instance
                            self.logger.info("æˆåŠŸè·å–Flask-Cachingå®ä¾‹")
                            break
                    
                    if cache_instance is None:
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä»åº”ç”¨ä¸­ç›´æ¥è·å–
                        if hasattr(current_app, 'cache'):
                            cache_instance = current_app.cache
                            self.logger.info("ä»åº”ç”¨å±æ€§è·å–Flask-Cachingå®ä¾‹")
                        else:
                            # åˆ›å»ºæ–°çš„å®ä¾‹
                            cache_instance = Cache()
                            cache_instance.init_app(current_app)
                            self.logger.info("åˆ›å»ºæ–°çš„Flask-Cachingå®ä¾‹")
                    
                    self._cache = cache_instance
                        
                else:
                    self.logger.warning("æœªåœ¨Flaskåº”ç”¨ä¸Šä¸‹æ–‡ä¸­ï¼Œæ— æ³•ä½¿ç”¨ç¼“å­˜")
                    
            except Exception as e:
                self.logger.warning(f"è·å–Flask-Cachingå®ä¾‹å¤±è´¥: {e}")
                self._cache = None
                
        return self._cache
    
    def _generate_cache_key(self, filters: Optional[Dict[str, Any]] = None, max_records: int = 9999) -> str:
        """
        ç”Ÿæˆç¼“å­˜é”® - åªåŸºäºç­›é€‰æ¡ä»¶ï¼Œä¸åŒ…å«max_records
        è¿™æ ·å¯ä»¥è®©ä¸åŒçš„max_recordså€¼å¤ç”¨åŒä¸€ä¸ªç¼“å­˜
        """
        # åˆ›å»ºå”¯ä¸€çš„ç¼“å­˜é”®ï¼ŒåªåŸºäºç­›é€‰æ¡ä»¶
        cache_data = {
            'filters': filters or {},
            'version': '1.0'  # ç‰ˆæœ¬å·ï¼Œç”¨äºç¼“å­˜å¤±æ•ˆ
        }
        
        # ç”Ÿæˆå“ˆå¸Œ
        cache_str = json.dumps(cache_data, sort_keys=True, ensure_ascii=False)
        cache_hash = hashlib.md5(cache_str.encode('utf-8')).hexdigest()[:16]
        
        return f"market_data:{cache_hash}"
    
    def _get_cached_data(self, filters: Optional[Dict[str, Any]] = None, max_records: int = 9999) -> Optional[pd.DataFrame]:
        """ä»Flask-Cachingè·å–å¸‚åœºæ•°æ®ï¼Œæ”¯æŒæ™ºèƒ½æˆªå–"""
        cache = self._get_cache()
        if not cache:
            return None
        
        try:
            cache_key = self._generate_cache_key(filters, max_records)
            cached_data = cache.get(cache_key)
            
            if cached_data is not None:
                df = None
                
                # ä»JSONæ ¼å¼è¿˜åŸDataFrame
                if isinstance(cached_data, dict) and 'data' in cached_data:
                    df = pd.DataFrame(cached_data['data'])
                    # å¦‚æœæœ‰ç´¢å¼•ä¿¡æ¯ï¼Œè®¾ç½®ç´¢å¼•
                    if 'index' in cached_data and cached_data['index'] and len(df) > 0:
                        if cached_data['index'] in df.columns:
                            df.set_index(cached_data['index'], inplace=True)
                elif isinstance(cached_data, pd.DataFrame):
                    # ç›´æ¥æ˜¯DataFrameï¼ˆå‘åå…¼å®¹ï¼‰
                    df = cached_data
                else:
                    # å…¶ä»–æ ¼å¼ï¼Œå°è¯•è½¬æ¢
                    self.logger.warning(f"ä»Flask-Cachingè·å–åˆ°æœªçŸ¥æ ¼å¼æ•°æ®: {type(cached_data)}")
                    return None
                
                if df is not None:
                    # æ™ºèƒ½æˆªå–ï¼šå¦‚æœç¼“å­˜æ•°æ®é‡å¤§äºè¯·æ±‚é‡ï¼Œæˆªå–å‰Næ¡
                    original_len = len(df)
                    if original_len > max_records:
                        df = df.head(max_records)
                        self.logger.info(f"ä»Flask-Cachingè·å–å¸‚åœºæ•°æ®å¹¶æˆªå–ï¼ŒåŸå§‹: {original_len} æ¡ï¼Œæˆªå–: {len(df)} æ¡")
                    else:
                        self.logger.info(f"ä»Flask-Cachingè·å–å¸‚åœºæ•°æ®ï¼Œæ•°æ®é‡: {len(df)} æ¡ï¼ˆæ»¡è¶³è¯·æ±‚é‡ {max_records}ï¼‰")
                    
                    return df
                    
        except Exception as e:
            self.logger.warning(f"ä»Flask-Cachingè·å–æ•°æ®å¤±è´¥: {e}")
        
        return None
    
    def _set_cached_data(self, filters: Optional[Dict[str, Any]], max_records: int, data: pd.DataFrame) -> bool:
        """
        è®¾ç½®Flask-Cachingç¼“å­˜æ•°æ®
        æ™ºèƒ½ç¼“å­˜ç­–ç•¥ï¼šä¼˜å…ˆç¼“å­˜æ›´å¤§çš„æ•°æ®é›†ï¼Œä»¥ä¾¿åç»­ä¸åŒmax_recordsçš„è¯·æ±‚éƒ½èƒ½å¤ç”¨
        """
        cache = self._get_cache()
        if not cache:
            self.logger.warning("ç¼“å­˜å®ä¾‹ä¸å¯ç”¨")
            return False
            
        if data.empty:
            self.logger.warning("æ•°æ®ä¸ºç©ºï¼Œä¸è¿›è¡Œç¼“å­˜")
            return False
        
        try:
            cache_key = self._generate_cache_key(filters, max_records)
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜
            existing_cached = cache.get(cache_key)
            if existing_cached and isinstance(existing_cached, dict):
                existing_count = existing_cached.get('record_count', 0)
                current_count = len(data)
                
                # å¦‚æœå½“å‰æ•°æ®é‡å°äºæˆ–ç­‰äºå·²ç¼“å­˜çš„æ•°æ®é‡ï¼Œä¸æ›´æ–°ç¼“å­˜
                if current_count <= existing_count:
                    self.logger.info(f"è·³è¿‡ç¼“å­˜æ›´æ–°ï¼Œå½“å‰æ•°æ®é‡ {current_count} <= å·²ç¼“å­˜æ•°æ®é‡ {existing_count}")
                    return True
                else:
                    self.logger.info(f"æ›´æ–°ç¼“å­˜ï¼Œå½“å‰æ•°æ®é‡ {current_count} > å·²ç¼“å­˜æ•°æ®é‡ {existing_count}")
            
            # å°†DataFrameè½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
            cache_data = {
                'data': data.reset_index().to_dict(orient='records'),
                'index': data.index.name if hasattr(data.index, 'name') and data.index.name else 'eid',
                'cached_at': datetime.now().isoformat(),
                'record_count': len(data),
                'max_records_used': max_records  # è®°å½•ç”¨äºç”Ÿæˆæ­¤ç¼“å­˜çš„max_recordsï¼Œç”¨äºè°ƒè¯•
            }
            
            # è®¾ç½®ç¼“å­˜ï¼Œä½¿ç”¨6å°æ—¶TTL
            result = cache.set(cache_key, cache_data, timeout=6*3600)
            
            # Flask-Caching çš„ set æ–¹æ³•è¿”å› True/False æˆ– None
            if result is True:
                self.logger.info(f"å¸‚åœºæ•°æ®å·²ç¼“å­˜åˆ°Flask-Cachingï¼Œæ•°æ®é‡: {len(data)}ï¼Œæ”¯æŒmax_records <= {len(data)}")
                return True
            elif result is False:
                self.logger.warning("Flask-Caching.set() è¿”å›False")
                return False
            elif result is None:
                # æœ‰äº›ç¼“å­˜åç«¯è¿”å›Noneè¡¨ç¤ºæˆåŠŸ
                self.logger.info(f"å¸‚åœºæ•°æ®å·²ç¼“å­˜åˆ°Flask-Cachingï¼ˆè¿”å›Noneï¼‰ï¼Œæ•°æ®é‡: {len(data)}ï¼Œæ”¯æŒmax_records <= {len(data)}")
                return True
            else:
                self.logger.warning(f"Flask-Caching.set() è¿”å›æœªé¢„æœŸå€¼: {result}")
                return False
            
        except Exception as e:
            self.logger.warning(f"è®¾ç½®Flask-Cachingå¤±è´¥: {str(e)}")
            return False
    
    
    def refresh_market_data(self, 
                       filters: Optional[Dict[str, Any]] = None,
                       max_records: int = 99999,
                       use_cache: bool = True,
                       force_refresh: bool = False,
                       batch_size: int = 2000) -> pd.DataFrame:
        """
        åˆ·æ–°å¸‚åœºæ•°æ® - ä¼˜å…ˆä»Rediså…¨é‡ç¼“å­˜è·å–ï¼Œæ”¯æŒç­›é€‰ã€åˆ†é¡µå’Œè¯¦ç»†è¿›åº¦è·Ÿè¸ª
        
        Args:
            filters: ç­›é€‰æ¡ä»¶å­—å…¸ï¼Œä¾‹å¦‚ {'level_min': 109, 'price_max': 10000}
            max_records: æœ€å¤§è®°å½•æ•°
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°å…¨é‡ç¼“å­˜
            batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆä»…åœ¨ä»æ•°æ®åº“åŠ è½½æ—¶ä½¿ç”¨ï¼‰
            
        Returns:
            pd.DataFrame: å¤„ç†åçš„å¸‚åœºæ•°æ®
        """
        try:
            start_time = time.time()
            
            # åˆå§‹åŒ–è¿›åº¦çŠ¶æ€
            self._refresh_status = "running"
            self._refresh_progress = 0
            self._refresh_message = "å‡†å¤‡å¼€å§‹..."
            self._refresh_start_time = datetime.now()
            self._refresh_processed_records = 0
            self._refresh_current_batch = 0
            self._refresh_total_batches = 0
            self._refresh_total_records = 0
            
            # å¦‚æœä½¿ç”¨ç¼“å­˜ä¸”ä¸å¼ºåˆ¶åˆ·æ–°ï¼Œå°è¯•ä»Rediså…¨é‡ç¼“å­˜è·å–
            if use_cache and not force_refresh:
                self._refresh_message = "æ£€æŸ¥Redisç¼“å­˜..."
                self._refresh_progress = 5
                
                cached_data = self._get_full_cached_data()
                if cached_data is not None and not cached_data.empty:
                    self._refresh_message = "ä»ç¼“å­˜ç­›é€‰æ•°æ®..."
                    self._refresh_progress = 30
                    
                    # åº”ç”¨ç­›é€‰æ¡ä»¶
                    filtered_data = self._apply_filters(cached_data, filters, max_records)
                    
                    self.market_data = filtered_data
                    self._data_loaded = True
                    self._last_refresh_time = datetime.now()
                    
                    # æ›´æ–°è¿›åº¦çŠ¶æ€ - ç¼“å­˜å‘½ä¸­å®Œæˆ
                    self._refresh_status = "completed"
                    self._refresh_progress = 100
                    self._refresh_message = "ä»ç¼“å­˜è·å–å®Œæˆï¼"
                    self._refresh_processed_records = len(filtered_data)
                    self._refresh_total_records = len(filtered_data)
                    
                    elapsed_time = time.time() - start_time
                    print(f"ä»Rediså…¨é‡ç¼“å­˜è·å–å¸‚åœºæ•°æ®æˆåŠŸï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
                    print(f"å…¨é‡ç¼“å­˜æ•°æ®: {len(cached_data)} æ¡ï¼Œç­›é€‰å: {len(filtered_data)} æ¡")
                    print(f"ç‰¹å¾ç»´åº¦: {len(filtered_data.columns)}")
                    if not filtered_data.empty:
                        print(f"ä»·æ ¼èŒƒå›´: {filtered_data['price'].min():.1f} - {filtered_data['price'].max():.1f}")
                    
                    return self.market_data
                else:
                    print("Rediså…¨é‡ç¼“å­˜æœªå‘½ä¸­æˆ–å·²è¿‡æœŸï¼Œå°†ä»MySQLé‡æ–°åŠ è½½å…¨é‡æ•°æ®")
                    self._refresh_message = "ç¼“å­˜æœªå‘½ä¸­ï¼Œå‡†å¤‡ä»æ•°æ®åº“åŠ è½½..."
                    self._refresh_progress = 10
            
            self._refresh_message = "å¼€å§‹ä»æ•°æ®åº“åŠ è½½å…¨é‡æ•°æ®..."
            print(f"å¼€å§‹ä»æ•°æ®åº“åŠ è½½å…¨é‡å¸‚åœºæ•°æ®åˆ°Redisç¼“å­˜")
            
            # å¯¼å…¥MySQLè¿æ¥ç›¸å…³æ¨¡å—
            from sqlalchemy import create_engine, text
            from src.app import create_app
            
            # åˆ›å»ºFlaskåº”ç”¨ä¸Šä¸‹æ–‡è·å–æ•°æ®åº“é…ç½®
            app = create_app()
            
            with app.app_context():
                # è·å–æ•°æ®åº“é…ç½®
                db_config = app.config.get('SQLALCHEMY_DATABASE_URI')
                if not db_config:
                    raise ValueError("æœªæ‰¾åˆ°æ•°æ®åº“é…ç½®")
                
                self._refresh_message = "è¿æ¥æ•°æ®åº“..."
                self._refresh_progress = 15
                print(f"è¿æ¥MySQLæ•°æ®åº“: {db_config}")
                
                # åˆ›å»ºä¼˜åŒ–çš„æ•°æ®åº“è¿æ¥ - ä½¿ç”¨è¿æ¥æ± 
                connection_config = self._get_optimized_connection_config(db_config)
                engine = create_engine(db_config, **connection_config)
                
                # è¾“å‡ºæ•°æ®åº“ç´¢å¼•ä¼˜åŒ–å»ºè®®
                self._optimize_database_indexes()
                
                # ä¼˜åŒ–çš„åˆ†æ‰¹æŸ¥è¯¢ç­–ç•¥ - é¿å…ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ•°æ®
                self._refresh_message = "åˆ†ææ•°æ®é‡..."
                self._refresh_progress = 20
                print(f"å¼€å§‹ä¼˜åŒ–çš„åˆ†æ‰¹å…¨é‡æŸ¥è¯¢...")
                
                # é¦–å…ˆè·å–æ€»è®°å½•æ•°
                count_query = """
                    SELECT COUNT(*) as total_count
                    FROM roles c
                    WHERE c.role_type = 'empty' AND c.price > 0
                """
                
                with engine.connect() as conn:
                    count_result = conn.execute(text(count_query))
                    total_count = count_result.fetchone()[0]
                
                print(f"æ€»è®°å½•æ•°: {total_count}")
                
                # åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°ï¼ˆå¦‚æœæ²¡æœ‰ä¼ å…¥batch_sizeå‚æ•°ï¼‰
                if batch_size == 2000:  # ä½¿ç”¨é»˜è®¤å€¼æ—¶æ‰è‡ªåŠ¨è°ƒæ•´
                    if total_count > 50000:
                        batch_size = 5000  # å¤§æ•°æ®é›†ä½¿ç”¨è¾ƒå¤§æ‰¹æ¬¡
                    elif total_count > 20000:
                        batch_size = 2000  # ä¸­ç­‰æ•°æ®é›†
                    else:
                        batch_size = 1000  # å°æ•°æ®é›†
                
                total_batches = (total_count + batch_size - 1) // batch_size
                
                # æ›´æ–°è¿›åº¦è·Ÿè¸ªä¿¡æ¯
                self._refresh_total_records = total_count
                self._refresh_total_batches = total_batches
                self._refresh_message = f"å¼€å§‹åˆ†æ‰¹å¤„ç†ï¼Œå…± {total_batches} æ‰¹..."
                self._refresh_progress = 25
                
                print(f"å°†åˆ† {total_batches} æ‰¹å¤„ç†ï¼Œæ¯æ‰¹ {batch_size} æ¡")
                
                # ä¼˜åŒ–çš„SQLæŸ¥è¯¢ - åªé€‰æ‹©å¿…è¦å­—æ®µï¼Œå‡å°‘æ•°æ®ä¼ è¾“
                base_query = """
                    SELECT 
                        c.eid, c.serverid, c.level, c.school, c.price, c.collect_num,
                        c.yushoushu_skill, c.school_skills, c.life_skills,
                        l.sum_exp, l.three_fly_lv, l.all_new_point, l.jiyuan_amount, 
                        l.packet_page, l.xianyu_amount, l.sum_amount,
                        l.expt_ski1, l.expt_ski2, l.expt_ski3, l.expt_ski4, l.expt_ski5,
                        l.beast_ski1, l.beast_ski2, l.beast_ski3, l.beast_ski4,
                        l.changesch_json, l.ex_avt_json, l.huge_horse_json, l.shenqi_json,
                        l.all_equip_json, l.all_summon_json, l.all_rider_json
                    FROM roles c
                    LEFT JOIN large_equip_desc_data l ON c.eid = l.eid
                    WHERE c.role_type = 'empty' AND c.price > 0
                    ORDER BY c.price ASC
                    LIMIT {batch_size} OFFSET {offset}
                """
                
                # åˆ†æ‰¹å¤„ç†æ•°æ®
                full_market_data = []
                processed_count = 0
                
                with engine.connect() as conn:
                    for batch_num in range(total_batches):
                        # æ›´æ–°å½“å‰æ‰¹æ¬¡
                        self._refresh_current_batch = batch_num + 1
                        offset = batch_num * batch_size
                        current_batch_query = base_query.format(batch_size=batch_size, offset=offset)
                        
                        # æ›´æ–°è¿›åº¦æ¶ˆæ¯
                        self._refresh_message = f"å¤„ç†ç¬¬ {batch_num + 1}/{total_batches} æ‰¹æ•°æ®..."
                        
                        # è®¡ç®—è¿›åº¦ï¼ˆ25% å·²ç”¨äºå‰æœŸå‡†å¤‡ï¼Œå‰©ä½™75%ç”¨äºæ•°æ®å¤„ç†ï¼‰
                        batch_progress = 25 + (batch_num / total_batches) * 70  # 70%ç”¨äºæ•°æ®å¤„ç†ï¼Œ5%ç•™ç»™æœ€åçš„ç¼“å­˜
                        self._refresh_progress = int(batch_progress)
                        
                        print(f"å¤„ç†ç¬¬ {batch_num + 1}/{total_batches} æ‰¹ï¼Œåç§»é‡: {offset}")
                        
                        try:
                            # æ‰§è¡Œæ‰¹æ¬¡æŸ¥è¯¢
                            batch_result = conn.execute(text(current_batch_query))
                            batch_columns = batch_result.keys()
                            batch_rows = batch_result.fetchall()
                            
                            if not batch_rows:
                                print(f"ç¬¬ {batch_num + 1} æ‰¹æ— æ•°æ®ï¼Œè·³è¿‡")
                                continue
                            
                            # ä½¿ç”¨å¹¶è¡Œå¤„ç†å½“å‰æ‰¹æ¬¡çš„æ•°æ®
                            batch_data = self._process_batch_data_parallel(batch_rows, batch_columns, batch_num + 1)
                            full_market_data.extend(batch_data)
                            
                            processed_count += len(batch_data)
                            self._refresh_processed_records = processed_count
                            
                            progress_percent = (processed_count / total_count) * 100
                            print(f"å·²å¤„ç† {processed_count}/{total_count} æ¡æ•°æ® ({progress_percent:.1f}%)")
                            
                            # æ¯å¤„ç†å‡ æ‰¹å°±å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼Œé‡Šæ”¾å†…å­˜
                            if batch_num % 5 == 0:
                                import gc
                                gc.collect()
                                
                        except Exception as e:
                            self.logger.error(f"å¤„ç†ç¬¬ {batch_num + 1} æ‰¹æ•°æ®å¤±è´¥: {e}")
                            continue
                
                # è½¬æ¢ä¸ºDataFrame
                self._refresh_message = "å¤„ç†æ•°æ®æ ¼å¼..."
                self._refresh_progress = 95
                full_data_df = pd.DataFrame(full_market_data)
                
                if not full_data_df.empty:
                    full_data_df.set_index('eid', inplace=True)
                    
                    elapsed_time = time.time() - start_time
                    print(f"å…¨é‡å¸‚åœºæ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(full_data_df)} æ¡æœ‰æ•ˆæ•°æ®ï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
                    print(f"æ•°æ®ç‰¹å¾ç»´åº¦: {len(full_data_df.columns)}")
                    print(f"ä»·æ ¼èŒƒå›´: {full_data_df['price'].min():.1f} - {full_data_df['price'].max():.1f}")
                    
                    # ç¼“å­˜å…¨é‡æ•°æ®åˆ°Redis
                    if use_cache:
                        self._refresh_message = "ç¼“å­˜æ•°æ®åˆ°Redis..."
                        cache_start = time.time()
                        if self._set_full_cached_data(full_data_df):
                            cache_time = time.time() - cache_start
                            print(f"å…¨é‡æ•°æ®å·²ç¼“å­˜åˆ°Redisï¼Œç¼“å­˜è€—æ—¶: {cache_time:.2f}ç§’")
                        else:
                            print("Rediså…¨é‡ç¼“å­˜è®¾ç½®å¤±è´¥ï¼Œä½†æ•°æ®è·å–æˆåŠŸ")
                    
                    # åº”ç”¨ç­›é€‰æ¡ä»¶å¹¶è¿”å›ç»“æœ
                    self._refresh_message = "åº”ç”¨ç­›é€‰æ¡ä»¶..."
                    filtered_data = self._apply_filters(full_data_df, filters, max_records)
                    self.market_data = filtered_data
                    
                    # æ›´æ–°ç¼“å­˜çŠ¶æ€
                    self._data_loaded = True
                    self._last_refresh_time = datetime.now()
                    
                    # å®Œæˆè¿›åº¦è·Ÿè¸ª
                    self._refresh_status = "completed"
                    self._refresh_progress = 100
                    self._refresh_message = "æ•°æ®åŠ è½½å®Œæˆï¼"
                    self._refresh_processed_records = len(filtered_data)
                    self._refresh_total_records = len(filtered_data)
                    
                    print(f"ç­›é€‰åæ•°æ®: {len(filtered_data)} æ¡")
                    
                else:
                    print("è­¦å‘Šï¼šæœªè·å–åˆ°æœ‰æ•ˆçš„å¸‚åœºæ•°æ®")
                    self.market_data = pd.DataFrame()
                    
                    # è®¾ç½®å¤±è´¥çŠ¶æ€
                    self._refresh_status = "error"
                    self._refresh_progress = 0
                    self._refresh_message = "æœªè·å–åˆ°æœ‰æ•ˆæ•°æ®"
                
                return self.market_data
                
        except Exception as e:
            # æ›´æ–°é”™è¯¯çŠ¶æ€
            self._refresh_status = "error"
            self._refresh_progress = 0
            self._refresh_message = f"åˆ·æ–°å¤±è´¥: {str(e)}"
            
            self.logger.error(f"åˆ·æ–°å¸‚åœºæ•°æ®å¤±è´¥: {e}")
            raise
    def get_refresh_status(self) -> Dict[str, Any]:
        """
        è·å–åˆ·æ–°è¿›åº¦çŠ¶æ€
        
        Returns:
            Dict: åŒ…å«è¿›åº¦ä¿¡æ¯çš„å­—å…¸
        """
        status_info = {
            "status": self._refresh_status,
            "progress": self._refresh_progress,
            "message": self._refresh_message,
            "processed_records": self._refresh_processed_records,
            "total_records": self._refresh_total_records,
            "current_batch": self._refresh_current_batch,
            "total_batches": self._refresh_total_batches,
            "start_time": self._refresh_start_time.isoformat() if self._refresh_start_time else None,
            "elapsed_seconds": (datetime.now() - self._refresh_start_time).total_seconds() if self._refresh_start_time else 0
        }
        return status_info

    def get_market_data(self, force_refresh: bool = False) -> pd.DataFrame:
        """
        è·å–å½“å‰çš„å¸‚åœºæ•°æ®ï¼Œæ”¯æŒæ™ºèƒ½ç¼“å­˜å’Œè¿‡æœŸæ£€æŸ¥
        
        Args:
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°æ•°æ®
            
        Returns:
            pd.DataFrame: å¸‚åœºæ•°æ®
        """
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°æ•°æ®
        need_refresh = (
            force_refresh or 
            not self._data_loaded or 
            self.market_data.empty or 
            self._is_cache_expired()
        )
        
        if need_refresh:
            print("ğŸ”„ æ•°æ®éœ€è¦åˆ·æ–°ï¼Œå¼€å§‹åˆ·æ–°å¸‚åœºæ•°æ®...")
            self.refresh_market_data()
        else:
            print("âœ… ä½¿ç”¨ç°æœ‰æ•°æ®ï¼Œæ•°æ®ä»åœ¨æœ‰æ•ˆæœŸå†…")
        
        return self.market_data

    def _is_cache_expired(self) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ"""
        if not self._last_refresh_time:
            return True
        
        expiry_time = self._last_refresh_time + timedelta(hours=self._cache_expiry_hours)
        return datetime.now() > expiry_time
                    
                    elapsed_time = time.time() - start_time
                    print(f"ä»Flask-Cachingç¼“å­˜è·å–æ•°æ®æˆåŠŸï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
                    print(f"ç¼“å­˜æ•°æ®é‡: {len(cached_data)} æ¡ï¼Œç‰¹å¾ç»´åº¦: {len(cached_data.columns)}")
                    print(f"âœ… æ™ºèƒ½ç¼“å­˜å¤ç”¨æˆåŠŸï¼max_records={max_records}")
                    
                    # æ£€æŸ¥æ˜¯å¦è¿›è¡Œäº†æ™ºèƒ½æˆªå–
                    if 'data' in cached_data.columns and len(cached_data) < max_records:
                        print(f"ğŸ’¡ ç¼“å­˜æ•°æ®({len(cached_data)}æ¡) < è¯·æ±‚é‡({max_records}æ¡)ï¼Œå·²è¿”å›å…¨éƒ¨ç¼“å­˜æ•°æ®")
                    
                    return self.market_data
            
            # åˆå§‹åŒ–è¿›åº¦çŠ¶æ€
            self._refresh_status = "running"
            self._refresh_progress = 0
            self._refresh_message = "å‡†å¤‡å¼€å§‹..."
            self._refresh_start_time = datetime.now()
            self._refresh_processed_records = 0
            self._refresh_current_batch = 0
            
            print(f"å¼€å§‹åˆ†æ‰¹åˆ·æ–°å¸‚åœºæ•°æ®ï¼Œæœ€å¤§è®°å½•æ•°: {max_records}ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
            
            # å¯¼å…¥MySQLè¿æ¥ç›¸å…³æ¨¡å—
            from sqlalchemy import create_engine, text
            from src.app import create_app
            
            # åˆ›å»ºFlaskåº”ç”¨ä¸Šä¸‹æ–‡è·å–æ•°æ®åº“é…ç½®
            app = create_app()
            
            with app.app_context():
                # è·å–æ•°æ®åº“é…ç½®
                db_config = app.config.get('SQLALCHEMY_DATABASE_URI')
                if not db_config:
                    raise ValueError("æœªæ‰¾åˆ°æ•°æ®åº“é…ç½®")
                
                self._refresh_message = "è¿æ¥æ•°æ®åº“..."
                self._refresh_progress = 5
                print(f"è¿æ¥MySQLæ•°æ®åº“: {db_config}")
                
                # åˆ›å»ºæ•°æ®åº“è¿æ¥
                engine = create_engine(db_config)
                
                # æ„å»ºåŸºç¡€SQLæŸ¥è¯¢
                base_query = """
                    SELECT 
                        c.eid, c.serverid, c.level, c.school,
                        c.price, c.collect_num,
                        c.yushoushu_skill, c.school_skills, c.life_skills, c.expire_time,
                        l.sum_exp, l.three_fly_lv, l.all_new_point,
                        l.jiyuan_amount, l.packet_page, l.xianyu_amount, l.learn_cash,
                        l.sum_amount, l.role_icon,
                        l.expt_ski1, l.expt_ski2, l.expt_ski3, l.expt_ski4, l.expt_ski5,
                        l.beast_ski1, l.beast_ski2, l.beast_ski3, l.beast_ski4,
                        l.changesch_json, l.ex_avt_json, l.huge_horse_json, l.shenqi_json,
                        l.all_equip_json, l.all_summon_json, l.all_rider_json
                    FROM roles c
                    LEFT JOIN large_equip_desc_data l ON c.eid = l.eid
                    WHERE c.role_type = 'empty' AND c.price > 0
                """
                
                # æ·»åŠ ç­›é€‰æ¡ä»¶
                conditions = []
                if filters:
                    if 'level_min' in filters:
                        conditions.append(f"c.level >= {filters['level_min']}")
                    if 'level_max' in filters:
                        conditions.append(f"c.level <= {filters['level_max']}")
                    if 'price_min' in filters:
                        conditions.append(f"c.price >= {filters['price_min']}")
                    if 'price_max' in filters:
                        conditions.append(f"c.price <= {filters['price_max']}")
                    if 'server_name' in filters:
                        conditions.append(f"c.server_name = '{filters['server_name']}'")
                    if 'school' in filters:
                        conditions.append(f"c.school = {filters['school']}")
                    if 'serverid' in filters:
                        conditions.append(f"c.serverid = {filters['serverid']}")
                
                if conditions:
                    base_query += " AND " + " AND ".join(conditions)
                
                base_query += " ORDER BY c.price ASC"
                
                # è®¡ç®—æ€»æ‰¹æ¬¡æ•°
                self._refresh_total_batches = (max_records + batch_size - 1) // batch_size
                self._refresh_total_records = max_records
                
                self._refresh_message = f"å¼€å§‹åˆ†æ‰¹å¤„ç†ï¼Œå…± {self._refresh_total_batches} æ‰¹..."
                self._refresh_progress = 10
                
                # åˆ†æ‰¹å¤„ç†æ•°æ®
                market_data = []
                
                with engine.connect() as conn:
                    for batch_num in range(self._refresh_total_batches):
                        # æ›´æ–°å½“å‰æ‰¹æ¬¡
                        self._refresh_current_batch = batch_num + 1
                        
                        # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„åç§»é‡å’Œå¤§å°
                        offset = batch_num * batch_size
                        current_batch_size = min(batch_size, max_records - offset)
                        
                        if current_batch_size <= 0:
                            break
                        
                        # åˆ†æ‰¹å¤„ç†ä¸ä½¿ç”¨æ‰¹æ¬¡çº§ç¼“å­˜ï¼Œåªåœ¨æœ€ç»ˆç¼“å­˜å®Œæ•´æ•°æ®
                        # è¿™æ ·ä¸åŒæ‰¹æ¬¡å¤§å°éƒ½å¯ä»¥ä½¿ç”¨åŒä¸€ä¸ªç¼“å­˜
                        
                        # æ„å»ºåˆ†é¡µæŸ¥è¯¢
                        paginated_query = f"{base_query} LIMIT {current_batch_size} OFFSET {offset}"
                        
                        self._refresh_message = f"å¤„ç†ç¬¬ {batch_num + 1}/{self._refresh_total_batches} æ‰¹æ•°æ®..."
                        
                        try:
                            # æ‰§è¡ŒæŸ¥è¯¢
                            result = conn.execute(text(paginated_query))
                            if batch_num == 0:  # ç¬¬ä¸€æ¬¡è·å–åˆ—å
                                columns = result.keys()
                            rows = result.fetchall()
                            
                            if not rows:
                                print(f"ç¬¬ {batch_num + 1} æ‰¹æŸ¥è¯¢æ— æ•°æ®ï¼Œåœæ­¢å¤„ç†")
                                break
                            
                            print(f"ç¬¬ {batch_num + 1} æ‰¹è·å–åˆ° {len(rows)} æ¡åŸå§‹æ•°æ®")
                            
                            # å¤„ç†å½“å‰æ‰¹æ¬¡æ•°æ®
                            batch_data = []
                            for i, row in enumerate(rows):
                                try:
                                    role_data = dict(zip(columns, row))
                                    
                                    # æå–ç‰¹å¾
                                    features = self.feature_extractor.extract_features(role_data)
                                    
                                    # æ·»åŠ åŸºæœ¬ä¿¡æ¯
                                    features.update({
                                        'eid': role_data.get('eid', ''),
                                        'price': role_data.get('price', 0),
                                        'school': role_data.get('school', 0),
                                        'serverid': role_data.get('serverid', 0)
                                    })
                                    
                                    batch_data.append(features)
                                    self._refresh_processed_records += 1
                                    
                                except Exception as e:
                                    self.logger.warning(f"æ‰¹æ¬¡ {batch_num + 1} ç¬¬ {i+1} æ¡æ•°æ®å¤„ç†å¤±è´¥: {e}")
                                    continue
                            
                            market_data.extend(batch_data)
                            
                            # ä¸è¿›è¡Œæ‰¹æ¬¡çº§ç¼“å­˜ï¼Œåªåœ¨æœ€ç»ˆç¼“å­˜å®Œæ•´æ•°æ®
                            # è¿™æ ·ä»»ä½•æ‰¹æ¬¡å¤§å°éƒ½å¯ä»¥ä½¿ç”¨ç›¸åŒçš„ç¼“å­˜
                            
                            # æ›´æ–°è¿›åº¦
                            batch_progress = 10 + (batch_num + 1) / self._refresh_total_batches * 80
                            self._refresh_progress = min(90, int(batch_progress))
                            
                            print(f"æ‰¹æ¬¡ {batch_num + 1} å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆæ•°æ®: {len(batch_data)} æ¡ï¼Œæ€»è®¡: {len(market_data)} æ¡")
                            
                            # æ‰¹æ¬¡é—´çŸ­æš‚ä¼‘æ¯
                            if batch_num < self._refresh_total_batches - 1:
                                time.sleep(0.05)  # 50msä¼‘æ¯ï¼Œå‡å°‘ç­‰å¾…æ—¶é—´
                                
                        except Exception as e:
                            self.logger.error(f"ç¬¬ {batch_num + 1} æ‰¹å¤„ç†å¤±è´¥: {e}")
                            break
                
                # æ„å»ºæœ€ç»ˆDataFrame
                self._refresh_message = "æ„å»ºæ•°æ®ç´¢å¼•..."
                self._refresh_progress = 95
                
                self.market_data = pd.DataFrame(market_data)
                
                if not self.market_data.empty:
                    self.market_data.set_index('eid', inplace=True)
                    
                    # æ›´æ–°ç¼“å­˜çŠ¶æ€
                    self._data_loaded = True
                    self._last_refresh_time = datetime.now()
                    
                    self._refresh_message = "åˆ·æ–°å®Œæˆï¼"
                    self._refresh_progress = 100
                    self._refresh_status = "completed"
                    
                    elapsed_time = time.time() - start_time
                    print(f"å¸‚åœºæ•°æ®åˆ†æ‰¹åˆ·æ–°å®Œæˆï¼Œå…± {len(self.market_data)} æ¡æœ‰æ•ˆæ•°æ®ï¼Œæ€»è€—æ—¶: {elapsed_time:.2f}ç§’")
                    print(f"æ•°æ®ç‰¹å¾ç»´åº¦: {len(self.market_data.columns)}")
                    print(f"ä»·æ ¼èŒƒå›´: {self.market_data['price'].min():.1f} - {self.market_data['price'].max():.1f}")
                    
                    # ç¼“å­˜å®Œæ•´æ•°æ®åˆ°Flask-Cachingï¼ˆåŸºäºç­›é€‰æ¡ä»¶å’Œè®°å½•æ•°ï¼‰
                    if use_cache:
                        cache_start = time.time()
                        if self._set_cached_data(filters, max_records, self.market_data):
                            cache_time = time.time() - cache_start
                            print(f"å®Œæ•´æ•°æ®å·²ç¼“å­˜åˆ°Flask-Cachingï¼Œç¼“å­˜è€—æ—¶: {cache_time:.2f}ç§’")
                            print(f"âœ… æ­¤ç¼“å­˜å¯è¢«ä»»ä½•æ‰¹æ¬¡å¤§å°å¤ç”¨ï¼")
                        else:
                            print("Flask-Cachingè®¾ç½®å¤±è´¥ï¼Œä½†æ•°æ®è·å–æˆåŠŸ")
                    
                else:
                    self._refresh_message = "æœªè·å–åˆ°æœ‰æ•ˆæ•°æ®"
                    self._refresh_status = "completed"
                    print("è­¦å‘Šï¼šæœªè·å–åˆ°æœ‰æ•ˆçš„å¸‚åœºæ•°æ®")
                
                return self.market_data
                
        except Exception as e:
            self._refresh_status = "error"
            self._refresh_message = f"åˆ·æ–°å¤±è´¥: {str(e)}"
            self._refresh_progress = 0
            self.logger.error(f"åˆ†æ‰¹åˆ·æ–°å¸‚åœºæ•°æ®å¤±è´¥: {e}")
            raise
    
    def get_refresh_status(self) -> Dict[str, Any]:
        """
        è·å–åˆ·æ–°è¿›åº¦çŠ¶æ€
        
        Returns:
            Dict: åŒ…å«è¿›åº¦ä¿¡æ¯çš„å­—å…¸
        """
        status_info = {
            "status": self._refresh_status,
            "progress": self._refresh_progress,
            "message": self._refresh_message,
            "processed_records": self._refresh_processed_records,
            "total_records": self._refresh_total_records,
            "current_batch": self._refresh_current_batch,
            "total_batches": self._refresh_total_batches,
            "start_time": self._refresh_start_time.isoformat() if self._refresh_start_time else None,
            "elapsed_seconds": int((datetime.now() - self._refresh_start_time).total_seconds()) if self._refresh_start_time else 0
        }
        
        return status_info
    
    def get_market_data(self, force_refresh: bool = False) -> pd.DataFrame:
        """
        è·å–å½“å‰çš„å¸‚åœºæ•°æ®ï¼Œæ”¯æŒæ™ºèƒ½ç¼“å­˜å’Œè¿‡æœŸæ£€æŸ¥
        
        Args:
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°æ•°æ®
            
        Returns:
            pd.DataFrame: å¸‚åœºæ•°æ®
        """
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°æ•°æ®
        need_refresh = (
            force_refresh or 
            not self._data_loaded or 
            self.market_data.empty or 
            self._is_cache_expired()
        )
        
        if need_refresh:
            if force_refresh:
                print("å¼ºåˆ¶åˆ·æ–°å¸‚åœºæ•°æ®...")
            elif not self._data_loaded:
                print("é¦–æ¬¡åŠ è½½å¸‚åœºæ•°æ®...")
            elif self.market_data.empty:
                print("å¸‚åœºæ•°æ®ä¸ºç©ºï¼Œæ­£åœ¨åˆ·æ–°...")
            elif self._is_cache_expired():
                print(f"ç¼“å­˜å·²è¿‡æœŸï¼ˆè¶…è¿‡{self._cache_expiry_hours}å°æ—¶ï¼‰ï¼Œæ­£åœ¨åˆ·æ–°...")
            
            self.refresh_market_data()
            self._data_loaded = True
            self._last_refresh_time = datetime.now()
        else:
            print(f"ä½¿ç”¨ç¼“å­˜çš„å¸‚åœºæ•°æ®ï¼Œä¸Šæ¬¡åˆ·æ–°æ—¶é—´: {self._last_refresh_time}, "
                  f"æ•°æ®é‡: {len(self.market_data)}")
        
        return self.market_data
    
    def _is_cache_expired(self) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ"""
        if not self._last_refresh_time:
            return True
        
        elapsed_time = datetime.now() - self._last_refresh_time
        return elapsed_time > timedelta(hours=self._cache_expiry_hours)
    
    
    def filter_market_data(self, 
                          level_range: Optional[tuple] = None,
                          price_range: Optional[tuple] = None,
                          school: Optional[str] = None,
                          server: Optional[str] = None) -> pd.DataFrame:
        """
        ç­›é€‰å¸‚åœºæ•°æ®
        
        Args:
            level_range: ç­‰çº§èŒƒå›´ (min, max)
            price_range: ä»·æ ¼èŒƒå›´ (min, max)  
            school: é—¨æ´¾
            server: æœåŠ¡å™¨
            
        Returns:
            pd.DataFrame: ç­›é€‰åçš„æ•°æ®
        """
        df = self.get_market_data().copy()
        
        if level_range:
            df = df[(df['level'] >= level_range[0]) & (df['level'] <= level_range[1])]
        
        if price_range:
            df = df[(df['price'] >= price_range[0]) & (df['price'] <= price_range[1])]
        
        if school:
            df = df[df['school_desc'] == school]
        
        if server:
            df = df[df['server_name'] == server]
        
        return df
    
    def get_market_data_for_similarity(self, 
                                      filters: Optional[Dict[str, Any]] = None) -> pd.DataFrame:
        """
        è·å–ç”¨äºç›¸ä¼¼åº¦è®¡ç®—çš„å¸‚åœºæ•°æ®
        
        Args:
            filters: é¢„è¿‡æ»¤æ¡ä»¶ï¼Œç”¨äºå‡å°‘è®¡ç®—é‡
            
        Returns:
            pd.DataFrame: å¸‚åœºæ•°æ®
        """
        self.logger.info(f"[GET_SIMILARITY_DATA] å¼€å§‹è·å–ç›¸ä¼¼åº¦è®¡ç®—æ•°æ®ï¼Œfilters: {filters}")
        
        market_data = self.get_market_data()
        
        self.logger.info(f"[GET_SIMILARITY_DATA] åŸå§‹å¸‚åœºæ•°æ®å¤§å°: {len(market_data)}")
        
        if market_data.empty:
            self.logger.warning("[GET_SIMILARITY_DATA] å¸‚åœºæ•°æ®ä¸ºç©º")
            return market_data
        
        # åº”ç”¨é¢„è¿‡æ»¤æ¡ä»¶ä»¥æé«˜æ•ˆç‡
        if filters:
            self.logger.info(f"[GET_SIMILARITY_DATA] åº”ç”¨é¢„è¿‡æ»¤æ¡ä»¶: {filters}")
            filtered_data = market_data.copy()
            
            # ç­‰çº§è¿‡æ»¤
            if 'level_range' in filters and filters['level_range'] is not None:
                level_range = filters['level_range']
                self.logger.debug(f"[GET_SIMILARITY_DATA] ç­‰çº§è¿‡æ»¤ï¼šlevel_range={level_range}, ç±»å‹={type(level_range)}")
                if isinstance(level_range, (tuple, list)) and len(level_range) == 2:
                    self.logger.debug(f"[GET_SIMILARITY_DATA] å¼€å§‹è§£åŒ…ç­‰çº§èŒƒå›´...")
                    level_min, level_max = level_range
                    self.logger.debug(f"[GET_SIMILARITY_DATA] ç­‰çº§èŒƒå›´è§£åŒ…æˆåŠŸ: {level_min} - {level_max}")
                    filtered_data = filtered_data[
                        (filtered_data['level'] >= level_min) & 
                        (filtered_data['level'] <= level_max)
                    ]
            
            # æ€»ä¿®ç‚¼ç­‰çº§è¿‡æ»¤
            if 'total_cultivation_range' in filters and filters['total_cultivation_range'] is not None:
                cultivation_range = filters['total_cultivation_range']
                self.logger.debug(f"[GET_SIMILARITY_DATA] ä¿®ç‚¼è¿‡æ»¤ï¼šcultivation_range={cultivation_range}, ç±»å‹={type(cultivation_range)}")
                if isinstance(cultivation_range, (tuple, list)) and len(cultivation_range) == 2:
                    self.logger.debug(f"[GET_SIMILARITY_DATA] å¼€å§‹è§£åŒ…ä¿®ç‚¼èŒƒå›´...")
                    cult_min, cult_max = cultivation_range
                    self.logger.debug(f"[GET_SIMILARITY_DATA] ä¿®ç‚¼èŒƒå›´è§£åŒ…æˆåŠŸ: {cult_min} - {cult_max}")
                    filtered_data = filtered_data[
                        (filtered_data['total_cultivation'] >= cult_min) & 
                        (filtered_data['total_cultivation'] <= cult_max)
                    ]
            
            # å¬å”¤å…½æ§åˆ¶åŠ›æ€»å’Œè¿‡æ»¤
            if 'total_beast_cultivation_range' in filters and filters['total_beast_cultivation_range'] is not None:
                beast_range = filters['total_beast_cultivation_range']
                if isinstance(beast_range, (tuple, list)) and len(beast_range) == 2:
                    beast_min, beast_max = beast_range
                    filtered_data = filtered_data[
                        (filtered_data['total_beast_cultivation'] >= beast_min) & 
                        (filtered_data['total_beast_cultivation'] <= beast_max)
                    ]
            
            # å¸ˆé—¨æŠ€èƒ½å¹³å‡å€¼è¿‡æ»¤
            if 'avg_school_skills_range' in filters and filters['avg_school_skills_range'] is not None:
                skills_range = filters['avg_school_skills_range']
                if isinstance(skills_range, (tuple, list)) and len(skills_range) == 2:
                    skills_min, skills_max = skills_range
                    filtered_data = filtered_data[
                        (filtered_data['avg_school_skills'] >= skills_min) & 
                        (filtered_data['avg_school_skills'] <= skills_max)
                    ]
            
            # ä»·æ ¼è¿‡æ»¤
            if 'price_range' in filters and filters['price_range'] is not None:
                price_range = filters['price_range']
                if isinstance(price_range, (tuple, list)) and len(price_range) == 2:
                    price_min, price_max = price_range
                    filtered_data = filtered_data[
                        (filtered_data['price'] >= price_min) & 
                        (filtered_data['price'] <= price_max)
                    ]
            
            # å…¶ä»–èŒƒå›´è¿‡æ»¤æ¡ä»¶ï¼ˆé€šç”¨å¤„ç†ï¼‰
            range_keys = ['total_cultivation_range', 'total_beast_cultivation_range', 
                         'avg_school_skills_range', 'level_range', 'price_range']
            for key, value in filters.items():
                if key not in range_keys and key in filtered_data.columns:
                    if isinstance(value, (list, tuple)) and len(value) == 2:
                        # èŒƒå›´è¿‡æ»¤
                        filtered_data = filtered_data[
                            (filtered_data[key] >= value[0]) & 
                            (filtered_data[key] <= value[1])
                        ]
                    else:
                        # ç²¾ç¡®åŒ¹é…
                        filtered_data = filtered_data[filtered_data[key] == value]
            
            return filtered_data
        
        return market_data
    
    @classmethod
    def clear_cache(cls):
        """æ¸…ç©ºå•ä¾‹å®ä¾‹çš„ç¼“å­˜"""
        with cls._lock:
            if cls._instance and hasattr(cls._instance, 'market_data'):
                cls._instance.market_data = pd.DataFrame()
                cls._instance._data_loaded = False
                cls._instance._last_refresh_time = None
                print("å·²æ¸…ç©ºå¸‚åœºæ•°æ®ç¼“å­˜")
    
    @classmethod
    def get_cache_status(cls) -> Dict[str, Any]:
        """è·å–å•ä¾‹å®ä¾‹çš„ç¼“å­˜çŠ¶æ€"""
        with cls._lock:
            if cls._instance:
                return {
                    'data_loaded': getattr(cls._instance, '_data_loaded', False),
                    'last_refresh_time': getattr(cls._instance, '_last_refresh_time', None),
                    'cache_expired': cls._instance._is_cache_expired() if hasattr(cls._instance, '_is_cache_expired') else True,
                    'data_size': len(cls._instance.market_data) if hasattr(cls._instance, 'market_data') and not cls._instance.market_data.empty else 0,
                    'data_source': 'MySQL (empty roles)'
                }
            else:
                return {
                    'data_loaded': False,
                    'last_refresh_time': None,
                    'cache_expired': True,
                    'data_size': 0,
                    'data_source': 'MySQL (empty roles)'
                }
    
    def clear_instance_cache(self):
        """æ¸…ç©ºå½“å‰å®ä¾‹çš„ç¼“å­˜"""
        self.market_data = pd.DataFrame()
        self._data_loaded = False
        self._last_refresh_time = None
        print("å·²æ¸…ç©ºå¸‚åœºæ•°æ®ç¼“å­˜")
    
    def set_cache_expiry(self, hours: float):
        """è®¾ç½®ç¼“å­˜è¿‡æœŸæ—¶é—´"""
        self._cache_expiry_hours = hours
        print(f"ç¼“å­˜è¿‡æœŸæ—¶é—´å·²è®¾ç½®ä¸º {hours} å°æ—¶")
    
    def get_cache_info(self) -> Dict[str, Any]:
        """è·å–å½“å‰å®ä¾‹çš„ç¼“å­˜ä¿¡æ¯ï¼ˆåŒ…æ‹¬Flask-Cachingä¿¡æ¯ï¼‰"""
        info = {
            'data_loaded': self._data_loaded,
            'last_refresh_time': self._last_refresh_time,
            'cache_expired': self._is_cache_expired(),
            'data_size': len(self.market_data) if not self.market_data.empty else 0,
            'cache_expiry_hours': self._cache_expiry_hours,
            'data_source': 'MySQL (empty roles)',
            'cache_available': False,
            'cache_type': None,
            'cache_config': {}
        }
        
        # æ·»åŠ Flask-Cachingä¿¡æ¯
        cache = self._get_cache()
        if cache:
            info['cache_available'] = True
            try:
                # è·å–ç¼“å­˜é…ç½®ä¿¡æ¯
                if hasattr(cache, 'config') and cache.config:
                    cache_config = cache.config
                elif hasattr(current_app, 'config'):
                    cache_config = current_app.config
                else:
                    cache_config = {}
                
                info['cache_type'] = cache_config.get('CACHE_TYPE', 'Unknown')
                info['cache_config'] = {
                    'cache_type': cache_config.get('CACHE_TYPE'),
                    'default_timeout': cache_config.get('CACHE_DEFAULT_TIMEOUT'),
                    'key_prefix': cache_config.get('CACHE_KEY_PREFIX'),
                }
                
                # å¦‚æœæ˜¯Redisç¼“å­˜ï¼Œæ·»åŠ Redisç‰¹å®šä¿¡æ¯
                if info['cache_type'] == 'RedisCache':
                    info['cache_config'].update({
                        'redis_host': cache_config.get('CACHE_REDIS_HOST'),
                        'redis_port': cache_config.get('CACHE_REDIS_PORT'),
                        'redis_db': cache_config.get('CACHE_REDIS_DB')
                    })
                        
            except Exception as e:
                info['cache_error'] = str(e)
        
        return info

    def _get_full_cached_data(self) -> Optional[pd.DataFrame]:
        """ä»Redisè·å–å…¨é‡ç¼“å­˜æ•°æ® - æ”¯æŒåˆ†å—å­˜å‚¨"""
        try:
            from src.utils.redis_cache import get_redis_cache
            redis_cache = get_redis_cache()
            
            if not redis_cache or not redis_cache.is_available():
                print("Redisä¸å¯ç”¨ï¼Œå°è¯•å¤‡ç”¨æ–¹æ¡ˆ")
                return self._fallback_get_full_cached_data()
            
            # ä½¿ç”¨å›ºå®šçš„å…¨é‡ç¼“å­˜é”®
            full_cache_key = "market_data_full_empty_roles"
            
            print("å°è¯•ä»Redisåˆ†å—ç¼“å­˜è·å–å…¨é‡æ•°æ®...")
            
            # å°è¯•è·å–åˆ†å—æ•°æ®
            cached_data = redis_cache.get_chunked_data(full_cache_key)
            
            if cached_data is not None and not cached_data.empty:
                print(f"ä»Redisåˆ†å—ç¼“å­˜è·å–æ•°æ®æˆåŠŸ: {len(cached_data)} æ¡")
                return cached_data
            else:
                print("Redisåˆ†å—ç¼“å­˜æœªå‘½ä¸­ï¼Œå°è¯•å¤‡ç”¨æ–¹æ¡ˆ")
                return self._fallback_get_full_cached_data()
                
        except Exception as e:
            self.logger.warning(f"è·å–Redisåˆ†å—ç¼“å­˜å¤±è´¥: {str(e)}")
            print("è·å–åˆ†å—ç¼“å­˜å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ¡ˆ")
            return self._fallback_get_full_cached_data()

    def _fallback_get_full_cached_data(self) -> Optional[pd.DataFrame]:
        """å¤‡ç”¨è·å–ç¼“å­˜æ–¹æ¡ˆ - ä½¿ç”¨SharedCacheManager"""
        try:
            from src.utils.shared_cache_manager import get_shared_cache_manager
            cache_manager = get_shared_cache_manager()
            
            if not cache_manager.is_available():
                return None
            
            # ä½¿ç”¨å›ºå®šçš„å…¨é‡ç¼“å­˜é”®
            full_cache_key = "market_data_full_empty_roles"
            cached_data = cache_manager.get_role_cache(full_cache_key)
            
            if cached_data is not None:
                print(f"ä»Rediså…¨é‡ç¼“å­˜è·å–æ•°æ®ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰: {len(cached_data)} æ¡")
                return cached_data
            else:
                print("Rediså…¨é‡ç¼“å­˜æœªå‘½ä¸­")
                return None
                
        except Exception as e:
            self.logger.warning(f"å¤‡ç”¨è·å–ç¼“å­˜æ–¹æ¡ˆå¤±è´¥: {str(e)}")
            return None

    def _set_full_cached_data(self, data: pd.DataFrame) -> bool:
        """å°†å…¨é‡æ•°æ®ç¼“å­˜åˆ°Redis - ä½¿ç”¨åˆ†å—å­˜å‚¨å’ŒPipelineä¼˜åŒ–"""
        try:
            from src.utils.redis_cache import get_redis_cache
            redis_cache = get_redis_cache()
            
            if not redis_cache or not redis_cache.is_available():
                print("Redisä¸å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨SharedCacheManager")
                return self._fallback_set_full_cached_data(data)
            
            # ä½¿ç”¨å›ºå®šçš„å…¨é‡ç¼“å­˜é”®
            full_cache_key = "market_data_full_empty_roles"
            
            # è®¾ç½®è¾ƒé•¿çš„ç¼“å­˜æ—¶é—´ï¼ˆ24å°æ—¶ï¼‰
            cache_hours = 24
            ttl_seconds = cache_hours * 3600
            
            print(f"å¼€å§‹ä½¿ç”¨åˆ†å—å­˜å‚¨å…¨é‡æ•°æ®: {len(data)} æ¡è®°å½•")
            
            # æ ¹æ®æ•°æ®å¤§å°åŠ¨æ€è°ƒæ•´å—å¤§å°
            if len(data) > 10000:
                chunk_size = 2000  # å¤§æ•°æ®é›†ä½¿ç”¨è¾ƒå¤§çš„å—
            elif len(data) > 5000:
                chunk_size = 1000  # ä¸­ç­‰æ•°æ®é›†
            else:
                chunk_size = 500   # å°æ•°æ®é›†
            
            # ä½¿ç”¨åˆ†å—å­˜å‚¨
            success = redis_cache.set_chunked_data(
                base_key=full_cache_key,
                data=data,
                chunk_size=chunk_size,
                ttl=ttl_seconds
            )
            
            if success:
                print(f"å…¨é‡æ•°æ®å·²åˆ†å—ç¼“å­˜åˆ°Redisï¼Œç¼“å­˜æ—¶é—´: {cache_hours}å°æ—¶ï¼Œå—å¤§å°: {chunk_size}")
                return True
            else:
                print("Redisåˆ†å—ç¼“å­˜è®¾ç½®å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ¡ˆ")
                return self._fallback_set_full_cached_data(data)
                
        except Exception as e:
            self.logger.warning(f"è®¾ç½®Rediså…¨é‡ç¼“å­˜å¤±è´¥: {str(e)}")
            print("Redisç¼“å­˜å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ¡ˆ")
            return self._fallback_set_full_cached_data(data)

    def _fallback_set_full_cached_data(self, data: pd.DataFrame) -> bool:
        """å¤‡ç”¨ç¼“å­˜æ–¹æ¡ˆ - ä½¿ç”¨SharedCacheManager"""
        try:
            from src.utils.shared_cache_manager import get_shared_cache_manager
            cache_manager = get_shared_cache_manager()
            
            if not cache_manager.is_available():
                return False
            
            # ä½¿ç”¨å›ºå®šçš„å…¨é‡ç¼“å­˜é”®
            full_cache_key = "market_data_full_empty_roles"
            
            # è®¾ç½®è¾ƒé•¿çš„ç¼“å­˜æ—¶é—´ï¼ˆ24å°æ—¶ï¼‰
            cache_hours = 24
            success = cache_manager.set_role_cache(
                cache_key=full_cache_key,
                data=data,
                cache_type='market_analysis',
                ttl_hours=cache_hours
            )
            
            if success:
                print(f"å…¨é‡æ•°æ®å·²ç¼“å­˜åˆ°Redisï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰ï¼Œç¼“å­˜æ—¶é—´: {cache_hours}å°æ—¶")
                return True
            else:
                print("å¤‡ç”¨Redisç¼“å­˜è®¾ç½®ä¹Ÿå¤±è´¥")
                return False
                
        except Exception as e:
            self.logger.warning(f"å¤‡ç”¨ç¼“å­˜æ–¹æ¡ˆä¹Ÿå¤±è´¥: {str(e)}")
            return False

    def _apply_filters(self, data: pd.DataFrame, filters: Optional[Dict[str, Any]], max_records: int) -> pd.DataFrame:
        """å¯¹æ•°æ®åº”ç”¨ç­›é€‰æ¡ä»¶"""
        if data.empty:
            return data
        
        try:
            filtered_data = data.copy()
            
            # åº”ç”¨ç­›é€‰æ¡ä»¶
            if filters:
                if 'level_min' in filters and 'level' in filtered_data.columns:
                    filtered_data = filtered_data[filtered_data['level'] >= filters['level_min']]
                
                if 'level_max' in filters and 'level' in filtered_data.columns:
                    filtered_data = filtered_data[filtered_data['level'] <= filters['level_max']]
                
                if 'price_min' in filters and 'price' in filtered_data.columns:
                    filtered_data = filtered_data[filtered_data['price'] >= filters['price_min']]
                
                if 'price_max' in filters and 'price' in filtered_data.columns:
                    filtered_data = filtered_data[filtered_data['price'] <= filters['price_max']]
                
                if 'school' in filters and 'school' in filtered_data.columns:
                    filtered_data = filtered_data[filtered_data['school'] == filters['school']]
                
                if 'serverid' in filters and 'serverid' in filtered_data.columns:
                    filtered_data = filtered_data[filtered_data['serverid'] == filters['serverid']]
            
            # åº”ç”¨è®°å½•æ•°é™åˆ¶
            if len(filtered_data) > max_records:
                # æŒ‰ä»·æ ¼æ’åºåå–å‰Næ¡
                if 'price' in filtered_data.columns:
                    filtered_data = filtered_data.sort_values('price').head(max_records)
                else:
                    filtered_data = filtered_data.head(max_records)
            
            return filtered_data
            
        except Exception as e:
            self.logger.warning(f"åº”ç”¨ç­›é€‰æ¡ä»¶å¤±è´¥: {str(e)}")
            return data.head(max_records) if len(data) > max_records else data

    def refresh_full_cache(self) -> bool:
        """
        æ‰‹åŠ¨åˆ·æ–°å…¨é‡ç¼“å­˜ - ä»MySQLé‡æ–°åŠ è½½æ‰€æœ‰emptyè§’è‰²æ•°æ®åˆ°Redis
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸåˆ·æ–°ç¼“å­˜
        """
        try:
            print("å¼€å§‹æ‰‹åŠ¨åˆ·æ–°å…¨é‡ç¼“å­˜...")
            
            # å¼ºåˆ¶ä»æ•°æ®åº“åˆ·æ–°ï¼Œä¸ä½¿ç”¨ç°æœ‰ç¼“å­˜
            self.refresh_market_data(
                filters=None, 
                max_records=999999,  # è®¾ç½®å¾ˆå¤§çš„å€¼ä»¥è·å–å…¨éƒ¨æ•°æ®
                use_cache=True,
                force_refresh=True
            )
            
            print("å…¨é‡ç¼“å­˜åˆ·æ–°å®Œæˆ")
            return True
            
        except Exception as e:
            self.logger.error(f"åˆ·æ–°å…¨é‡ç¼“å­˜å¤±è´¥: {e}")
            return False

    def get_cache_status(self) -> Dict[str, Any]:
        """
        è·å–ç¼“å­˜çŠ¶æ€ä¿¡æ¯ - æ”¯æŒåˆ†å—ç¼“å­˜
        
        Returns:
            Dict: ç¼“å­˜çŠ¶æ€ä¿¡æ¯
        """
        try:
            from src.utils.redis_cache import get_redis_cache
            redis_cache = get_redis_cache()
            
            status = {
                'redis_available': False,
                'full_cache_exists': False,
                'full_cache_size': 0,
                'full_cache_last_update': None,
                'cache_type': 'unknown',
                'chunk_info': {}
            }
            
            # æ£€æŸ¥Redisè¿æ¥
            if redis_cache and redis_cache.is_available():
                status['redis_available'] = True
                
                # æ£€æŸ¥åˆ†å—ç¼“å­˜
                full_cache_key = "market_data_full_empty_roles"
                
                # å…ˆæ£€æŸ¥åˆ†å—ç¼“å­˜çš„å…ƒæ•°æ®
                try:
                    metadata = redis_cache.get(f"{full_cache_key}:meta")
                    if metadata:
                        status['full_cache_exists'] = True
                        status['cache_type'] = 'chunked'
                        status['full_cache_size'] = metadata.get('total_rows', 0)
                        status['full_cache_last_update'] = metadata.get('created_at')
                        status['chunk_info'] = {
                            'total_chunks': metadata.get('total_chunks', 0),
                            'chunk_size': metadata.get('chunk_size', 0),
                            'columns_count': len(metadata.get('columns', []))
                        }
                        
                        # éªŒè¯æ‰€æœ‰åˆ†å—æ˜¯å¦å®Œæ•´
                        total_chunks = metadata.get('total_chunks', 0)
                        if total_chunks > 0:
                            chunk_keys = [f"{full_cache_key}:chunk_{i}" for i in range(total_chunks)]
                            existing_chunks = redis_cache.get_batch(chunk_keys)
                            status['chunk_info']['existing_chunks'] = len(existing_chunks)
                            status['chunk_info']['is_complete'] = len(existing_chunks) == total_chunks
                        
                        return status
                except Exception as e:
                    self.logger.debug(f"æ£€æŸ¥åˆ†å—ç¼“å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
                
                # å¦‚æœåˆ†å—ç¼“å­˜ä¸å­˜åœ¨ï¼Œæ£€æŸ¥å¤‡ç”¨ç¼“å­˜
                try:
                    from src.utils.shared_cache_manager import get_shared_cache_manager
                    cache_manager = get_shared_cache_manager()
                    
                    if cache_manager.is_available():
                        cached_data = cache_manager.get_role_cache(full_cache_key)
                        if cached_data is not None and not cached_data.empty:
                            status['full_cache_exists'] = True
                            status['cache_type'] = 'legacy'
                            status['full_cache_size'] = len(cached_data)
                            
                            # å°è¯•è·å–ç¼“å­˜æ—¶é—´ä¿¡æ¯
                            try:
                                cache_info = cache_manager.redis_cache.client.hgetall(
                                    cache_manager.redis_cache._make_key(f"role_data:{full_cache_key}:meta")
                                )
                                if cache_info and b'timestamp' in cache_info:
                                    status['full_cache_last_update'] = cache_info[b'timestamp'].decode('utf-8')
                            except:
                                pass
                except Exception as e:
                    self.logger.debug(f"æ£€æŸ¥å¤‡ç”¨ç¼“å­˜å¤±è´¥: {e}")
            
            return status
            
        except Exception as e:
            self.logger.error(f"è·å–ç¼“å­˜çŠ¶æ€å¤±è´¥: {e}")
            return {
                'redis_available': False,
                'full_cache_exists': False,
                'full_cache_size': 0,
                'full_cache_last_update': None,
                'cache_type': 'error',
                'error': str(e)
            }

    def _process_batch_data(self, batch_rows: List, batch_columns: List, batch_num: int) -> List[Dict]:
        """
        æ‰¹é‡å¤„ç†æ•°æ® - ä¼˜åŒ–ç‰¹å¾æå–æ€§èƒ½
        
        Args:
            batch_rows: æ•°æ®åº“æŸ¥è¯¢ç»“æœè¡Œ
            batch_columns: åˆ—ååˆ—è¡¨
            batch_num: æ‰¹æ¬¡å·
            
        Returns:
            List[Dict]: å¤„ç†åçš„ç‰¹å¾æ•°æ®åˆ—è¡¨
        """
        try:
            batch_data = []
            
            # é¢„å…ˆè½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨ï¼Œå‡å°‘é‡å¤æ“ä½œ
            role_data_list = [dict(zip(batch_columns, row)) for row in batch_rows]
            
            # æ‰¹é‡æå–ç‰¹å¾
            for i, role_data in enumerate(role_data_list):
                try:
                    # æå–ç‰¹å¾
                    features = self.feature_extractor.extract_features(role_data)
                    
                    # æ·»åŠ åŸºæœ¬ä¿¡æ¯
                    features.update({
                        'eid': role_data.get('eid', ''),
                        'price': role_data.get('price', 0),
                        'school': role_data.get('school', 0),
                        'serverid': role_data.get('serverid', 0)
                    })
                    
                    batch_data.append(features)
                    
                except Exception as e:
                    self.logger.warning(f"å¤„ç†ç¬¬{batch_num}æ‰¹ç¬¬{i+1}æ¡æ•°æ®æ—¶å‡ºé”™: {e}")
                    continue
            
            print(f"ç¬¬{batch_num}æ‰¹å¤„ç†å®Œæˆ: {len(batch_data)}/{len(batch_rows)} æ¡æœ‰æ•ˆæ•°æ®")
            return batch_data
            
        except Exception as e:
            self.logger.error(f"æ‰¹é‡å¤„ç†ç¬¬{batch_num}æ‰¹æ•°æ®å¤±è´¥: {e}")
            return []

    def _optimize_database_indexes(self):
        """
        ä¼˜åŒ–æ•°æ®åº“ç´¢å¼•å»ºè®® - ä»…è¾“å‡ºå»ºè®®ï¼Œä¸æ‰§è¡Œ
        """
        index_suggestions = [
            "CREATE INDEX idx_roles_type_price ON roles(role_type, price);",
            "CREATE INDEX idx_roles_eid ON roles(eid);",
            "CREATE INDEX idx_large_equip_eid ON large_equip_desc_data(eid);",
            "CREATE INDEX idx_roles_level ON roles(level);",
            "CREATE INDEX idx_roles_serverid ON roles(serverid);",
            "CREATE INDEX idx_roles_school ON roles(school);"
        ]
        
        print("ğŸ” æ•°æ®åº“ç´¢å¼•ä¼˜åŒ–å»ºè®®:")
        for suggestion in index_suggestions:
            print(f"  {suggestion}")
        print("ğŸ“ è¯·åœ¨æ•°æ®åº“ä¸­æ‰‹åŠ¨æ‰§è¡Œè¿™äº›ç´¢å¼•åˆ›å»ºè¯­å¥ä»¥æå‡æŸ¥è¯¢æ€§èƒ½")

    def _get_optimized_connection_config(self, db_config: str) -> Dict:
        """
        è·å–ä¼˜åŒ–çš„æ•°æ®åº“è¿æ¥é…ç½®
        
        Args:
            db_config: åŸå§‹æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²
            
        Returns:
            Dict: ä¼˜åŒ–çš„è¿æ¥å‚æ•°
        """
        return {
            'pool_size': 10,           # è¿æ¥æ± å¤§å°
            'max_overflow': 20,        # æœ€å¤§æº¢å‡ºè¿æ¥æ•°
            'pool_timeout': 30,        # è·å–è¿æ¥è¶…æ—¶æ—¶é—´
            'pool_recycle': 3600,      # è¿æ¥å›æ”¶æ—¶é—´ï¼ˆ1å°æ—¶ï¼‰
            'pool_pre_ping': True,     # è¿æ¥å‰pingæµ‹è¯•
            'echo': False,             # ä¸æ‰“å°SQL
            'connect_args': {
                'charset': 'utf8mb4',
                'connect_timeout': 60,
                'read_timeout': 300,
                'write_timeout': 300,
                'autocommit': True
            }
        }

    def _process_batch_data_parallel(self, batch_rows: List, batch_columns: List, batch_num: int, max_workers: int = 4) -> List[Dict]:
        """
        å¹¶è¡Œæ‰¹é‡å¤„ç†æ•°æ® - ä½¿ç”¨å¤šçº¿ç¨‹ä¼˜åŒ–ç‰¹å¾æå–æ€§èƒ½
        
        Args:
            batch_rows: æ•°æ®åº“æŸ¥è¯¢ç»“æœè¡Œ
            batch_columns: åˆ—ååˆ—è¡¨
            batch_num: æ‰¹æ¬¡å·
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
            
        Returns:
            List[Dict]: å¤„ç†åçš„ç‰¹å¾æ•°æ®åˆ—è¡¨
        """
        try:
            if len(batch_rows) < 100:
                # å°æ‰¹æ¬¡æ•°æ®ç›´æ¥ä¸²è¡Œå¤„ç†ï¼Œé¿å…çº¿ç¨‹å¼€é”€
                return self._process_batch_data(batch_rows, batch_columns, batch_num)
            
            # é¢„å…ˆè½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            role_data_list = [dict(zip(batch_columns, row)) for row in batch_rows]
            
            # åˆ†å‰²æ•°æ®ä¾›å¤šçº¿ç¨‹å¤„ç†
            chunk_size = max(1, len(role_data_list) // max_workers)
            chunks = [role_data_list[i:i + chunk_size] for i in range(0, len(role_data_list), chunk_size)]
            
            batch_data = []
            
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤ä»»åŠ¡
                future_to_chunk = {
                    executor.submit(self._process_data_chunk, chunk, f"{batch_num}-{i+1}"): i 
                    for i, chunk in enumerate(chunks)
                }
                
                # æ”¶é›†ç»“æœ
                for future in concurrent.futures.as_completed(future_to_chunk):
                    chunk_index = future_to_chunk[future]
                    try:
                        chunk_result = future.result()
                        batch_data.extend(chunk_result)
                    except Exception as e:
                        self.logger.error(f"å¤„ç†ç¬¬{batch_num}æ‰¹ç¬¬{chunk_index+1}å—æ•°æ®å¤±è´¥: {e}")
                        continue
            
            print(f"ç¬¬{batch_num}æ‰¹å¹¶è¡Œå¤„ç†å®Œæˆ: {len(batch_data)}/{len(batch_rows)} æ¡æœ‰æ•ˆæ•°æ®")
            return batch_data
            
        except Exception as e:
            self.logger.error(f"å¹¶è¡Œå¤„ç†ç¬¬{batch_num}æ‰¹æ•°æ®å¤±è´¥: {e}")
            # é™çº§åˆ°ä¸²è¡Œå¤„ç†
            return self._process_batch_data(batch_rows, batch_columns, batch_num)

    def _process_data_chunk(self, role_data_chunk: List[Dict], chunk_id: str) -> List[Dict]:
        """
        å¤„ç†æ•°æ®å— - çº¿ç¨‹å®‰å…¨çš„ç‰¹å¾æå–
        
        Args:
            role_data_chunk: è§’è‰²æ•°æ®å—
            chunk_id: å—æ ‡è¯†
            
        Returns:
            List[Dict]: å¤„ç†åçš„ç‰¹å¾æ•°æ®
        """
        try:
            chunk_data = []
            
            for i, role_data in enumerate(role_data_chunk):
                try:
                    # æå–ç‰¹å¾
                    features = self.feature_extractor.extract_features(role_data)
                    
                    # æ·»åŠ åŸºæœ¬ä¿¡æ¯
                    features.update({
                        'eid': role_data.get('eid', ''),
                        'price': role_data.get('price', 0),
                        'school': role_data.get('school', 0),
                        'serverid': role_data.get('serverid', 0)
                    })
                    
                    chunk_data.append(features)
                    
                except Exception as e:
                    self.logger.warning(f"å¤„ç†å—{chunk_id}ç¬¬{i+1}æ¡æ•°æ®æ—¶å‡ºé”™: {e}")
                    continue
            
            return chunk_data
            
        except Exception as e:
            self.logger.error(f"å¤„ç†æ•°æ®å—{chunk_id}å¤±è´¥: {e}")
            return []

    def get_performance_stats(self) -> Dict[str, Any]:
        """
        è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict: æ€§èƒ½ç»Ÿè®¡æ•°æ®
        """
        try:
            stats = {
                'data_loaded': self._data_loaded,
                'last_refresh_time': self._last_refresh_time.isoformat() if self._last_refresh_time else None,
                'data_count': len(self.market_data) if not self.market_data.empty else 0,
                'memory_usage_mb': self.market_data.memory_usage(deep=True).sum() / 1024 / 1024 if not self.market_data.empty else 0,
                'cache_expiry_hours': self._cache_expiry_hours,
                'optimization_suggestions': self._get_optimization_suggestions()
            }
            
            # æ·»åŠ æ•°æ®åº“è¿æ¥æ± çŠ¶æ€
            try:
                from src.app import create_app
                app = create_app()
                with app.app_context():
                    db_config = app.config.get('SQLALCHEMY_DATABASE_URI')
                    if db_config:
                        stats['database_config'] = {
                            'connection_pool_enabled': True,
                            'suggested_indexes': self._get_index_suggestions()
                        }
            except:
                pass
                
            return stats
            
        except Exception as e:
            self.logger.error(f"è·å–æ€§èƒ½ç»Ÿè®¡å¤±è´¥: {e}")
            return {'error': str(e)}

    def _get_optimization_suggestions(self) -> List[str]:
        """è·å–ä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        if hasattr(self, 'market_data') and not self.market_data.empty:
            data_count = len(self.market_data)
            
            if data_count > 100000:
                suggestions.append("æ•°æ®é‡è¾ƒå¤§ï¼Œå»ºè®®ä½¿ç”¨åˆ†å—ç¼“å­˜ç­–ç•¥")
            
            if data_count > 50000:
                suggestions.append("å»ºè®®åœ¨æ•°æ®åº“ä¸­åˆ›å»ºå¤åˆç´¢å¼•ä»¥æå‡æŸ¥è¯¢æ€§èƒ½")
                
            memory_mb = self.market_data.memory_usage(deep=True).sum() / 1024 / 1024
            if memory_mb > 500:
                suggestions.append("å†…å­˜ä½¿ç”¨è¾ƒé«˜ï¼Œå»ºè®®ä¼˜åŒ–æ•°æ®ç±»å‹æˆ–ä½¿ç”¨åˆ†æ‰¹å¤„ç†")
        
        suggestions.extend([
            "å»ºè®®åœ¨æ•°æ®åº“ä¸­åˆ›å»ºæ¨èçš„ç´¢å¼•ä»¥æå‡æŸ¥è¯¢æ€§èƒ½",
            "ä½¿ç”¨Redisåˆ†å—ç¼“å­˜å¯ä»¥æ˜¾è‘—æå‡è¯»å–é€Ÿåº¦",
            "å¹¶è¡Œå¤„ç†å¯ä»¥åŠ é€Ÿç‰¹å¾æå–è¿‡ç¨‹"
        ])
        
        return suggestions

    def _get_index_suggestions(self) -> List[str]:
        """è·å–ç´¢å¼•å»ºè®®"""
        return [
            "CREATE INDEX idx_roles_type_price ON roles(role_type, price);",
            "CREATE INDEX idx_roles_eid ON roles(eid);", 
            "CREATE INDEX idx_large_equip_eid ON large_equip_desc_data(eid);",
            "CREATE INDEX idx_roles_level ON roles(level);",
            "CREATE INDEX idx_roles_serverid ON roles(serverid);"
        ]