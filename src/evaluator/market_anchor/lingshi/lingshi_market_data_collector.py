from datetime import datetime
from typing import Dict, Any, List, Optional, Union, Tuple
import logging
import numpy as np
import pandas as pd

# å¯¼å…¥çµé¥°ä¼˜å…ˆçº§é…ç½®
from src.evaluator.constants.lingshi_priorities import (
    RING_EARRING_PRIORITY, BRACELET_ACCESSORY_PRIORITY,
    get_priority_by_attr_name
)

from src.evaluator.feature_extractor.lingshi_feature_extractor import LingshiFeatureExtractor
from src.database import db
from src.models.equipment import Equipment
from sqlalchemy import and_, or_, func, text



class LingshiMarketDataCollector:
    """çµé¥°å¸‚åœºæ•°æ®é‡‡é›†å™¨ - ä»æ•°æ®åº“ä¸­è·å–å’Œå¤„ç†çµé¥°å¸‚åœºæ•°æ®"""

    def __init__(self):
        """
        åˆå§‹åŒ–çµé¥°å¸‚åœºæ•°æ®é‡‡é›†å™¨
        """
        self.feature_extractor = LingshiFeatureExtractor()
        self.logger = logging.getLogger(__name__)
        self.target_features = None  # ä¿å­˜ç›®æ ‡ç‰¹å¾ï¼Œç”¨äºä¼ é€’target_match_attrs
        
        # è·å–è£…å¤‡æ•°æ®é‡‡é›†å™¨å®ä¾‹ï¼Œå…±äº«ç¼“å­˜
        self.equip_collector = None
        self._init_equip_collector()
        
        # ç¼“å­˜è¿‡æ»¤åçš„çµé¥°æ•°æ®ï¼Œé¿å…é‡å¤è¯»å–å’Œè¿‡æ»¤
        self._cached_lingshi_data = None
        self._cache_timestamp = None
        
        # è®¢é˜…è£…å¤‡æ•°æ®æ›´æ–°æ¶ˆæ¯
        self._setup_equipment_update_subscription()

        print(f"çµé¥°æ•°æ®é‡‡é›†å™¨åˆå§‹åŒ–ï¼Œä½¿ç”¨MySQLæ•°æ®åº“")
    
    def _init_equip_collector(self):
        """åˆå§‹åŒ–è£…å¤‡æ•°æ®é‡‡é›†å™¨å®ä¾‹"""
        try:
            from src.evaluator.market_anchor.equip.equip_market_data_collector import EquipMarketDataCollector
            self.equip_collector = EquipMarketDataCollector.get_instance()
            print(" æˆåŠŸè·å–è£…å¤‡æ•°æ®é‡‡é›†å™¨å®ä¾‹ï¼Œå¯å…±äº«ç¼“å­˜")
        except Exception as e:
            self.logger.warning(f"è·å–è£…å¤‡æ•°æ®é‡‡é›†å™¨å®ä¾‹å¤±è´¥: {e}")
            print(f" æ— æ³•å…±äº«è£…å¤‡æ•°æ®é‡‡é›†å™¨ç¼“å­˜: {e}")
    
    def _setup_equipment_update_subscription(self):
        """è®¾ç½®è£…å¤‡æ•°æ®æ›´æ–°æ¶ˆæ¯è®¢é˜…"""
        try:
            from src.utils.redis_pubsub import get_redis_pubsub, Channel
            
            # è·å–Rediså‘å¸ƒè®¢é˜…å®ä¾‹
            redis_pubsub = get_redis_pubsub()
            
            # è®¢é˜…è£…å¤‡æ•°æ®æ›´æ–°æ¶ˆæ¯
            success = redis_pubsub.subscribe(
                Channel.EQUIPMENT_UPDATES,
                self.handle_equipment_update_message
            )
            
            if success:
                self.logger.info("ğŸ“¨ çµé¥°é‡‡é›†å™¨å·²è®¢é˜…è£…å¤‡æ•°æ®æ›´æ–°æ¶ˆæ¯")
                print(" çµé¥°é‡‡é›†å™¨å·²è®¢é˜…è£…å¤‡æ•°æ®æ›´æ–°æ¶ˆæ¯")
            else:
                self.logger.warning("ğŸ“¨ çµé¥°é‡‡é›†å™¨è®¢é˜…è£…å¤‡æ•°æ®æ›´æ–°æ¶ˆæ¯å¤±è´¥")
                print(" çµé¥°é‡‡é›†å™¨è®¢é˜…è£…å¤‡æ•°æ®æ›´æ–°æ¶ˆæ¯å¤±è´¥")
                
        except Exception as e:
            self.logger.error(f"è®¾ç½®è£…å¤‡æ•°æ®æ›´æ–°è®¢é˜…å¤±è´¥: {e}")
            print(f" è®¾ç½®è£…å¤‡æ•°æ®æ›´æ–°è®¢é˜…å¤±è´¥: {e}")
    
    def _get_shared_cache_data(self, kindid: Optional[int] = None) -> Optional[pd.DataFrame]:
        """
        ä»è£…å¤‡æ•°æ®é‡‡é›†å™¨è·å–å…±äº«ç¼“å­˜æ•°æ®ï¼Œä¼˜å…ˆä½¿ç”¨å®ä¾‹ç¼“å­˜
        
        Args:
            kindid: çµé¥°ç±»å‹IDç­›é€‰ (61:æˆ’æŒ‡, 62:è€³é¥°, 63:æ‰‹é•¯, 64:ä½©é¥°)
            
        Returns:
            è¿‡æ»¤åçš„çµé¥°æ•°æ®DataFrameï¼Œå¦‚æœç¼“å­˜ä¸å¯ç”¨åˆ™è¿”å›None
        """
        if not self.equip_collector:
            return None
            
        try:
            # æ£€æŸ¥å®ä¾‹ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
            if self._cached_lingshi_data is not None:
                print(f" ä½¿ç”¨å®ä¾‹ç¼“å­˜çš„çµé¥°æ•°æ®ï¼Œå…± {len(self._cached_lingshi_data)} æ¡")
                
                # å¦‚æœæŒ‡å®šäº†kindidï¼Œè¿›ä¸€æ­¥è¿‡æ»¤
                if kindid is not None:
                    filtered_data = self._cached_lingshi_data[self._cached_lingshi_data['kindid'] == kindid]
                    if not filtered_data.empty:
                        print(f" æŒ‰kindid={kindid}è¿‡æ»¤åå¾—åˆ° {len(filtered_data)} æ¡çµé¥°æ•°æ®")
                        return filtered_data
                    else:
                        print(f"å®ä¾‹ç¼“å­˜ä¸­æ²¡æœ‰æ‰¾åˆ°kindid={kindid}çš„çµé¥°æ•°æ®")
                        return None
                else:
                    return self._cached_lingshi_data
            
            # å®ä¾‹ç¼“å­˜ä¸ºç©ºï¼Œä»è£…å¤‡æ•°æ®é‡‡é›†å™¨è·å–å…¨é‡ç¼“å­˜
            full_data = self.equip_collector._get_full_data_from_redis()
            
            if full_data is None or full_data.empty:
                print("è£…å¤‡æ•°æ®é‡‡é›†å™¨ç¼“å­˜ä¸ºç©ºï¼Œæ— æ³•å…±äº«")
                return None
            
            # è¿‡æ»¤å‡ºçµé¥°æ•°æ® (kindid: 61-64) å¹¶ä¿å­˜åˆ°å®ä¾‹ç¼“å­˜
            self._cached_lingshi_data = full_data[full_data['kindid'].isin([61, 62, 63, 64])].copy()
            self._cache_timestamp = datetime.now()
            
            if not self._cached_lingshi_data.empty:
                print(f" ä»è£…å¤‡æ•°æ®é‡‡é›†å™¨è·å–å¹¶ç¼“å­˜ {len(self._cached_lingshi_data)} æ¡çµé¥°æ•°æ®")
                
                # å¦‚æœæŒ‡å®šäº†kindidï¼Œè¿›ä¸€æ­¥è¿‡æ»¤
                if kindid is not None:
                    filtered_data = self._cached_lingshi_data[self._cached_lingshi_data['kindid'] == kindid]
                    if not filtered_data.empty:
                        print(f" æŒ‰kindid={kindid}è¿‡æ»¤åå¾—åˆ° {len(filtered_data)} æ¡çµé¥°æ•°æ®")
                        return filtered_data
                    else:
                        print(f"ç¼“å­˜ä¸­æ²¡æœ‰æ‰¾åˆ°kindid={kindid}çš„çµé¥°æ•°æ®")
                        return None
                else:
                    return self._cached_lingshi_data
            else:
                print("è£…å¤‡æ•°æ®é‡‡é›†å™¨ç¼“å­˜ä¸­æ²¡æœ‰æ‰¾åˆ°çµé¥°æ•°æ®")
                return None
                
        except Exception as e:
            self.logger.warning(f"è·å–å…±äº«ç¼“å­˜æ•°æ®å¤±è´¥: {e}")
            print(f" å…±äº«ç¼“å­˜è·å–å¤±è´¥: {e}")
            return None

    def clear_cache(self):
        """æ¸…é™¤å®ä¾‹ç¼“å­˜ï¼Œå¼ºåˆ¶ä¸‹æ¬¡é‡æ–°ä»è£…å¤‡æ•°æ®é‡‡é›†å™¨è·å–æ•°æ®"""
        self._cached_lingshi_data = None
        self._cache_timestamp = None
        print(" å·²æ¸…é™¤çµé¥°æ•°æ®å®ä¾‹ç¼“å­˜")
    
    def force_refresh_cache(self):
        """å¼ºåˆ¶åˆ·æ–°ç¼“å­˜ï¼ŒåŒ…æ‹¬è£…å¤‡æ•°æ®é‡‡é›†å™¨çš„ç¼“å­˜"""
        print(" å¼ºåˆ¶åˆ·æ–°çµé¥°æ•°æ®ç¼“å­˜...")
        
        # 1. æ¸…é™¤å®ä¾‹ç¼“å­˜
        self.clear_cache()
        
        # 2. å¼ºåˆ¶åˆ·æ–°è£…å¤‡æ•°æ®é‡‡é›†å™¨çš„ç¼“å­˜
        if self.equip_collector:
            print(" åˆ·æ–°è£…å¤‡æ•°æ®é‡‡é›†å™¨ç¼“å­˜...")
            success = self.equip_collector.force_refresh_full_cache()
            if success:
                print(" è£…å¤‡æ•°æ®é‡‡é›†å™¨ç¼“å­˜åˆ·æ–°æˆåŠŸ")
            else:
                print(" è£…å¤‡æ•°æ®é‡‡é›†å™¨ç¼“å­˜åˆ·æ–°å¤±è´¥")
        
        # 3. é‡æ–°è·å–æ•°æ®
        print(" é‡æ–°è·å–çµé¥°æ•°æ®...")
        return self._get_shared_cache_data()
    
    def handle_equipment_update_message(self, message: Dict[str, Any]):
        """
        å¤„ç†è£…å¤‡æ•°æ®æ›´æ–°æ¶ˆæ¯ï¼Œè‡ªåŠ¨åŒæ­¥æ–°å¢çš„çµé¥°æ•°æ®
        
        Args:
            message: è£…å¤‡æ•°æ®æ›´æ–°æ¶ˆæ¯
        """
        try:
            message_type = message.get('type')
            action = message.get('action', 'refresh')
            
            self.logger.info(f"ğŸ“¨ çµé¥°é‡‡é›†å™¨æ”¶åˆ°è£…å¤‡æ•°æ®æ›´æ–°æ¶ˆæ¯: {message_type}, æ“ä½œ: {action}")
            
            if message_type == 'equipment_data_saved':
                if action == 'add_dataframe' and 'dataframe' in message:
                    # ç›´æ¥æ›´æ–°çµé¥°ç¼“å­˜ï¼ŒåªåŒæ­¥æ–°å¢çš„çµé¥°æ•°æ®
                    dataframe = message['dataframe']
                    self._update_lingshi_cache_with_dataframe(dataframe)
                else:
                    # æ¸…é™¤å®ä¾‹ç¼“å­˜ï¼Œä¸‹æ¬¡ä¼šè‡ªåŠ¨ä»è£…å¤‡æ•°æ®é‡‡é›†å™¨è·å–æœ€æ–°æ•°æ®
                    self.clear_cache()
                    self.logger.info("ğŸ“¨ å·²æ¸…é™¤çµé¥°æ•°æ®ç¼“å­˜ï¼Œä¸‹æ¬¡å°†è·å–æœ€æ–°æ•°æ®")
                
        except Exception as e:
            self.logger.error(f"âŒ å¤„ç†è£…å¤‡æ•°æ®æ›´æ–°æ¶ˆæ¯å¤±è´¥: {e}")
    
    def _update_lingshi_cache_with_dataframe(self, new_dataframe: pd.DataFrame):
        """
        ç›´æ¥ä½¿ç”¨DataFrameæ›´æ–°çµé¥°ç¼“å­˜ï¼ŒåªåŒæ­¥çµé¥°æ•°æ®
        
        Args:
            new_dataframe: æ–°çš„è£…å¤‡æ•°æ®DataFrame
        """
        try:
            # è¿‡æ»¤å‡ºçµé¥°æ•°æ® (kindid: 61-64)
            lingshi_data = new_dataframe[new_dataframe['kindid'].isin([61, 62, 63, 64])].copy()
            
            if lingshi_data.empty:
                self.logger.info("ğŸ“¨ æ–°å¢æ•°æ®ä¸­æ²¡æœ‰çµé¥°æ•°æ®ï¼Œè·³è¿‡åŒæ­¥")
                return
            
            self.logger.info(f"ğŸ“¨ ä»æ–°å¢æ•°æ®ä¸­æå–åˆ° {len(lingshi_data)} æ¡çµé¥°æ•°æ®")
            
            # æ›´æ–°å®ä¾‹ç¼“å­˜
            if self._cached_lingshi_data is None or self._cached_lingshi_data.empty:
                # å¦‚æœç¼“å­˜ä¸ºç©ºï¼Œç›´æ¥ä½¿ç”¨æ–°æ•°æ®
                self._cached_lingshi_data = lingshi_data
            else:
                # åˆå¹¶æ–°æ•°æ®åˆ°ç°æœ‰ç¼“å­˜
                self._cached_lingshi_data = pd.concat([self._cached_lingshi_data, lingshi_data], ignore_index=True)
                # å»é‡ï¼ˆåŸºäºequip_snï¼‰
                self._cached_lingshi_data = self._cached_lingshi_data.drop_duplicates(subset=['equip_sn'], keep='last')
            
            self._cache_timestamp = datetime.now()
            self.logger.info(f"ğŸ“¨ çµé¥°ç¼“å­˜æ›´æ–°æˆåŠŸï¼Œå½“å‰ç¼“å­˜ {len(self._cached_lingshi_data)} æ¡æ•°æ®")
            
        except Exception as e:
            self.logger.error(f"âŒ æ›´æ–°çµé¥°ç¼“å­˜å¤±è´¥: {e}")
            # å¦‚æœæ›´æ–°å¤±è´¥ï¼Œæ¸…é™¤ç¼“å­˜ä»¥ç¡®ä¿æ•°æ®ä¸€è‡´æ€§
            self.clear_cache()

    def get_market_data(self,
                        kindid: Optional[int] = None,
                        level_range: Optional[Tuple[int, int]] = None,
                        main_attr: Optional[str] = None,
                        attrs: Optional[List[Dict[str, Any]]] = None,
                        is_super_simple: Optional[bool] = None,
                        price_range: Optional[Tuple[float, float]] = None,
                        server: Optional[str] = None,
                        limit: int = 1000,
                        use_shared_cache: bool = True) -> pd.DataFrame:
        """
        è·å–å¸‚åœºçµé¥°æ•°æ®ï¼Œä¼˜å…ˆä»è£…å¤‡æ•°æ®é‡‡é›†å™¨çš„å…±äº«ç¼“å­˜è·å–æ•°æ®

        Args:
            kindid: çµé¥°ç±»å‹IDç­›é€‰ (61:æˆ’æŒ‡, 62:è€³é¥°, 63:æ‰‹é•¯, 64:ä½©é¥°)
            main_attr: ä¸»å±æ€§(damageã€defenseã€magic_damageã€magic_defenseã€fengyinã€anti_fengyinã€speed)
            attrs: é™„åŠ å±æ€§ List[Dict] - é™„åŠ å±æ€§å¯¹è±¡åˆ—è¡¨ï¼Œæœ€å¤š3ä¸ª
                æ¯ä¸ªå¯¹è±¡åŒ…å«:
                - attr_type: str - å±æ€§ç±»å‹ï¼Œå¦‚"ä¼¤å®³"ã€"æ³•æœ¯ä¼¤å®³"
                - attr_value: int - å±æ€§æ•°å€¼
            level_range: ç­‰çº§èŒƒå›´ (min_level, max_level)
            price_range: ä»·æ ¼èŒƒå›´ (min_price, max_price)
            server: æœåŠ¡å™¨ç­›é€‰
            is_super_simple: æ˜¯å¦è¶…çº§ç®€æ˜“ç­›é€‰ (åŸºäºspecial_effect=="[1]"åˆ¤æ–­)
            limit: è¿”å›æ•°æ®æ¡æ•°é™åˆ¶
            use_shared_cache: æ˜¯å¦ä½¿ç”¨å…±äº«ç¼“å­˜

        Returns:
            çµé¥°å¸‚åœºæ•°æ®DataFrame
        """
        try:
            import time
            start_time = time.time()
            
            # ä¼˜å…ˆä»å…±äº«ç¼“å­˜è·å–æ•°æ®
            if use_shared_cache:
                cached_data = self._get_shared_cache_data(kindid)
                
                if cached_data is not None and not cached_data.empty:
                    # å¯¹ç¼“å­˜æ•°æ®è¿›è¡Œè¿›ä¸€æ­¥ç­›é€‰
                    filtered_data = self._filter_cached_data(
                        cached_data,
                        level_range=level_range,
                        main_attr=main_attr,
                        is_super_simple=is_super_simple,
                        price_range=price_range,
                        server=server,
                        limit=limit
                    )
                    
                    # é™„åŠ å±æ€§ç­›é€‰ï¼šæ ¹æ®attr_typeè¿›è¡Œç­›é€‰
                    if attrs is not None and len(attrs) > 0:
                        filtered_data = self._filter_by_attrs(filtered_data, attrs)
                    
                    elapsed_time = time.time() - start_time
                    print(f" ä»å…±äº«ç¼“å­˜è·å–çµé¥°æ•°æ®å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.3f}ç§’ï¼Œè¿”å›: {len(filtered_data)} æ¡æ•°æ®")
                    return filtered_data
            
            # é™çº§åˆ°MySQLæŸ¥è¯¢
            print("ä½¿ç”¨MySQLæ•°æ®åº“æŸ¥è¯¢çµé¥°æ•°æ®ï¼ˆé™çº§æ¨¡å¼ï¼‰...")
            return self._get_market_data_from_mysql(
                kindid=kindid,
                level_range=level_range,
                main_attr=main_attr,
                is_super_simple=is_super_simple,
                price_range=price_range,
                server=server,
                limit=limit,
                attrs=attrs
            )
            
        except Exception as e:
            self.logger.error(f"è·å–çµé¥°å¸‚åœºæ•°æ®å¤±è´¥: {e}")
            print(f"è·å–çµé¥°å¸‚åœºæ•°æ®å¼‚å¸¸: {e}")
            return pd.DataFrame()
    
    def _filter_cached_data(self, cached_data: pd.DataFrame, 
                           level_range: Optional[Tuple[int, int]] = None,
                           main_attr: Optional[str] = None,
                           is_super_simple: Optional[bool] = None,
                           price_range: Optional[Tuple[float, float]] = None,
                           server: Optional[str] = None,
                           limit: int = 1000) -> pd.DataFrame:
        """
        å¯¹ç¼“å­˜æ•°æ®è¿›è¡Œç­›é€‰
        
        Args:
            cached_data: ç¼“å­˜çš„æ•°æ®
            level_range: ç­‰çº§èŒƒå›´ç­›é€‰
            main_attr: ä¸»å±æ€§ç­›é€‰
            is_super_simple: è¶…çº§ç®€æ˜“ç­›é€‰
            price_range: ä»·æ ¼èŒƒå›´ç­›é€‰
            server: æœåŠ¡å™¨ç­›é€‰
            limit: æ•°æ®æ¡æ•°é™åˆ¶
            
        Returns:
            ç­›é€‰åçš„DataFrame
        """
        filtered_data = cached_data.copy()
        
        # ç­‰çº§èŒƒå›´ç­›é€‰
        if level_range is not None:
            min_level, max_level = level_range
            filtered_data = filtered_data[
                (filtered_data['equip_level'] >= min_level) & 
                (filtered_data['equip_level'] <= max_level)
            ]
        
        # ä»·æ ¼èŒƒå›´ç­›é€‰
        if price_range is not None:
            min_price, max_price = price_range
            filtered_data = filtered_data[
                (filtered_data['price'] >= min_price) & 
                (filtered_data['price'] <= max_price)
            ]
        
        # æœåŠ¡å™¨ç­›é€‰
        if server is not None:
            filtered_data = filtered_data[filtered_data['server_name'] == server]
        
        # è¶…çº§ç®€æ˜“ç­›é€‰
        if is_super_simple is not None:
            if is_super_simple:
                filtered_data = filtered_data[filtered_data['special_effect'] == "[1]"]
            else:
                filtered_data = filtered_data[
                    (filtered_data['special_effect'] != "[1]") | 
                    (filtered_data['special_effect'].isna())
                ]
        
        # ä¸»å±æ€§ç­›é€‰
        if main_attr is not None:
            valid_main_attrs = ['damage', 'defense', 'magic_damage', 'magic_defense', 'fengyin', 'anti_fengyin', 'speed']
            if main_attr in valid_main_attrs:
                filtered_data = filtered_data[filtered_data[main_attr] > 0]
            else:
                self.logger.warning(f"æ— æ•ˆçš„ä¸»å±æ€§å­—æ®µ: {main_attr}, æ”¯æŒçš„å­—æ®µ: {valid_main_attrs}")
        
        # æŒ‰æ›´æ–°æ—¶é—´æ’åºå¹¶é™åˆ¶æ¡æ•°
        filtered_data = filtered_data.sort_values('update_time', ascending=False).head(limit)
        
        return filtered_data
    
    def _get_market_data_from_mysql(self,
                                   kindid: Optional[int] = None,
                                   level_range: Optional[Tuple[int, int]] = None,
                                   main_attr: Optional[str] = None,
                                   is_super_simple: Optional[bool] = None,
                                   price_range: Optional[Tuple[float, float]] = None,
                                   server: Optional[str] = None,
                                   limit: int = 1000,
                                   attrs: Optional[List[Dict[str, Any]]] = None) -> pd.DataFrame:
        """
        ä»MySQLæ•°æ®åº“è·å–å¸‚åœºçµé¥°æ•°æ®
        
        Args:
            å‚æ•°åŒget_market_dataæ–¹æ³•
            
        Returns:
            çµé¥°å¸‚åœºæ•°æ®DataFrame
        """
        try:
            # æ„å»ºSQLAlchemyæŸ¥è¯¢
            query = db.session.query(Equipment)

            if kindid is not None:
                query = query.filter(Equipment.kindid == kindid)

            if level_range is not None:
                min_level, max_level = level_range
                query = query.filter(Equipment.equip_level.between(min_level, max_level))

            if price_range is not None:
                min_price, max_price = price_range
                query = query.filter(Equipment.price.between(min_price, max_price))

            if server is not None:
                query = query.filter(Equipment.server_name == server)

            if is_super_simple is not None:
                if is_super_simple:
                    query = query.filter(Equipment.special_effect == "[1]")
                else:
                    query = query.filter(
                        or_(
                            Equipment.special_effect != "[1]",
                            Equipment.special_effect.is_(None)
                        )
                    )

            # ä¸»å±æ€§ç­›é€‰ï¼šå½“ä¼ å…¥main_attræ—¶ï¼Œç­›é€‰è¯¥å±æ€§å€¼å¤§äº0çš„è£…å¤‡
            if main_attr is not None:
                # éªŒè¯ä¸»å±æ€§å­—æ®µåæ˜¯å¦æœ‰æ•ˆ
                valid_main_attrs = ['damage', 'defense', 'magic_damage', 'magic_defense', 'fengyin', 'anti_fengyin', 'speed']
                if main_attr in valid_main_attrs:
                    query = query.filter(getattr(Equipment, main_attr) > 0)
                else:
                    self.logger.warning(f"æ— æ•ˆçš„ä¸»å±æ€§å­—æ®µ: {main_attr}, æ”¯æŒçš„å­—æ®µ: {valid_main_attrs}")

            # æ’åºå’Œé™åˆ¶
            query = query.order_by(Equipment.update_time.desc()).limit(limit)

            # æ‰§è¡ŒæŸ¥è¯¢å¹¶è½¬æ¢ä¸ºDataFrame
            equipments = query.all()
            
            if equipments:
                # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                data_list = []
                for equipment in equipments:
                    equipment_dict = {}
                    for column in equipment.__table__.columns:
                        value = getattr(equipment, column.name)
                        if hasattr(value, 'isoformat'):  # datetimeå¯¹è±¡
                            equipment_dict[column.name] = value.isoformat()
                        else:
                            equipment_dict[column.name] = value
                    data_list.append(equipment_dict)
                
                result_df = pd.DataFrame(data_list)
                
                # å»é‡
                result_df = result_df.drop_duplicates(subset=['equip_sn'], keep='first')
                
                # é™„åŠ å±æ€§ç­›é€‰ï¼šæ ¹æ®attr_typeè¿›è¡Œç­›é€‰
                if attrs is not None and len(attrs) > 0:
                    result_df = self._filter_by_attrs(result_df, attrs)
                
                print(f"ä»MySQLæ•°æ®åº“åŠ è½½äº† {len(result_df)} æ¡çµé¥°æ•°æ®")
                return result_df
            else:
                print(f"ä»MySQLæ•°æ®åº“æŸ¥è¯¢åˆ°0æ¡çµé¥°æ•°æ®")
                return pd.DataFrame()

        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢çµé¥°æ•°æ®å¤±è´¥: {e}")
            print(f"SQLæ‰§è¡Œå¼‚å¸¸: {e}")
            return pd.DataFrame()

    def _filter_by_attrs(self, data_df: pd.DataFrame, target_attrs: List[Dict[str, Any]]) -> pd.DataFrame:
        """
        æ ¹æ®é™„åŠ å±æ€§ç±»å‹ç­›é€‰æ•°æ®
       
        Args:
            data_df: å¸‚åœºæ•°æ®DataFrame
            target_attrs: ç›®æ ‡é™„åŠ å±æ€§åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«attr_typeå’Œattr_value
        
        Returns:
            ç­›é€‰åçš„DataFrame
        """
        if data_df.empty or not target_attrs:
            return data_df
        
        # ä¿å­˜ç›®æ ‡ç‰¹å¾ï¼Œä¾›åç»­ç‰¹å¾è®¡ç®—ä½¿ç”¨
        self.target_features = {'attrs': target_attrs}
        
        # æå–ç›®æ ‡é™„åŠ å±æ€§çš„ç±»å‹
        target_attr_types = []
        for attr in target_attrs:
            attr_type = attr.get('attr_type', '')
            if attr_type:
                target_attr_types.append(attr_type)
        
        if not target_attr_types:
            return data_df
        
        print(f"[é™„åŠ å±æ€§ç­›é€‰] ç›®æ ‡å±æ€§ç±»å‹: {target_attr_types}")
        
        # å®šä¹‰å±æ€§ä¼˜å…ˆçº§æ˜ å°„
        # æˆ’æŒ‡/è€³é¥°å±æ€§ä¼˜å…ˆçº§ï¼š
        # ç‰©ç†ç³»ï¼šä¼¤å®³(S)ã€ç‰©ç†æš´å‡»ç­‰çº§(A)ã€ç©¿åˆºç­‰çº§(B)ã€ç‹‚æš´ç­‰çº§(C)
        # æ³•æœ¯ç³»ï¼šæ³•æœ¯ä¼¤å®³(S)ã€æ³•æœ¯æš´å‡»ç­‰çº§(A)ã€æ³•æœ¯ä¼¤å®³ç»“æœ(B)
        # è¾…åŠ©ç³»ï¼šå›ºå®šä¼¤å®³(S)ã€æ²»ç–—èƒ½åŠ›(A)ã€é€Ÿåº¦(B)ã€å°å°å‘½ä¸­ç­‰çº§(C)
        # ä½¿ç”¨ç»Ÿä¸€çš„çµé¥°å±æ€§ä¼˜å…ˆçº§é…ç½®
        # æˆ’æŒ‡/è€³é¥°å±æ€§ä¼˜å…ˆçº§ï¼šä¼¤å®³(1)ã€ç‰©ç†æš´å‡»ç­‰çº§(2)ã€ç©¿åˆºç­‰çº§(3)ã€ç‹‚æš´ç­‰çº§(4)ã€æ³•æœ¯ä¼¤å®³(1)ã€æ³•æœ¯æš´å‡»ç­‰çº§(2)ã€æ³•æœ¯ä¼¤å®³ç»“æœ(3)ã€å›ºå®šä¼¤å®³(1)ã€æ²»ç–—èƒ½åŠ›(2)ã€å°å°å‘½ä¸­ç­‰çº§(3)ã€é€Ÿåº¦(4)
        # æ‰‹é•¯/ä½©é¥°å±æ€§ä¼˜å…ˆçº§ï¼šæ°”è¡€(1)ã€é˜²å¾¡(1)ã€æŠµæŠ—å°å°ç­‰çº§(2)ã€æŠ—ç‰©ç†æš´å‡»(2)ã€æ ¼æŒ¡å€¼(3)ã€æ³•æœ¯é˜²å¾¡(3)ã€æŠ—æ³•æœ¯æš´å‡»(4)ã€æ°”è¡€å›å¤æ•ˆæœ(4)
        
        # æ ¹æ®ç›®æ ‡å±æ€§æ•°é‡ç¡®å®šåŒ¹é…ç­–ç•¥
        target_attr_count = len(target_attr_types)
        
        # è·å–ç›®æ ‡å±æ€§çš„ä¼˜å…ˆçº§æ’åº
        def get_priority_sorted_attrs(attr_types, equipment_type):
            """æ ¹æ®è£…å¤‡ç±»å‹è·å–ä¼˜å…ˆçº§æ’åºçš„å±æ€§åˆ—è¡¨"""
            # ä½¿ç”¨ç»Ÿä¸€çš„ä¼˜å…ˆçº§é…ç½®
            # æŒ‰ä¼˜å…ˆçº§æ’åºï¼Œä¼˜å…ˆçº§ç›¸åŒçš„ä¿æŒåŸé¡ºåº
            sorted_attrs = sorted(attr_types, key=lambda x: get_priority_by_attr_name(x, equipment_type))
            return sorted_attrs
        
        # æ ¹æ®ç›®æ ‡å±æ€§æ•°é‡ç¡®å®šåŒ¹é…æ‰€éœ€çš„å±æ€§
        def get_match_attrs(target_attrs, equipment_type):
            """æ ¹æ®ç›®æ ‡å±æ€§æ•°é‡å’Œè£…å¤‡ç±»å‹ç¡®å®šåŒ¹é…æ‰€éœ€çš„å±æ€§ï¼Œè¿”å›(åŒ¹é…å±æ€§åˆ—è¡¨, æœªé€‰ä¸­å±æ€§)"""
            if target_attr_count == 2:
                # 2æ¡å±æ€§æ—¶ï¼Œéœ€è¦2æ¡å±æ€§ç±»å‹ç›¸åŒ
                return list(target_attrs), None
            elif target_attr_count == 3:
                # 3æ¡å±æ€§æ—¶ï¼Œæ£€æŸ¥æ˜¯å¦éƒ½ç›¸åŒ
                unique_attrs = set(target_attrs)
                if len(unique_attrs) == 1:
                    # 3æ¡å±æ€§éƒ½ä¸€æ ·ï¼Œå¿…é¡»3æ¡åŒ¹é…
                    return list(target_attrs), None
                else:
                    # 3æ¡å±æ€§ä¸å…¨ç›¸åŒï¼Œä¼˜å…ˆé€‰æ‹©é‡å¤çš„å±æ€§
                    from collections import Counter
                    attr_counter = Counter(target_attrs)
                    
                    # å¦‚æœæœ‰é‡å¤å±æ€§ï¼Œä¼˜å…ˆé€‰æ‹©é‡å¤æœ€å¤šçš„å±æ€§
                    if len(attr_counter) < len(target_attrs):
                        # æœ‰é‡å¤å±æ€§ï¼Œé€‰æ‹©é‡å¤æœ€å¤šçš„å±æ€§
                        most_common_attr = attr_counter.most_common(1)[0][0]
                        # å¦‚æœé‡å¤å±æ€§æœ‰2ä¸ªæˆ–ä»¥ä¸Šï¼Œé€‰æ‹©2ä¸ªé‡å¤å±æ€§
                        if attr_counter[most_common_attr] >= 2:
                            # é€‰æ‹©2ä¸ªé‡å¤å±æ€§ï¼Œæœªé€‰ä¸­çš„æ˜¯å…¶ä»–å±æ€§
                            unmatched_attrs = [attr for attr in target_attrs if attr != most_common_attr]
                            return [most_common_attr, most_common_attr], unmatched_attrs[0] if unmatched_attrs else None
                        else:
                            # é‡å¤å±æ€§åªæœ‰1ä¸ªï¼Œé€‰æ‹©1ä¸ªé‡å¤å±æ€§ + 1ä¸ªå…¶ä»–å±æ€§
                            other_attrs = [attr for attr in target_attrs if attr != most_common_attr]
                            # æœªé€‰ä¸­çš„æ˜¯å…¶ä»–å±æ€§ä¸­é™¤äº†å·²é€‰æ‹©çš„é‚£ä¸€ä¸ª
                            unmatched_attr = other_attrs[1] if len(other_attrs) > 1 else None
                            return [most_common_attr, other_attrs[0]], unmatched_attr
                    else:
                        # æ²¡æœ‰é‡å¤å±æ€§ï¼ŒæŒ‰ä¼˜å…ˆçº§å–2æ¡
                        sorted_attrs = get_priority_sorted_attrs(target_attrs, equipment_type)
                        # æœªé€‰ä¸­çš„æ˜¯æ’åºåç¬¬3ä¸ªå±æ€§
                        unmatched_attr = sorted_attrs[2] if len(sorted_attrs) > 2 else None
                        return sorted_attrs[:2], unmatched_attr
            else:
                # çµé¥°æœ€å¤šåªæœ‰3æ¡å±æ€§ï¼Œè¿™é‡Œä¸åº”è¯¥åˆ°è¾¾
                raise ValueError(f"çµé¥°å±æ€§æ•°é‡å¼‚å¸¸: {target_attr_count}ï¼Œæœ€å¤šåªèƒ½æœ‰3æ¡å±æ€§")
        
        # é¢„å…ˆè®¡ç®—ç›®æ ‡åŒ¹é…å±æ€§ï¼Œé¿å…åœ¨å¾ªç¯ä¸­é‡å¤è®¡ç®—
        # ç”±äºequipment_typeåœ¨åŒç±»è£…å¤‡ä¸­æ˜¯å›ºå®šçš„ï¼Œå¯ä»¥é¢„å…ˆè®¡ç®—
        # ä»ç¬¬ä¸€è¡Œæ•°æ®è·å–equipment_typeï¼Œå› ä¸ºåŒç±»è£…å¤‡çš„equipment_typeæ˜¯å›ºå®šçš„
        first_equipment_type = data_df.iloc[0].get('kindid', 0) if not data_df.empty else 0
        target_match_attrs, unmatched_attr = get_match_attrs(target_attr_types, first_equipment_type)
        print(f"é¢„å…ˆè®¡ç®—ç›®æ ‡åŒ¹é…å±æ€§target_match_attrs {target_match_attrs}")
        print(f"é¢„å…ˆè®¡ç®—æœªé€‰ä¸­å±æ€§unmatched_attr {unmatched_attr}")
        
        # å°†target_match_attrsä¿¡æ¯æ·»åŠ åˆ°ç›®æ ‡ç‰¹å¾ä¸­ï¼Œä¾›åç»­ç‰¹å¾è®¡ç®—ä½¿ç”¨
        if hasattr(self, 'target_features') and self.target_features:
            self.target_features['target_match_attrs'] = target_match_attrs
            self.target_features['attr_3_type'] = unmatched_attr
        filtered_rows = []
        
        for _, row in data_df.iterrows():
            try:
                # ç›´æ¥ä½¿ç”¨æ•°æ®åº“ä¸­å·²ç»å­˜åœ¨çš„agg_added_attrså­—æ®µï¼Œé¿å…é‡å¤ç‰¹å¾æå–
                market_attrs = row.get('agg_added_attrs', [])
                
                # å¦‚æœagg_added_attrsæ˜¯å­—ç¬¦ä¸²ï¼ˆJSONï¼‰ï¼Œéœ€è¦è§£æ
                if isinstance(market_attrs, str):
                    import json
                    try:
                        market_attrs = json.loads(market_attrs)
                    except json.JSONDecodeError:
                        market_attrs = []

                if not market_attrs:
                    continue
                
                # è·å–è£…å¤‡ç±»å‹
                equipment_type = row.get('kindid', 0)
                
                # ç»Ÿè®¡å¸‚åœºè£…å¤‡çš„é™„åŠ å±æ€§ç±»å‹
                market_attr_types = []
                for attr in market_attrs:
                    attr_type = attr.get('attr_type', '')
                    if attr_type:
                        market_attr_types.append(attr_type)
                # æ£€æŸ¥åŒ¹é…æ¡ä»¶
                if target_attr_count == 2:
                    # 2æ¡å±æ€§æ—¶ï¼Œéœ€è¦2æ¡å±æ€§ç±»å‹ç›¸åŒ
                    # æ£€æŸ¥ç›®æ ‡å±æ€§æ˜¯å¦éƒ½ç›¸åŒ
                    unique_target_attrs = set(target_attr_types)
                    if len(unique_target_attrs) == 1:
                        # 2æ¡å±æ€§éƒ½ä¸€æ ·ï¼Œéœ€è¦å¸‚åœºè£…å¤‡è‡³å°‘æœ‰2æ¡è¯¥ç±»å‹å±æ€§
                        target_attr_type = list(unique_target_attrs)[0]
                        market_attr_counter = {}
                        for attr_type in market_attr_types:
                            market_attr_counter[attr_type] = market_attr_counter.get(attr_type, 0) + 1
                        
                        if market_attr_counter.get(target_attr_type, 0) >= 2:
                            filtered_rows.append(row)
                    else:
                        # 2æ¡å±æ€§ä¸åŒï¼Œéœ€è¦å¸‚åœºè£…å¤‡åŒ…å«è¿™2ç§å±æ€§
                        market_attr_set = set(market_attr_types)
                        target_match_set = set(target_match_attrs)
                        if len(target_match_set.intersection(market_attr_set)) >= 2:
                            filtered_rows.append(row)
                elif target_attr_count == 3:
                    # 3æ¡å±æ€§æ—¶çš„ç‰¹æ®Šå¤„ç†
                    unique_target_attrs = set(target_attr_types)
                    if len(unique_target_attrs) == 1:
                        # 3æ¡å±æ€§éƒ½ä¸€æ ·ï¼Œå¿…é¡»3æ¡åŒ¹é…
                        market_attr_counter = {}
                        for attr_type in market_attr_types:
                            market_attr_counter[attr_type] = market_attr_counter.get(attr_type, 0) + 1
                        
                        target_attr_type = list(unique_target_attrs)[0]
                        if market_attr_counter.get(target_attr_type, 0) >= 3:
                            filtered_rows.append(row)
                    else:
                        # 3æ¡å±æ€§ä¸å…¨ç›¸åŒï¼Œä¼˜å…ˆé€‰æ‹©é‡å¤çš„å±æ€§
                        # ä½¿ç”¨è®¡æ•°æ–¹å¼ï¼Œå› ä¸ºtarget_match_attrså¯èƒ½åŒ…å«é‡å¤å±æ€§
                        market_attr_counter = {}
                        for attr_type in market_attr_types:
                            market_attr_counter[attr_type] = market_attr_counter.get(attr_type, 0) + 1
                        
                        # æ£€æŸ¥target_match_attrsä¸­çš„æ¯ä¸ªå±æ€§æ˜¯å¦åœ¨å¸‚åœºè£…å¤‡ä¸­å­˜åœ¨
                        match_count = 0
                        for target_attr in target_match_attrs:
                            if market_attr_counter.get(target_attr, 0) > 0:
                                match_count += 1
                                # å‡å°‘è®¡æ•°ï¼Œé¿å…é‡å¤è®¡ç®—
                                market_attr_counter[target_attr] -= 1
                        
                        if match_count >= 2:
                            filtered_rows.append(row)
                else:
                    # å…¶ä»–æƒ…å†µï¼Œä½¿ç”¨äº¤é›†åŒ¹é…
                    market_attr_set = set(market_attr_types)
                    target_match_set = set(target_match_attrs)
                    common_attr_types = target_match_set.intersection(market_attr_set)
                    if len(common_attr_types) >= min(2, len(target_match_set)):
                        filtered_rows.append(row)
                    
            except Exception as e:
                self.logger.warning(f"å¤„ç†è£…å¤‡ {row.get('id', 'unknown')} çš„é™„åŠ å±æ€§æ—¶å‡ºé”™: {e}")
                continue
        
        if filtered_rows:
            result_df = pd.DataFrame(filtered_rows)
            print(f"[é™„åŠ å±æ€§ç­›é€‰] ç­›é€‰å‰: {len(data_df)} æ¡ï¼Œç­›é€‰å: {len(result_df)} æ¡")
            return result_df
        else:
            print(f"[é™„åŠ å±æ€§ç­›é€‰] æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è£…å¤‡")
            return pd.DataFrame()

    def get_market_data_for_similarity(self,
                                       target_features: Dict[str, Any]) -> pd.DataFrame:
        """
        æ ¹æ®ç›®æ ‡ç‰¹å¾è·å–ç”¨äºç›¸ä¼¼åº¦è®¡ç®—çš„å¸‚åœºæ•°æ®

        Args:
            target_features: ç›®æ ‡çµé¥°ç‰¹å¾

        Returns:
            è¿‡æ»¤åçš„å¸‚åœºæ•°æ®DataFrame
        """
        # åŸºç¡€è¿‡æ»¤æ¡ä»¶
        kindid = target_features.get('kindid')
        
        # è¶…çº§ç®€æ˜“è¿‡æ»¤
        is_super_simple = target_features.get('is_super_simple', False)
        level_range = target_features.get('equip_level_range',(0,160))
        main_attr = target_features.get('main_attr', None)
        attrs = target_features.get('attrs', None)
        print(f"get_market_data_for_similarity_target_features: {target_features}")
        # è·å–å¸‚åœºæ•°æ®
        market_data = self.get_market_data(
            kindid=kindid,
            level_range=level_range,
            is_super_simple=is_super_simple,
            main_attr=main_attr,
            attrs=attrs,
            limit=5000
        )
        
        if market_data.empty:
            return market_data
            
        # æå–ç‰¹å¾
        features_list = []
        for _, row in market_data.iterrows():
            try:
                features = self.feature_extractor.extract_features(row.to_dict())
                features['equip_sn'] = row.get('equip_sn', row.get('eid', row.get('id', None)))
                features['price'] = row['price']
                features_list.append(features)
            except Exception as e:
                self.logger.warning(f"æå–ç‰¹å¾å¤±è´¥: {e}")
                continue
                
        if features_list:
            return pd.DataFrame(features_list)
        else:
            return pd.DataFrame()

    def get_market_data_with_business_rules(self,
                                           target_features: Dict[str, Any],
                                           **kwargs) -> pd.DataFrame:
        """
        æ ¹æ®ä¸šåŠ¡è§„åˆ™è·å–å¸‚åœºæ•°æ®

        Args:
            target_features: ç›®æ ‡çµé¥°ç‰¹å¾
            **kwargs: å…¶ä»–è¿‡æ»¤å‚æ•°

        Returns:
            è¿‡æ»¤åçš„å¸‚åœºæ•°æ®DataFrame (å·²æå–ç‰¹å¾)
        """
        # è·å–åŸºç¡€å¸‚åœºæ•°æ® (å·²åŒ…å«æå–çš„ç‰¹å¾)
        market_data = self.get_market_data_for_similarity(target_features)
        
        if market_data.empty:
            return market_data
            
        # åº”ç”¨ä¸šåŠ¡è§„åˆ™è¿‡æ»¤ (ä¿æŒç‰¹å¾æ•°æ®æ ¼å¼)
        filtered_mask = []
        
        for _, row in market_data.iterrows():
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šçš„ä¸šåŠ¡è§„åˆ™è¿‡æ»¤é€»è¾‘
            # ä¾‹å¦‚ï¼šä»·æ ¼å¼‚å¸¸å€¼è¿‡æ»¤ã€å±æ€§ç»„åˆè¿‡æ»¤ç­‰
            
            # ç¤ºä¾‹ï¼šè¿‡æ»¤ä»·æ ¼å¼‚å¸¸å€¼ï¼ˆä»·æ ¼è¿‡é«˜æˆ–è¿‡ä½çš„è£…å¤‡ï¼‰
            price = row.get('price', 0)
            if price <= 0 or price > 1000000:  # ä»·æ ¼èŒƒå›´æ£€æŸ¥
                filtered_mask.append(False)
                continue
            
            filtered_mask.append(True)
            
        # ä½¿ç”¨å¸ƒå°”æ©ç è¿‡æ»¤ï¼Œä¿æŒDataFrameç»“æ„å’Œç‰¹å¾æ•°æ®
        if any(filtered_mask):
            filtered_data = market_data[filtered_mask].copy()
            print(f"[ä¸šåŠ¡è§„åˆ™è¿‡æ»¤] ç­›é€‰å‰: {len(market_data)} æ¡ï¼Œç­›é€‰å: {len(filtered_data)} æ¡")
            return filtered_data
        else:
            print(f"[ä¸šåŠ¡è§„åˆ™è¿‡æ»¤] æ‰€æœ‰æ•°æ®éƒ½è¢«è¿‡æ»¤æ‰äº†")
            return pd.DataFrame()
