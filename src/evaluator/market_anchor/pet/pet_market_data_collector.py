import pandas as pd
import logging
from typing import Dict, Any, List, Optional, Tuple, Union
from datetime import datetime

from src.evaluator.feature_extractor.pet_feature_extractor import PetFeatureExtractor
from src.database import db
from src.models.pet import Pet
from sqlalchemy import and_, or_, func, text


class PetMarketDataCollector:
    """å® ç‰©å¸‚åœºæ•°æ®é‡‡é›†å™¨ - ä»æ•°æ®åº“ä¸­è·å–å’Œå¤„ç†å® ç‰©å¸‚åœºæ•°æ®"""

    _instance = None  # å•ä¾‹å®ä¾‹
    _lock = None  # çº¿ç¨‹é”ï¼Œç¡®ä¿çº¿ç¨‹å®‰å…¨
    
    def __new__(cls):
        """å•ä¾‹æ¨¡å¼å®ç°"""
        import threading
        if cls._lock is None:
            cls._lock = threading.Lock()
            
        with cls._lock:
            if cls._instance is None:
                instance = super(PetMarketDataCollector, cls).__new__(cls)
                cls._instance = instance
                # æ ‡è®°å®ä¾‹æ˜¯å¦å·²åˆå§‹åŒ–ï¼Œé¿å…é‡å¤åˆå§‹åŒ–
                instance._initialized = False
                print("åˆ›å»ºæ–°çš„ PetMarketDataCollector å•ä¾‹å®ä¾‹")
            else:
                print("ä½¿ç”¨ç°æœ‰çš„ PetMarketDataCollector å•ä¾‹å®ä¾‹")
            
            return cls._instance

    def __init__(self):
        """
        åˆå§‹åŒ–å® ç‰©å¸‚åœºæ•°æ®é‡‡é›†å™¨ - æ”¯æŒRediså…¨é‡ç¼“å­˜ï¼ˆå•ä¾‹æ¨¡å¼ä¸‹åªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰
        """
        # é¿å…é‡å¤åˆå§‹åŒ–
        if hasattr(self, '_initialized') and self._initialized:
            return
            
        self.feature_extractor = PetFeatureExtractor()
        self.logger = logging.getLogger(__name__)

        # åˆå§‹åŒ–Redisç¼“å­˜
        try:
            from src.utils.redis_cache import get_redis_cache
            self.redis_cache = get_redis_cache()
            if self.redis_cache and self.redis_cache.is_available():
                self.logger.info("Redisç¼“å­˜åˆå§‹åŒ–æˆåŠŸï¼Œå°†ä½¿ç”¨Rediså…¨é‡ç¼“å­˜æ¨¡å¼")
                print("å® ç‰©æ•°æ®é‡‡é›†å™¨åˆå§‹åŒ–ï¼Œä½¿ç”¨Rediså…¨é‡ç¼“å­˜æ¨¡å¼")
            else:
                self.redis_cache = None
                print("å® ç‰©æ•°æ®é‡‡é›†å™¨åˆå§‹åŒ–ï¼ŒRedisä¸å¯ç”¨ï¼Œä½¿ç”¨MySQLæ•°æ®åº“")
        except Exception as e:
            self.logger.warning(f"Redisç¼“å­˜åˆå§‹åŒ–å¤±è´¥: {e}")
            self.redis_cache = None
            print("å® ç‰©æ•°æ®é‡‡é›†å™¨åˆå§‹åŒ–ï¼ŒRedisåˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨MySQLæ•°æ®åº“")
        
        # å…¨é‡ç¼“å­˜ç›¸å…³å±æ€§
        self._full_cache_key = "pet_market_data_full"
        self._cache_ttl_hours = -1  # æ°¸ä¸è¿‡æœŸï¼Œåªèƒ½æ‰‹åŠ¨åˆ·æ–°
        self._full_data_cache = None  # å†…å­˜ä¸­çš„å…¨é‡æ•°æ®ç¼“å­˜
        
        # è¿›åº¦è·Ÿè¸ªç›¸å…³å±æ€§
        self._refresh_status = "idle"  # idle, running, completed, error
        self._refresh_progress = 0  # 0-100
        self._refresh_message = ""
        self._refresh_start_time = None
        self._refresh_total_records = 0
        self._refresh_processed_records = 0
        self._refresh_current_batch = 0
        self._refresh_total_batches = 0
        
        # MySQLæ•°æ®ç»Ÿè®¡
        self.mysql_data_count = 0  # MySQLä¸­petsè¡¨çš„æ€»è®°å½•æ•°
        
        self._initialized = True
        cache_mode = "æ°¸ä¸è¿‡æœŸæ¨¡å¼" if self._cache_ttl_hours == -1 else f"{self._cache_ttl_hours}å°æ—¶è¿‡æœŸ"
        print(f"å® ç‰©å¸‚åœºæ•°æ®é‡‡é›†å™¨å•ä¾‹åˆå§‹åŒ–å®Œæˆï¼Œæ”¯æŒRediså…¨é‡ç¼“å­˜ï¼ˆ{cache_mode}ï¼‰")
    
    
    def get_market_data(self,
                        level_range: Optional[Tuple[int, int]] = None,
                        role_grade_limit_range: Optional[Tuple[int, int]] = None,
                        price_range: Optional[Tuple[float, float]] = None,
                        server: Optional[str] = None,
                        all_skill: Optional[Union[str, List[str]]] = None,
                        limit: int = 1000,
                        use_redis_cache: bool = True) -> pd.DataFrame:
        """
        è·å–å¸‚åœºå® ç‰©æ•°æ® - ä¼˜å…ˆä»Rediså…¨é‡ç¼“å­˜è·å–å¹¶ç­›é€‰

        Args:
            level_range: ç­‰çº§èŒƒå›´ (min_level, max_level)
            role_grade_limit_range: æºå¸¦ç­‰çº§ (min_role_grade_limit, max_role_grade_limit)
            price_range: ä»·æ ¼èŒƒå›´ (min_price, max_price)
            server: æœåŠ¡å™¨ç­›é€‰
            all_skill: æŠ€èƒ½ ä½¿ç”¨äº†ç®¡é“ç¬¦æ‹¼æ¥çš„æŠ€èƒ½å­—ç¬¦ä¸²ä»¥"|"
            limit: è¿”å›æ•°æ®æ¡æ•°é™åˆ¶
            use_redis_cache: æ˜¯å¦ä½¿ç”¨Redisç¼“å­˜

        Returns:
            å® ç‰©å¸‚åœºæ•°æ®DataFrame
        """
        try:
            import time
            start_time = time.time()
            
            # ä¼˜å…ˆä»Rediså…¨é‡ç¼“å­˜è·å–æ•°æ®
            if use_redis_cache and self.redis_cache:
                full_data = self._get_full_data_from_redis()
                
                if full_data is None or full_data.empty:
                    # ç¼“å­˜æœªå‘½ä¸­ï¼Œå°è¯•åŠ è½½å…¨é‡æ•°æ®åˆ°Redis
                    print("Redisç¼“å­˜æœªå‘½ä¸­ï¼Œå¼€å§‹åŠ è½½å…¨é‡æ•°æ®...")
                    if self._load_full_data_to_redis():
                        full_data = self._get_full_data_from_redis()
                
                if full_data is not None and not full_data.empty:
                    # ä»Rediså…¨é‡æ•°æ®ä¸­è¿›è¡Œç­›é€‰
                    filtered_data = self._filter_data_from_full_cache(
                        full_data=full_data,
                        level_range=level_range,
                        role_grade_limit_range=role_grade_limit_range,
                        price_range=price_range,
                        server=server,
                        all_skill=all_skill,
                        limit=limit
                    )
                    
                    elapsed_time = time.time() - start_time
                    print(f"ä»Rediså…¨é‡ç¼“å­˜ç­›é€‰å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.3f}ç§’ï¼Œè¿”å›: {len(filtered_data)} æ¡æ•°æ®")
                    return filtered_data
            
            # é™çº§åˆ°MySQLæŸ¥è¯¢
            print("ä½¿ç”¨MySQLæ•°æ®åº“æŸ¥è¯¢ï¼ˆé™çº§æ¨¡å¼ï¼‰...")
            return self._get_market_data_from_mysql(
                level_range=level_range,
                role_grade_limit_range=role_grade_limit_range,
                price_range=price_range,
                server=server,
                all_skill=all_skill,
                limit=limit
            )

        except Exception as e:
            self.logger.error(f"è·å–å¸‚åœºæ•°æ®å¤±è´¥: {e}")
            print(f"æŸ¥è¯¢å¼‚å¸¸: {e}")
            return pd.DataFrame()

    def _get_market_data_from_mysql(self,
                                   level_range: Optional[Tuple[int, int]] = None,
                                   role_grade_limit_range: Optional[Tuple[int, int]] = None,
                                   price_range: Optional[Tuple[float, float]] = None,
                                   server: Optional[str] = None,
                                   all_skill: Optional[Union[str, List[str]]] = None,
                                   limit: int = 1000) -> pd.DataFrame:
        """
        ä»MySQLæ•°æ®åº“è·å–å® ç‰©æ•°æ®ï¼ˆåŸå§‹æŸ¥è¯¢é€»è¾‘ï¼‰
        """
        try:
            # æ„å»ºSQLAlchemyæŸ¥è¯¢ - åªæŸ¥è¯¢éœ€è¦çš„13ä¸ªå­—æ®µ
            query = db.session.query(
                Pet.role_grade_limit,
                Pet.equip_level,
                Pet.growth,
                Pet.is_baobao,
                Pet.all_skill,
                Pet.evol_skill_list,
                Pet.texing,
                Pet.lx,
                Pet.equip_list,
                Pet.equip_list_amount,
                Pet.neidan,
                Pet.equip_sn,
                Pet.price,
                Pet.update_time  # ç”¨äºæ’åº
            )

            # å¤„ç†all_skillå‚æ•°ï¼Œæ”¯æŒå­—ç¬¦ä¸²æˆ–åˆ—è¡¨
            target_skills = []
            if all_skill:
                if isinstance(all_skill, str):
                    target_skills = [s for s in all_skill.split('|') if s]
                elif isinstance(all_skill, list):
                    target_skills = [str(s) for s in all_skill if s]

            # åŸºç¡€ç­›é€‰æ¡ä»¶
            if level_range is not None:
                min_level, max_level = level_range
                query = query.filter(Pet.equip_level.between(min_level, max_level))

            if role_grade_limit_range is not None:
                min_role_grade_limit, max_role_grade_limit = role_grade_limit_range
                query = query.filter(Pet.role_grade_limit.between(min_role_grade_limit, max_role_grade_limit))

            if price_range is not None:
                min_price, max_price = price_range
                query = query.filter(Pet.price.between(min_price, max_price))

            # æ³¨æ„ï¼šserverå­—æ®µä¸åœ¨æŸ¥è¯¢çš„13ä¸ªå­—æ®µä¸­ï¼Œå¦‚æœéœ€è¦æœåŠ¡å™¨ç­›é€‰ï¼Œéœ€è¦æ·»åŠ Pet.server_nameåˆ°æŸ¥è¯¢ä¸­
            # if server is not None:
            #     query = query.filter(Pet.server_name == server)

            # æŠ€èƒ½ç­›é€‰
            if target_skills:
                # æŠ€èƒ½SQLåˆæ­¥è¿‡æ»¤
                for skill in target_skills:
                    query = query.filter(Pet.all_skill.like(f"%{skill}%"))
                
                # æŠ€èƒ½æ•°é‡è¿‡æ»¤ è¿‡æ»¤å‡ºæŠ€èƒ½æ•°é‡ <= len(target_skills)+2 çš„æ•°æ®
                skill_count_limit = len(target_skills) + 1
                query = query.filter(
                    func.length(Pet.all_skill) - func.length(func.replace(Pet.all_skill, '|', '')) + 1 <= skill_count_limit
                )

            # æ’åºå’Œé™åˆ¶
            query = query.order_by(Pet.update_time.desc()).limit(limit)

            # æ‰§è¡ŒæŸ¥è¯¢
            pets = query.all()
            
            if pets:
                # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨ - æŸ¥è¯¢ç»“æœå·²ç»æ˜¯å…ƒç»„ï¼Œéœ€è¦æŒ‰é¡ºåºæ˜ å°„å­—æ®µ
                data_list = []
                for pet_tuple in pets:
                    pet_dict = {
                        'role_grade_limit': pet_tuple[0],
                        'equip_level': pet_tuple[1],
                        'growth': pet_tuple[2],
                        'is_baobao': pet_tuple[3],
                        'all_skill': pet_tuple[4],
                        'evol_skill_list': pet_tuple[5],
                        'texing': pet_tuple[6],
                        'lx': pet_tuple[7],
                        'equip_list': pet_tuple[8],
                        'equip_list_amount': pet_tuple[9],
                        'neidan': pet_tuple[10],
                        'equip_sn': pet_tuple[11],
                        'price': pet_tuple[12],
                        'update_time': pet_tuple[13]  # ç”¨äºæ’åºçš„å­—æ®µ
                    }
                    data_list.append(pet_dict)
                
                result_df = pd.DataFrame(data_list)
                
                # Pythoné›†åˆç²¾ç¡®è¿‡æ»¤æŠ€èƒ½
                if target_skills:
                    target_set = set(target_skills)
                    def match(row):
                        all_skill_val = row.get('all_skill', '')
                        skill_set = set(all_skill_val.split('|')) if all_skill_val else set()
                        return target_set.issubset(skill_set)
                    result_df = result_df[result_df.apply(match, axis=1)]
                
                # å»é‡
                result_df = result_df.drop_duplicates(subset=['equip_sn'], keep='first')
                
                print(f"ä»MySQLæ•°æ®åº“åŠ è½½äº† {len(result_df)} æ¡å® ç‰©å¸‚åœºæ•°æ®")
                return result_df
            else:
                print(f"ä»MySQLæ•°æ®åº“æŸ¥è¯¢åˆ°0æ¡å® ç‰©å¸‚åœºæ•°æ®")
                return pd.DataFrame()

        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢å® ç‰©å¸‚åœºæ•°æ®å¤±è´¥: {e}")
            print(f"SQLæ‰§è¡Œå¼‚å¸¸: {e}")
            return pd.DataFrame()
        
    def get_market_data_for_similarity(self,
                                       target_features: Dict[str, Any]) -> pd.DataFrame:
        """
        æ ¹æ®ç›®æ ‡ç‰¹å¾è·å–ç”¨äºç›¸ä¼¼åº¦è®¡ç®—çš„å¸‚åœºæ•°æ®

        Args:
            target_features: ç›®æ ‡å¬å”¤å…½ç‰¹å¾

        Returns:
            è¿‡æ»¤åçš„å¸‚åœºæ•°æ®DataFrame
        """
        # ä¼˜å…ˆä½¿ç”¨å†…å­˜ç¼“å­˜
        if self._full_data_cache is not None and not self._full_data_cache.empty:
            print("ä½¿ç”¨å†…å­˜ç¼“å­˜è¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—...")
            # åŸºç¡€è¿‡æ»¤æ¡ä»¶
            role_grade_limit = target_features.get('role_grade_limit', 0)
            
            # ç­‰çº§èŒƒå›´ï¼šç›®æ ‡ç­‰çº§Â±20çº§
            role_grade_limit_range = (max(0, role_grade_limit - 20), role_grade_limit + 20)

            all_skill = target_features.get('all_skill', '')
            
            # ä»å†…å­˜ç¼“å­˜ä¸­ç­›é€‰æ•°æ®
            market_data = self._filter_data_from_full_cache(
                full_data=self._full_data_cache,
                role_grade_limit_range=role_grade_limit_range,
                all_skill=all_skill,
                limit=5000
            )
        else:
            # åŸºç¡€è¿‡æ»¤æ¡ä»¶
            role_grade_limit = target_features.get('role_grade_limit', 0)
            
            # ç­‰çº§èŒƒå›´ï¼šç›®æ ‡ç­‰çº§Â±20çº§
            role_grade_limit_range = (max(0, role_grade_limit - 20), role_grade_limit + 20)

            all_skill = target_features.get('all_skill', '')
            
            # è·å–å¸‚åœºæ•°æ®
            market_data = self.get_market_data(
                role_grade_limit_range=role_grade_limit_range,
                all_skill=all_skill,
                limit=5000
            )
        
        if market_data.empty:
            return market_data
            
        # æå–ç‰¹å¾
        features_list = []
        for _, row in market_data.iterrows():
            try:
                features = self.feature_extractor.extract_features(row.to_dict())
                
                # ä¿ç•™åŸå§‹å…³é”®å­—æ®µï¼Œç¡®ä¿æ¥å£è¿”å›æ—¶æœ‰å®Œæ•´ä¿¡æ¯
                features['equip_sn'] = row.get('equip_sn', row.get('eid',None))
                features['price'] = row.get('price', 0)
                
                features_list.append(features)
            except Exception as e:
                self.logger.warning(f"æå–ç‰¹å¾å¤±è´¥: {e}")
                continue
                
        if features_list:
            return pd.DataFrame(features_list)
        else:
            return pd.DataFrame()
        
    def get_market_data_with_business_rules(self,
                                           target_features: Dict[str, Any],
                                           **kwargs) -> pd.DataFrame:
        """
        æ ¹æ®ä¸šåŠ¡è§„åˆ™è·å–å¸‚åœºæ•°æ®

        Args:
            target_features: ç›®æ ‡å¬å”¤å…½ç‰¹å¾
            **kwargs: å…¶ä»–è¿‡æ»¤å‚æ•°

        Returns: 
            è¿‡æ»¤åçš„å¸‚åœºæ•°æ®DataFrame
        """
        # è·å–åŸºç¡€å¸‚åœºæ•°æ®
        market_data = self.get_market_data_for_similarity(target_features)
        
        if market_data.empty:
            return market_data
            
        # åº”ç”¨ä¸šåŠ¡è§„åˆ™è¿‡æ»¤
        filtered_data = []
        
        for _, row in market_data.iterrows():
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šçš„ä¸šåŠ¡è§„åˆ™è¿‡æ»¤é€»è¾‘
            # ä¾‹å¦‚ï¼šä»·æ ¼å¼‚å¸¸å€¼è¿‡æ»¤ã€å±æ€§ç»„åˆè¿‡æ»¤ç­‰
            
            # ç¤ºä¾‹ï¼šè¿‡æ»¤ä»·æ ¼å¼‚å¸¸å€¼ï¼ˆä»·æ ¼è¿‡é«˜æˆ–è¿‡ä½çš„è£…å¤‡ï¼‰
            # price = row.get('price', 0)
            # if price <= 0 or price > 1000000:  # ä»·æ ¼èŒƒå›´æ£€æŸ¥
            #     continue
                
            filtered_data.append(row)
            
        if filtered_data:
            return pd.DataFrame(filtered_data)
        else:
            return pd.DataFrame()

    def _get_mysql_pets_count(self) -> int:
        """
        è·å–MySQLä¸­petsè¡¨çš„æ€»è®°å½•æ•°
        
        Returns:
            int: petsè¡¨æ€»è®°å½•æ•°
        """
        try:
            from src.database import db
            from src.models.pet import Pet
            from flask import current_app
            from src.app import create_app
            
            # ç¡®ä¿åœ¨Flaskåº”ç”¨ä¸Šä¸‹æ–‡ä¸­
            if not current_app:
                # åˆ›å»ºåº”ç”¨ä¸Šä¸‹æ–‡
                app = create_app()
                with app.app_context():
                    return self._get_mysql_pets_count()
            
            # æŸ¥è¯¢petsè¡¨æ€»æ•°
            count = db.session.query(Pet).count()
            self.mysql_data_count = count
            self.logger.info(f"MySQL petsè¡¨æ€»è®°å½•æ•°: {count:,}")
            return count
            
        except Exception as e:
            self.logger.error(f"è·å–MySQLå® ç‰©æ•°æ®æ€»æ•°å¤±è´¥: {e}")
            return 0

    def _load_full_data_to_redis(self, force_refresh: bool = False) -> bool:
        """
        åŠ è½½å…¨é‡å® ç‰©æ•°æ®åˆ°Redis - å‚è€ƒè£…å¤‡æ¨¡å—çš„æ‰¹æ¬¡å¤„ç†å’Œè¿›åº¦è·Ÿè¸ª
        
        Args:
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°
            
        Returns:
            bool: æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        if not self.redis_cache:
            return False
            
        try:
            import time
            from datetime import datetime
            
            # åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ª
            self._refresh_status = "running"
            self._refresh_progress = 0
            self._refresh_message = "å¼€å§‹åŠ è½½å® ç‰©æ•°æ®..."
            self._refresh_start_time = datetime.now()
            self._refresh_processed_records = 0
            self._refresh_current_batch = 0
            
            start_time = time.time()
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜ä¸”ä¸éœ€è¦å¼ºåˆ¶åˆ·æ–°
            if not force_refresh:
                self._refresh_message = "æ£€æŸ¥Rediså…¨é‡ç¼“å­˜..."
                self._refresh_progress = 5
                
                try:
                    print("ğŸ” å¼€å§‹æ£€æŸ¥Redisç¼“å­˜...")
                    hash_key = f"{self._full_cache_key}:hash"
                    cached_data = self.redis_cache.get_hash_data(hash_key)
                    print(f"ğŸ” Redisç¼“å­˜æ£€æŸ¥å®Œæˆï¼Œç»“æœ: {cached_data is not None}")
                    
                    if cached_data is not None and not cached_data.empty:
                        print(f"Rediså…¨é‡ç¼“å­˜å·²å­˜åœ¨ï¼Œæ•°æ®é‡: {len(cached_data)} æ¡")
                        # æ­£ç¡®è®¾ç½®çŠ¶æ€ä¿¡æ¯
                        self._refresh_status = "completed"
                        self._refresh_progress = 100
                        self._refresh_message = "ä½¿ç”¨ç°æœ‰ç¼“å­˜"
                        self._refresh_total_records = len(cached_data)
                        self._refresh_processed_records = len(cached_data)
                        self._refresh_total_batches = 1
                        self._refresh_current_batch = 1
                        # å°†æ•°æ®åŠ è½½åˆ°å†…å­˜ç¼“å­˜
                        self._full_data_cache = cached_data
                        return True
                    else:
                        print("Redisç¼“å­˜ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œå°†é‡æ–°åŠ è½½æ•°æ®")
                except Exception as e:
                    print(f"æ£€æŸ¥Redisç¼“å­˜æ—¶å‡ºé”™: {e}")
                    self._refresh_message = f"æ£€æŸ¥ç¼“å­˜å¤±è´¥: {str(e)}"
                    # ç»§ç»­æ‰§è¡Œé‡æ–°åŠ è½½
            
            print(" å¼€å§‹ä»MySQLåŠ è½½å® ç‰©æ•°æ®åˆ°Redis...")
            
            # ä»æ•°æ®åº“åŠ è½½å…¨é‡æ•°æ®
            from src.database import db
            from src.models.pet import Pet
            from flask import current_app
            from src.app import create_app
            
            # ç¡®ä¿åœ¨Flaskåº”ç”¨ä¸Šä¸‹æ–‡ä¸­
            if not current_app:
                # åˆ›å»ºåº”ç”¨ä¸Šä¸‹æ–‡
                app = create_app()
                with app.app_context():
                    return self._load_full_data_to_redis(force_refresh)
            
            # è·å–æ€»è®°å½•æ•°
            self._refresh_message = "ç»Ÿè®¡æ•°æ®æ€»é‡..."
            self._refresh_progress = 10
            
            # è·å–MySQLå® ç‰©æ•°æ®æ€»æ•°
            full_count = db.session.query(Pet).count()
            self.mysql_data_count = full_count
            total_count = full_count  # åŠ è½½å…¨éƒ¨æ•°æ®

            print(f"å® ç‰©æ€»è®°å½•æ•°: {full_count}ï¼Œæœ¬æ¬¡åŠ è½½: {total_count} æ¡")
            
            # åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°
            if total_count > 50000:
                batch_size = 300   # å¤§æ•°æ®é›†ï¼šå°æ‰¹æ¬¡
            elif total_count > 20000:
                batch_size = 500   # ä¸­ç­‰æ•°æ®é›†
            elif total_count > 5000:
                batch_size = 800   # ä¸­å°æ•°æ®é›†
            else:
                batch_size = 300   # å°æ•°æ®é›†
            
            total_batches = (total_count + batch_size - 1) // batch_size
            self._refresh_total_records = total_count
            self._refresh_total_batches = total_batches
            
            print(f"å°†åˆ† {total_batches} æ‰¹å¤„ç†ï¼Œæ¯æ‰¹ {batch_size} æ¡")
            
            # åˆ†æ‰¹åŠ è½½æ•°æ®
            all_data = []
            offset = 0
            
            for batch_num in range(total_batches):
                # æ›´æ–°è¿›åº¦
                self._refresh_current_batch = batch_num + 1
                batch_progress = 10 + int(((batch_num + 1) / total_batches) * 80)  # 10-90%çš„è¿›åº¦èŒƒå›´
                self._refresh_progress = min(batch_progress, 90)
                self._refresh_message = f"å¤„ç†ç¬¬ {batch_num + 1}/{total_batches} æ‰¹å® ç‰©æ•°æ®..."
                
                print(f"å¤„ç†ç¬¬ {batch_num + 1}/{total_batches} æ‰¹ï¼Œåç§»é‡: {offset}")
                
                try:
                    # æ„å»ºæ‰¹æ¬¡æŸ¥è¯¢ï¼Œç¡®ä¿ä¸è¶…è¿‡æ€»é™åˆ¶
                    remaining = total_count - offset
                    actual_limit = min(batch_size, remaining)
                    if actual_limit <= 0:
                        break
                        
                    # æŸ¥è¯¢å® ç‰©æ•°æ® - åªæŸ¥è¯¢éœ€è¦çš„13ä¸ªå­—æ®µ
                    query = db.session.query(
                        Pet.role_grade_limit,
                        Pet.equip_level,
                        Pet.growth,
                        Pet.is_baobao,
                        Pet.all_skill,
                        Pet.evol_skill_list,
                        Pet.texing,
                        Pet.lx,
                        Pet.equip_list,
                        Pet.equip_list_amount,
                        Pet.neidan,
                        Pet.equip_sn,
                        Pet.price,
                        Pet.update_time  # ç”¨äºæ’åº
                    ).offset(offset).limit(actual_limit)
                    pets = query.all()
                    
                    if not pets:
                        print(f"ç¬¬ {batch_num + 1} æ‰¹æ— æ•°æ®ï¼Œè·³è¿‡")
                        continue
                    
                    # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ - æŸ¥è¯¢ç»“æœå·²ç»æ˜¯å…ƒç»„ï¼Œéœ€è¦æŒ‰é¡ºåºæ˜ å°„å­—æ®µ
                    batch_data = []
                    for pet_tuple in pets:
                        pet_dict = {
                            'role_grade_limit': pet_tuple[0],
                            'equip_level': pet_tuple[1],
                            'growth': pet_tuple[2],
                            'is_baobao': pet_tuple[3],
                            'all_skill': pet_tuple[4],
                            'evol_skill_list': pet_tuple[5],
                            'texing': pet_tuple[6],
                            'lx': pet_tuple[7],
                            'equip_list': pet_tuple[8],
                            'equip_list_amount': pet_tuple[9],
                            'neidan': pet_tuple[10],
                            'equip_sn': pet_tuple[11],
                            'price': pet_tuple[12],
                            'update_time': pet_tuple[13]  # ç”¨äºæ’åºçš„å­—æ®µ
                        }
                        batch_data.append(pet_dict)
                    
                    all_data.extend(batch_data)
                    self._refresh_processed_records += len(batch_data)
                    
                    progress_percentage = (self._refresh_processed_records / total_count) * 100
                    print(f"å·²å¤„ç† {self._refresh_processed_records}/{total_count} æ¡æ•°æ® ({progress_percentage:.1f}%)")
                    
                    offset += batch_size
                    
                    # æ¯å¤„ç†å‡ æ‰¹å°±å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼Œé‡Šæ”¾å†…å­˜
                    if batch_num % 5 == 0:
                        import gc
                        gc.collect()
                        
                except Exception as e:
                    self.logger.error(f"å¤„ç†ç¬¬ {batch_num + 1} æ‰¹æ•°æ®å¤±è´¥: {e}")
                    continue
            
            if not all_data:
                print("æœªæ‰¾åˆ°å® ç‰©æ•°æ®")
                self._refresh_status = "error"
                self._refresh_message = "æœªæ‰¾åˆ°å® ç‰©æ•°æ®"
                return False
            
            # è½¬æ¢ä¸ºDataFrame
            self._refresh_message = "æ„å»ºæ•°æ®ç»“æ„..."
            self._refresh_progress = 92
            
            df = pd.DataFrame(all_data)
            print(f"æ€»å…±åŠ è½½ {len(df)} æ¡å® ç‰©æ•°æ®")
            
            # å­˜å‚¨åˆ°Redisåˆ†å—ç¼“å­˜
            self._refresh_message = "ä¿å­˜åˆ°Redisç¼“å­˜..."
            self._refresh_progress = 95
            
            chunk_size = 500  # å‡å°å—å¤§å°ï¼Œé¿å…è¶…æ—¶
            ttl_seconds = None if self._cache_ttl_hours == -1 else self._cache_ttl_hours * 3600
            
            print(f"å‡†å¤‡å­˜å‚¨åˆ°Redisï¼Œæ•°æ®é‡: {len(df)} æ¡ï¼Œå—å¤§å°: {chunk_size}")
            
            # æ— ç¼æ›´æ–°ç­–ç•¥ï¼šå…ˆå­˜å‚¨æ–°æ•°æ®ï¼Œå†æ¸…ç†æ—§æ•°æ®
            # ä½¿ç”¨ä¸´æ—¶é”®åå­˜å‚¨æ–°æ•°æ®ï¼Œé¿å…è¦†ç›–ç°æœ‰æ•°æ®
            temp_cache_key = f"{self._full_cache_key}_temp_{int(time.time())}"
            print(f"ä½¿ç”¨ä¸´æ—¶é”®åå­˜å‚¨æ–°æ•°æ®: {temp_cache_key}")
            
            # é‡è¯•æœºåˆ¶ - å…ˆå­˜å‚¨åˆ°ä¸´æ—¶é”®
            success = False
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    print(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•å­˜å‚¨æ–°æ•°æ®åˆ°ä¸´æ—¶é”®...")
                    temp_hash_key = f"{temp_cache_key}:hash"
                    success = self.redis_cache.set_hash_data(
                        hash_key=temp_hash_key,
                        data=df,
                        ttl=ttl_seconds
                    )
                    if success:
                        print("æ–°æ•°æ®å­˜å‚¨åˆ°ä¸´æ—¶é”®æˆåŠŸï¼")
                        break
                    else:
                        print(f"ç¬¬ {attempt + 1} æ¬¡å­˜å‚¨å¤±è´¥ï¼Œå‡†å¤‡é‡è¯•...")
                except Exception as e:
                    print(f"ç¬¬ {attempt + 1} æ¬¡å­˜å‚¨å¼‚å¸¸: {e}")
                    if attempt < max_retries - 1:
                        import time
                        time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
                    else:
                        print("æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")
            
            if success:
                # æ–°æ•°æ®å­˜å‚¨æˆåŠŸï¼Œå¼€å§‹æ— ç¼åˆ‡æ¢
                print(" å¼€å§‹æ— ç¼åˆ‡æ¢ï¼šå°†ä¸´æ—¶æ•°æ®åˆ‡æ¢ä¸ºæ­£å¼æ•°æ®...")
                
                # 1. å…ˆæ¸…ç†æ—§çš„æ­£å¼ç¼“å­˜æ•°æ®
                print("æ¸…ç†æ—§çš„æ­£å¼ç¼“å­˜æ•°æ®...")
                old_cleared_count = self.redis_cache.clear_pattern(f"{self._full_cache_key}:*")
                if old_cleared_count > 0:
                    print(f"å·²æ¸…ç† {old_cleared_count} ä¸ªæ—§æ­£å¼ç¼“å­˜é”®")
                else:
                    print("æ²¡æœ‰æ‰¾åˆ°æ—§çš„æ­£å¼ç¼“å­˜æ•°æ®")
                
                # 2. ç›´æ¥é‡æ–°å­˜å‚¨åˆ°æ­£å¼é”®ï¼ˆæ›´ç®€å•å¯é çš„æ–¹å¼ï¼‰
                print("å°†ä¸´æ—¶æ•°æ®å¤åˆ¶åˆ°æ­£å¼é”®...")
                copy_success = self._copy_temp_cache_to_official(temp_cache_key, self._full_cache_key, df, chunk_size, ttl_seconds)
                
                if copy_success:
                    print(" æ— ç¼åˆ‡æ¢å®Œæˆï¼æ–°æ•°æ®å·²ç”Ÿæ•ˆ")
                    elapsed_time = time.time() - start_time
                    cache_info = "æ°¸ä¸è¿‡æœŸï¼ˆä»…æ‰‹åŠ¨åˆ·æ–°ï¼‰" if self._cache_ttl_hours == -1 else f"{self._cache_ttl_hours}å°æ—¶"
                    print(f"å…¨é‡å® ç‰©æ•°æ®å·²ç¼“å­˜åˆ°Redisï¼Œç¼“å­˜ç­–ç•¥: {cache_info}ï¼Œæ€»è€—æ—¶: {elapsed_time:.2f}ç§’")
                    self._full_data_cache = df  # åŒæ—¶ç¼“å­˜åˆ°å†…å­˜
                    
                    # å®Œæˆè¿›åº¦è·Ÿè¸ª
                    self._refresh_status = "completed"
                    self._refresh_progress = 100
                    self._refresh_message = "å® ç‰©æ•°æ®åŠ è½½å®Œæˆï¼"
                    
                    # æ¸…ç†ä¸´æ—¶æ•°æ®
                    print("æ¸…ç†ä¸´æ—¶æ•°æ®...")
                    self.redis_cache.clear_pattern(f"{temp_cache_key}:*")
                    
                    return True
                else:
                    print(" æ— ç¼åˆ‡æ¢å¤±è´¥ï¼Œæ¸…ç†ä¸´æ—¶æ•°æ®...")
                    self.redis_cache.clear_pattern(f"{temp_cache_key}:*")
                    self._refresh_status = "error"
                    self._refresh_message = "æ— ç¼åˆ‡æ¢å¤±è´¥"
                    return False
            else:
                print(" æ–°æ•°æ®å­˜å‚¨å¤±è´¥ï¼Œæ¸…ç†ä¸´æ—¶æ•°æ®...")
                self.redis_cache.clear_pattern(f"{temp_cache_key}:*")
                self._refresh_status = "error"
                self._refresh_message = "æ–°æ•°æ®å­˜å‚¨å¤±è´¥"
                return False
                
        except Exception as e:
            self.logger.error(f"åŠ è½½å…¨é‡æ•°æ®åˆ°Rediså¤±è´¥: {e}")
            print(f"åŠ è½½å…¨é‡æ•°æ®å¤±è´¥: {e}")
            self._refresh_status = "error"
            self._refresh_message = f"åŠ è½½å¤±è´¥: {str(e)}"
            return False

    def _get_full_data_from_redis(self) -> Optional[pd.DataFrame]:
        """ä»Redisè·å–å…¨é‡å® ç‰©æ•°æ®"""
        if not self.redis_cache:
            return None
            
        try:
            # å…ˆæ£€æŸ¥å†…å­˜ç¼“å­˜
            if self._full_data_cache is not None and not self._full_data_cache.empty:
                print(f"ä»å†…å­˜ç¼“å­˜è·å–å…¨é‡æ•°æ®: {len(self._full_data_cache)} æ¡")
                return self._full_data_cache
            
            # ä»Redisè·å–Hashæ•°æ®
            hash_key = f"{self._full_cache_key}:hash"
            cached_data = self.redis_cache.get_hash_data(hash_key)
            
            if cached_data is not None and not cached_data.empty:
                print(f"ä»Redis Hashç¼“å­˜è·å–å…¨é‡æ•°æ®: {len(cached_data)} æ¡")
                self._full_data_cache = cached_data  # ç¼“å­˜åˆ°å†…å­˜
                return cached_data
            else:
                print("Rediså…¨é‡ç¼“å­˜æœªå‘½ä¸­")
                return None
                
        except Exception as e:
            self.logger.warning(f"ä»Redisè·å–å…¨é‡æ•°æ®å¤±è´¥: {e}")
            return None

    def _filter_data_from_full_cache(self, full_data: pd.DataFrame, **filters) -> pd.DataFrame:
        """
        ä»Rediså…¨é‡æ•°æ®ä¸­è¿›è¡Œç­›é€‰ - ä½¿ç”¨pandasé«˜æ•ˆç­›é€‰
        
        Args:
            full_data: å…¨é‡å® ç‰©æ•°æ®
            **filters: ç­›é€‰æ¡ä»¶
            
        Returns:
            ç­›é€‰åçš„DataFrame
        """
        try:
            filtered_df = full_data.copy()
            
            # åŸºç¡€ç­›é€‰æ¡ä»¶
            level_range = filters.get('level_range')
            role_grade_limit_range = filters.get('role_grade_limit_range')
            price_range = filters.get('price_range')
            server = filters.get('server')
            all_skill = filters.get('all_skill')
            limit = filters.get('limit', 1000)
            
            print(f"å¼€å§‹ä» {len(filtered_df)} æ¡å…¨é‡æ•°æ®ä¸­ç­›é€‰...")
            
            # 1. ç­‰çº§èŒƒå›´ç­›é€‰
            if level_range:
                min_level, max_level = level_range
                filtered_df = filtered_df[
                    (filtered_df['equip_level'] >= min_level) & 
                    (filtered_df['equip_level'] <= max_level)
                ]
                print(f"æŒ‰ç­‰çº§èŒƒå›´({min_level}-{max_level})ç­›é€‰å: {len(filtered_df)} æ¡")
            
            # 2. æºå¸¦ç­‰çº§èŒƒå›´ç­›é€‰
            if role_grade_limit_range:
                min_role_grade_limit, max_role_grade_limit = role_grade_limit_range
                filtered_df = filtered_df[
                    (filtered_df['role_grade_limit'] >= min_role_grade_limit) & 
                    (filtered_df['role_grade_limit'] <= max_role_grade_limit)
                ]
                print(f"æŒ‰æºå¸¦ç­‰çº§èŒƒå›´({min_role_grade_limit}-{max_role_grade_limit})ç­›é€‰å: {len(filtered_df)} æ¡")
            
            # 3. ä»·æ ¼èŒƒå›´ç­›é€‰
            if price_range:
                min_price, max_price = price_range
                filtered_df = filtered_df[
                    (filtered_df['price'] >= min_price) & 
                    (filtered_df['price'] <= max_price)
                ]
                print(f"æŒ‰ä»·æ ¼èŒƒå›´({min_price}-{max_price})ç­›é€‰å: {len(filtered_df)} æ¡")
            
            # 4. æœåŠ¡å™¨ç­›é€‰
            if server:
                filtered_df = filtered_df[filtered_df['server_name'] == server]
                print(f"æŒ‰æœåŠ¡å™¨({server})ç­›é€‰å: {len(filtered_df)} æ¡")
            
            # 5. æŠ€èƒ½ç­›é€‰
            if all_skill:
                target_skills = []
                if isinstance(all_skill, str):
                    target_skills = [s for s in all_skill.split('|') if s]
                elif isinstance(all_skill, list):
                    target_skills = [str(s) for s in all_skill if s]
                
                if target_skills:
                    target_set = set(target_skills)
                    def match(row):
                        all_skill_val = row.get('all_skill', '')
                        skill_set = set(all_skill_val.split('|')) if all_skill_val else set()
                        return target_set.issubset(skill_set)
                    filtered_df = filtered_df[filtered_df.apply(match, axis=1)]
                    print(f"æŒ‰æŠ€èƒ½ç­›é€‰å: {len(filtered_df)} æ¡")
            
            # 6. æŒ‰æ›´æ–°æ—¶é—´æ’åºå¹¶é™åˆ¶æ•°é‡
            if 'update_time' in filtered_df.columns:
                # ç¡®ä¿update_timeæ˜¯datetimeç±»å‹
                if filtered_df['update_time'].dtype == 'object':
                    filtered_df['update_time'] = pd.to_datetime(filtered_df['update_time'])
                
                filtered_df = filtered_df.sort_values('update_time', ascending=False)
            
            # 7. é™åˆ¶è¿”å›æ•°é‡
            if len(filtered_df) > limit:
                filtered_df = filtered_df.head(limit)
                print(f"é™åˆ¶è¿”å›æ•°é‡åˆ°: {limit} æ¡")
            
            return filtered_df
            
        except Exception as e:
            self.logger.error(f"ä»Rediså…¨é‡æ•°æ®ç­›é€‰å¤±è´¥: {e}")
            print(f"Redisç­›é€‰å¼‚å¸¸: {e}")
            return pd.DataFrame()

    def _copy_temp_cache_to_official(self, temp_key: str, official_key: str, df: pd.DataFrame, chunk_size: int, ttl_seconds: Optional[int]) -> bool:
        """
        å°†ä¸´æ—¶ç¼“å­˜å¤åˆ¶åˆ°æ­£å¼ç¼“å­˜ï¼ˆæ— ç¼åˆ‡æ¢ï¼‰
        
        Args:
            temp_key: ä¸´æ—¶ç¼“å­˜é”®å
            official_key: æ­£å¼ç¼“å­˜é”®å
            df: æ•°æ®DataFrame
            chunk_size: å—å¤§å°
            ttl_seconds: TTLç§’æ•°
            
        Returns:
            bool: æ˜¯å¦å¤åˆ¶æˆåŠŸ
        """
        try:
            print(f"å¼€å§‹å¤åˆ¶ä¸´æ—¶ç¼“å­˜ {temp_key} åˆ°æ­£å¼ç¼“å­˜ {official_key}...")
            
            # ç›´æ¥ä½¿ç”¨set_hash_dataé‡æ–°å­˜å‚¨åˆ°æ­£å¼Hashé”®
            official_hash_key = f"{official_key}:hash"
            success = self.redis_cache.set_hash_data(
                hash_key=official_hash_key,
                data=df,
                ttl=ttl_seconds
            )
            
            if success:
                print(" ä¸´æ—¶ç¼“å­˜å¤åˆ¶åˆ°æ­£å¼ç¼“å­˜æˆåŠŸ")
                return True
            else:
                print(" ä¸´æ—¶ç¼“å­˜å¤åˆ¶åˆ°æ­£å¼ç¼“å­˜å¤±è´¥")
                return False
                
        except Exception as e:
            print(f" å¤åˆ¶ä¸´æ—¶ç¼“å­˜å¤±è´¥: {e}")
            return False

    def refresh_full_cache(self) -> bool:
        """æ‰‹åŠ¨åˆ·æ–°å…¨é‡ç¼“å­˜"""
        print(" æ‰‹åŠ¨åˆ·æ–°å® ç‰©å…¨é‡ç¼“å­˜...")
        self._full_data_cache = None  # æ¸…ç©ºå†…å­˜ç¼“å­˜
        return self._load_full_data_to_redis(force_refresh=True)
    
    def set_cache_expiry(self, hours: int):
        """
        è®¾ç½®ç¼“å­˜è¿‡æœŸæ—¶é—´
        
        Args:
            hours: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆå°æ—¶ï¼‰ï¼Œ-1è¡¨ç¤ºæ°¸ä¸è¿‡æœŸ
        """
        self._cache_ttl_hours = hours
        if hours == -1:
            print("å® ç‰©ç¼“å­˜è®¾ç½®ä¸ºæ°¸ä¸è¿‡æœŸæ¨¡å¼ï¼ˆä»…æ‰‹åŠ¨åˆ·æ–°ï¼‰")
        else:
            print(f"å® ç‰©ç¼“å­˜è®¾ç½®ä¸º {hours} å°æ—¶è‡ªåŠ¨è¿‡æœŸ")
    
    def manual_refresh(self) -> bool:
        """
        æ‰‹åŠ¨åˆ·æ–°ç¼“å­˜ï¼ˆæ˜¾å¼è°ƒç”¨ï¼‰
        """
        print(" ç”¨æˆ·æ‰‹åŠ¨åˆ·æ–°å® ç‰©ç¼“å­˜")
        return self.refresh_full_cache()

    def get_cache_status(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜çŠ¶æ€ä¿¡æ¯"""
        try:
            # è·å–MySQLå® ç‰©æ•°æ®æ€»æ•°
            mysql_count = self._get_mysql_pets_count()
            
            status = {
                'redis_available': False,
                'full_cache_exists': False,
                'full_cache_size': 0,
                'memory_cache_size': 0,
                'cache_key': self._full_cache_key,
                'cache_ttl_hours': self._cache_ttl_hours,
                'cache_never_expires': self._cache_ttl_hours == -1,
                'refresh_mode': 'manual_only' if self._cache_ttl_hours == -1 else 'auto_expire',
                'mysql_data_count': mysql_count
            }
            
            if self.redis_cache and self.redis_cache.is_available():
                status['redis_available'] = True
                
                # æ£€æŸ¥Redisä¸­çš„å…¨é‡ç¼“å­˜
                try:
                    metadata = self.redis_cache.get(f"{self._full_cache_key}:meta")
                    if metadata:
                        status['full_cache_exists'] = True
                        status['full_cache_size'] = metadata.get('total_rows', 0)
                        status['cache_created_at'] = metadata.get('created_at')
                        status['chunk_info'] = {
                            'total_chunks': metadata.get('total_chunks', 0),
                            'chunk_size': metadata.get('chunk_size', 0)
                        }
                except Exception as e:
                    self.logger.debug(f"æ£€æŸ¥Redisç¼“å­˜çŠ¶æ€å¤±è´¥: {e}")
            
            # æ£€æŸ¥å†…å­˜ç¼“å­˜
            if self._full_data_cache is not None:
                status['memory_cache_size'] = len(self._full_data_cache)
            
            return status
            
        except Exception as e:
            self.logger.error(f"è·å–ç¼“å­˜çŠ¶æ€å¤±è´¥: {e}")
            return {'error': str(e)}

    def get_refresh_status(self) -> Dict[str, Any]:
        """
        è·å–åˆ·æ–°è¿›åº¦çŠ¶æ€
        
        Returns:
            Dict: åŒ…å«è¿›åº¦ä¿¡æ¯çš„å­—å…¸
        """
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        print(f"ğŸ” è·å–åˆ·æ–°çŠ¶æ€ - å®ä¾‹ID: {id(self)}")
        print(f"ğŸ” å½“å‰çŠ¶æ€: {self._refresh_status}, è¿›åº¦: {self._refresh_progress}%")
        print(f"ğŸ” å†…å­˜ç¼“å­˜çŠ¶æ€: {self._full_data_cache is not None and not self._full_data_cache.empty if self._full_data_cache is not None else False}")
        
        status_info = {
            "status": self._refresh_status,
            "progress": self._refresh_progress,
            "message": self._refresh_message,
            "processed_records": self._refresh_processed_records,
            "total_records": self._refresh_total_records,
            "current_batch": self._refresh_current_batch,
            "total_batches": self._refresh_total_batches,
            "start_time": self._refresh_start_time.isoformat() if self._refresh_start_time else None,
            "elapsed_seconds": int((datetime.now() - self._refresh_start_time).total_seconds()) if self._refresh_start_time else 0
        }
        
        return status_info

    @classmethod
    def clear_cache(cls):
        """æ¸…ç©ºå•ä¾‹å®ä¾‹çš„ç¼“å­˜"""
        with cls._lock:
            if cls._instance and hasattr(cls._instance, '_full_data_cache'):
                cls._instance._full_data_cache = None
                cls._instance._refresh_status = "idle"
                cls._instance._refresh_progress = 0
                cls._instance._refresh_message = ""
                print("å·²æ¸…ç©ºå® ç‰©æ•°æ®ç¼“å­˜")

    @classmethod
    def get_cache_status_static(cls) -> Dict[str, Any]:
        """è·å–å•ä¾‹å®ä¾‹çš„ç¼“å­˜çŠ¶æ€"""
        with cls._lock:
            if cls._instance:
                return cls._instance.get_cache_status()
            else:
                return {
                    'redis_available': False,
                    'full_cache_exists': False,
                    'full_cache_size': 0,
                    'memory_cache_size': 0
                }

    @classmethod
    def get_refresh_status_static(cls) -> Dict[str, Any]:
        """è·å–å•ä¾‹å®ä¾‹çš„åˆ·æ–°çŠ¶æ€"""
        with cls._lock:
            if cls._instance:
                return cls._instance.get_refresh_status()
            else:
                return {
                    "status": "idle",
                    "progress": 0,
                    "message": "",
                    "processed_records": 0,
                    "total_records": 0,
                    "current_batch": 0,
                    "total_batches": 0,
                    "start_time": None,
                    "elapsed_seconds": 0
                }