from datetime import datetime
from typing import Dict, Any, List, Optional, Union, Tuple
import logging
import pandas as pd
import os

# ä»é…ç½®æ–‡ä»¶åŠ è½½å¸¸é‡
from .constant import (
    get_agility_suits, get_magic_suits, get_high_value_suits,
    get_precise_filter_suits, get_high_value_effects, get_important_effects,
    get_high_value_simple_levels, get_simple_effect_id,
    get_low_value_special_skills, get_low_value_effects
)

# å¥—è£…æ•ˆæœIDå¸¸é‡å®šä¹‰
# é«˜ä»·å€¼å¥—è£…
AGILITY_SUITS = get_agility_suits()  # æ•æ·å¥—è£…
MAGIC_SUITS = get_magic_suits()  # é­”åŠ›å¥—è£…
HIGH_VALUE_SUITS = get_high_value_suits()  # åˆå¹¶é«˜ä»·å€¼å¥—è£…

# ç²¾ç¡®ç­›é€‰å¥—è£…ï¼ˆå…è®¸ç²¾ç¡®ç­›é€‰çš„å¥—è£…æ•ˆæœï¼‰
PRECISE_FILTER_SUITS = get_precise_filter_suits()  # å®šå¿ƒæœ¯ã€å˜èº«ã€ç¢æ˜Ÿè¯€ã€å¤©ç¥æŠ¤ä½“ã€æ»¡å¤©èŠ±é›¨ã€æµªæ¶Œ

# é«˜ä»·å€¼ç‰¹æ•ˆ
HIGH_VALUE_EFFECTS = get_high_value_effects()  # æ— çº§åˆ«ï¼Œæ„¤æ€’ï¼Œæ°¸ä¸ç£¨æŸ é«˜ä»·å€¼ç‰¹æ•ˆ
IMPORTANT_EFFECTS = get_important_effects()  # ç›¸ä¼¼åº¦è®¡ç®—ä¸­é‡è¦çš„ç‰¹æ•ˆ

# é«˜ä»·å€¼ç®€æ˜“è£…å¤‡ç­‰çº§
HIGH_VALUE_EQUIP_LEVELS = get_high_value_simple_levels()  # é«˜ä»·å€¼ç®€æ˜“è£…å¤‡ç­‰çº§
SIMPLE_EFFECT_ID = get_simple_effect_id()  # ç®€æ˜“è£…å¤‡ç‰¹æ•ˆç¼–å·

# ä½ä»·å€¼ç‰¹æŠ€
LOW_VALUE_SPECIAL_SKILLS = get_low_value_special_skills()
# ä½ä»·å€¼ç‰¹æ•ˆ
LOW_VALUE_EFFECTS = get_low_value_effects()

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œè§£å†³æ¨¡å—å¯¼å…¥é—®é¢˜
current_dir = os.path.dirname(os.path.abspath(__file__))

try:
    from ...feature_extractor.equip_feature_extractor import EquipFeatureExtractor
    from src.database import db
    from src.models.equipment import Equipment
    from sqlalchemy import and_, or_, func, text
except ImportError:
    try:
        from src.evaluator.feature_extractor.equip_feature_extractor import EquipFeatureExtractor
        from src.database import db
        from src.models.equipment import Equipment
        from sqlalchemy import and_, or_, func, text
    except ImportError:
        # å¦‚æœéƒ½å¯¼å…¥å¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„å ä½ç¬¦
        class EquipFeatureExtractor:
            def __init__(self):
                pass

            def extract_features(self, equip_data):
                return {}


class EquipMarketDataCollector:
    """è£…å¤‡å¸‚åœºæ•°æ®é‡‡é›†å™¨ - ä»æ•°æ®åº“ä¸­è·å–å’Œå¤„ç†è£…å¤‡å¸‚åœºæ•°æ®"""

    _instance = None  # å•ä¾‹å®ä¾‹
    _lock = None  # çº¿ç¨‹é”ï¼Œç¡®ä¿çº¿ç¨‹å®‰å…¨
    
    def __new__(cls):
        """å•ä¾‹æ¨¡å¼å®ç°"""
        import threading
        if cls._lock is None:
            cls._lock = threading.Lock()
            
        with cls._lock:
            if cls._instance is None:
                instance = super(EquipMarketDataCollector, cls).__new__(cls)
                cls._instance = instance
                # æ ‡è®°å®ä¾‹æ˜¯å¦å·²åˆå§‹åŒ–ï¼Œé¿å…é‡å¤åˆå§‹åŒ–
                instance._initialized = False
                print("åˆ›å»ºæ–°çš„ EquipMarketDataCollector å•ä¾‹å®ä¾‹")
            else:
                print("ä½¿ç”¨ç°æœ‰çš„ EquipMarketDataCollector å•ä¾‹å®ä¾‹")
            
            return cls._instance

    def __init__(self):
        """
        åˆå§‹åŒ–è£…å¤‡å¸‚åœºæ•°æ®é‡‡é›†å™¨ - æ”¯æŒRediså…¨é‡ç¼“å­˜ï¼ˆå•ä¾‹æ¨¡å¼ä¸‹åªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰
        """
        # é¿å…é‡å¤åˆå§‹åŒ–
        if hasattr(self, '_initialized') and self._initialized:
            return
            
        self.feature_extractor = EquipFeatureExtractor()
        self.logger = logging.getLogger(__name__)

        # åˆå§‹åŒ–Redisç¼“å­˜
        try:
            from src.utils.redis_cache import get_redis_cache
            self.redis_cache = get_redis_cache()
            if self.redis_cache and self.redis_cache.is_available():
                self.logger.info("Redisç¼“å­˜åˆå§‹åŒ–æˆåŠŸï¼Œå°†ä½¿ç”¨Rediså…¨é‡ç¼“å­˜æ¨¡å¼")
                print("è£…å¤‡æ•°æ®é‡‡é›†å™¨åˆå§‹åŒ–ï¼Œä½¿ç”¨Rediså…¨é‡ç¼“å­˜æ¨¡å¼")
            else:
                self.redis_cache = None
                print("è£…å¤‡æ•°æ®é‡‡é›†å™¨åˆå§‹åŒ–ï¼ŒRedisä¸å¯ç”¨ï¼Œä½¿ç”¨MySQLæ•°æ®åº“")
        except Exception as e:
            self.logger.warning(f"Redisç¼“å­˜åˆå§‹åŒ–å¤±è´¥: {e}")
            self.redis_cache = None
            print("è£…å¤‡æ•°æ®é‡‡é›†å™¨åˆå§‹åŒ–ï¼ŒRedisåˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨MySQLæ•°æ®åº“")
        
        # å…¨é‡ç¼“å­˜ç›¸å…³å±æ€§
        self._full_cache_key = "equipment_market_data_full"
        self._cache_ttl_hours = -1  # æ°¸ä¸è¿‡æœŸï¼Œåªèƒ½æ‰‹åŠ¨åˆ·æ–°
        self._full_data_cache = None  # å†…å­˜ä¸­çš„å…¨é‡æ•°æ®ç¼“å­˜
        
        # è¿›åº¦è·Ÿè¸ªç›¸å…³å±æ€§
        self._refresh_status = "idle"  # idle, running, completed, error
        self._refresh_progress = 0  # 0-100
        self._refresh_message = ""
        self._refresh_start_time = None
        self._refresh_total_records = 0
        self._refresh_processed_records = 0
        self._refresh_current_batch = 0
        self._refresh_total_batches = 0
        
        # MySQLæ•°æ®ç»Ÿè®¡
        self.mysql_data_count = 0  # MySQLä¸­equipmentsè¡¨çš„æ€»è®°å½•æ•°
        
        self._initialized = True
        cache_mode = "æ°¸ä¸è¿‡æœŸæ¨¡å¼" if self._cache_ttl_hours == -1 else f"{self._cache_ttl_hours}å°æ—¶è¿‡æœŸ"
        print(f"è£…å¤‡å¸‚åœºæ•°æ®é‡‡é›†å™¨å•ä¾‹åˆå§‹åŒ–å®Œæˆï¼Œæ”¯æŒRediså…¨é‡ç¼“å­˜ï¼ˆ{cache_mode}ï¼‰")

    def _get_mysql_equipments_count(self) -> int:
        """
        è·å–MySQLä¸­equipmentsè¡¨çš„æ€»è®°å½•æ•°
        
        Returns:
            int: equipmentsè¡¨æ€»è®°å½•æ•°
        """
        try:
            from src.database import db
            from src.models.equipment import Equipment
            from flask import current_app
            from src.app import create_app
            
            # ç¡®ä¿åœ¨Flaskåº”ç”¨ä¸Šä¸‹æ–‡ä¸­
            if not current_app:
                # åˆ›å»ºåº”ç”¨ä¸Šä¸‹æ–‡
                app = create_app()
                with app.app_context():
                    return self._get_mysql_equipments_count()
            
            # æŸ¥è¯¢equipmentsè¡¨æ€»æ•°
            count = db.session.query(Equipment).count()
            self.mysql_data_count = count
            self.logger.info(f"MySQL equipmentsè¡¨æ€»è®°å½•æ•°: {count:,}")
            return count
            
        except Exception as e:
            self.logger.error(f"è·å–MySQLè£…å¤‡æ•°æ®æ€»æ•°å¤±è´¥: {e}")
            return 0

    def _load_full_data_to_redis(self, force_refresh: bool = False) -> bool:
        """
        åŠ è½½å…¨é‡è£…å¤‡æ•°æ®åˆ°Redis - å‚è€ƒè§’è‰²æ¨¡å—çš„æ‰¹æ¬¡å¤„ç†å’Œè¿›åº¦è·Ÿè¸ª
        
        Args:
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°
            
        Returns:
            bool: æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        if not self.redis_cache:
            return False
            
        try:
            import time
            from datetime import datetime
            
            # åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ª
            self._refresh_status = "running"
            self._refresh_progress = 0
            self._refresh_message = "å¼€å§‹åŠ è½½è£…å¤‡æ•°æ®..."
            self._refresh_start_time = datetime.now()
            self._refresh_processed_records = 0
            self._refresh_current_batch = 0
            
            start_time = time.time()
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜ä¸”ä¸éœ€è¦å¼ºåˆ¶åˆ·æ–°
            if not force_refresh:
                self._refresh_message = "æ£€æŸ¥Rediså…¨é‡ç¼“å­˜..."
                self._refresh_progress = 5
                
                try:
                    print("ğŸ” å¼€å§‹æ£€æŸ¥Redisç¼“å­˜...")
                    cached_data = self.redis_cache.get_chunked_data(self._full_cache_key)
                    print(f"ğŸ” Redisç¼“å­˜æ£€æŸ¥å®Œæˆï¼Œç»“æœ: {cached_data is not None}")
                    
                    if cached_data is not None and not cached_data.empty:
                        print(f"Rediså…¨é‡ç¼“å­˜å·²å­˜åœ¨ï¼Œæ•°æ®é‡: {len(cached_data)} æ¡")
                        # æ­£ç¡®è®¾ç½®çŠ¶æ€ä¿¡æ¯
                        self._refresh_status = "completed"
                        self._refresh_progress = 100
                        self._refresh_message = "ä½¿ç”¨ç°æœ‰ç¼“å­˜"
                        self._refresh_total_records = len(cached_data)
                        self._refresh_processed_records = len(cached_data)
                        self._refresh_total_batches = 1
                        self._refresh_current_batch = 1
                        # å°†æ•°æ®åŠ è½½åˆ°å†…å­˜ç¼“å­˜
                        self._full_data_cache = cached_data
                        return True
                    else:
                        print("Redisç¼“å­˜ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œå°†é‡æ–°åŠ è½½æ•°æ®")
                except Exception as e:
                    print(f"æ£€æŸ¥Redisç¼“å­˜æ—¶å‡ºé”™: {e}")
                    self._refresh_message = f"æ£€æŸ¥ç¼“å­˜å¤±è´¥: {str(e)}"
                    # ç»§ç»­æ‰§è¡Œé‡æ–°åŠ è½½
            
            print("ğŸ§ª [ä¸´æ—¶æµ‹è¯•æ¨¡å¼] å¼€å§‹ä»MySQLåŠ è½½è£…å¤‡æ•°æ®åˆ°Redis...")
            
            # ä»æ•°æ®åº“åŠ è½½å…¨é‡æ•°æ®
            from src.database import db
            from src.models.equipment import Equipment
            from flask import current_app
            from src.app import create_app
            
            # ç¡®ä¿åœ¨Flaskåº”ç”¨ä¸Šä¸‹æ–‡ä¸­
            if not current_app:
                # åˆ›å»ºåº”ç”¨ä¸Šä¸‹æ–‡
                app = create_app()
                with app.app_context():
                    return self._load_full_data_to_redis(force_refresh)
            
            # è·å–æ€»è®°å½•æ•°
            self._refresh_message = "ç»Ÿè®¡æ•°æ®æ€»é‡..."
            self._refresh_progress = 10
            
            # è·å–MySQLè£…å¤‡æ•°æ®æ€»æ•°
            full_count = db.session.query(Equipment).count()
            self.mysql_data_count = full_count
            total_count = full_count  # åŠ è½½å…¨éƒ¨æ•°æ®
            # total_count = 1500  # ä¸´æ—¶æµ‹è¯•ï¼šåŠ è½½500æ¡æ•°æ®

            print(f"è£…å¤‡æ€»è®°å½•æ•°: {full_count}ï¼Œæœ¬æ¬¡åŠ è½½: {total_count} æ¡")
            
            # åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°ï¼ˆå‚è€ƒè§’è‰²æ¨¡å—ï¼‰
            if total_count > 50000:
                batch_size = 300   # å¤§æ•°æ®é›†ï¼šå°æ‰¹æ¬¡
            elif total_count > 20000:
                batch_size = 500   # ä¸­ç­‰æ•°æ®é›†
            elif total_count > 5000:
                batch_size = 800   # ä¸­å°æ•°æ®é›†
            else:
                batch_size = 300   # å°æ•°æ®é›†
            
            total_batches = (total_count + batch_size - 1) // batch_size
            self._refresh_total_records = total_count
            self._refresh_total_batches = total_batches
            
            print(f"å°†åˆ† {total_batches} æ‰¹å¤„ç†ï¼Œæ¯æ‰¹ {batch_size} æ¡")
            
            # åˆ†æ‰¹åŠ è½½æ•°æ®
            all_data = []
            offset = 0
            
            for batch_num in range(total_batches):
                # æ›´æ–°è¿›åº¦
                self._refresh_current_batch = batch_num + 1
                batch_progress = 10 + int(((batch_num + 1) / total_batches) * 80)  # 10-90%çš„è¿›åº¦èŒƒå›´
                self._refresh_progress = min(batch_progress, 90)
                self._refresh_message = f"å¤„ç†ç¬¬ {batch_num + 1}/{total_batches} æ‰¹è£…å¤‡æ•°æ®..."
                
                print(f"å¤„ç†ç¬¬ {batch_num + 1}/{total_batches} æ‰¹ï¼Œåç§»é‡: {offset}")
                
                try:
                    # æ„å»ºæ‰¹æ¬¡æŸ¥è¯¢ï¼Œç¡®ä¿ä¸è¶…è¿‡æ€»é™åˆ¶
                    remaining = total_count - offset
                    actual_limit = min(batch_size, remaining)
                    if actual_limit <= 0:
                        break
                        
                    # åªæŸ¥è¯¢ç‰¹å¾æå–å™¨éœ€è¦çš„å­—æ®µï¼ˆæ’é™¤iTypeå’ŒcDescï¼‰
                    # å¢åŠ çµé¥°ç‰¹å¾æå–å™¨éœ€è¦çš„å­—æ®µï¼š'damage', 'defense', 'magic_damage', 'magic_defense', 'fengyin', 'anti_fengyin', 'speed'
                    required_fields = [
                        Equipment.equip_level, Equipment.kindid, Equipment.init_damage, Equipment.init_damage_raw,
                        Equipment.all_damage, Equipment.init_wakan, Equipment.init_defense, Equipment.init_hp,
                        Equipment.init_dex, Equipment.mingzhong, Equipment.shanghai, Equipment.addon_tizhi,
                        Equipment.addon_liliang, Equipment.addon_naili, Equipment.addon_minjie, Equipment.addon_lingli,
                        Equipment.addon_moli, Equipment.agg_added_attrs, Equipment.gem_value, Equipment.gem_level,
                        Equipment.special_skill, Equipment.special_effect, Equipment.suit_effect, Equipment.large_equip_desc,
                        # çµé¥°ç‰¹å¾æå–å™¨éœ€è¦çš„å­—æ®µ
                        Equipment.damage, Equipment.defense, Equipment.magic_damage, Equipment.magic_defense,
                        Equipment.fengyin, Equipment.anti_fengyin, Equipment.speed,
                        # å¬å”¤å…½è£…å¤‡ç‰¹å¾æå–å™¨éœ€è¦çš„å­—æ®µ
                        Equipment.fangyu, Equipment.qixue, Equipment.addon_fali, Equipment.xiang_qian_level, Equipment.addon_status,
                        # åŸºç¡€å­—æ®µ
                        Equipment.equip_sn, Equipment.price, Equipment.server_name, Equipment.update_time
                    ]
                    query = db.session.query(*required_fields).offset(offset).limit(actual_limit)
                    equipments = query.all()
                    
                    if not equipments:
                        print(f"ç¬¬ {batch_num + 1} æ‰¹æ— æ•°æ®ï¼Œè·³è¿‡")
                        continue
                    
                    # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ - ç°åœ¨æŸ¥è¯¢è¿”å›çš„æ˜¯å…ƒç»„
                    batch_data = []
                    field_names = [
                        'equip_level', 'kindid', 'init_damage', 'init_damage_raw', 'all_damage',
                        'init_wakan', 'init_defense', 'init_hp', 'init_dex', 'mingzhong', 'shanghai',
                        'addon_tizhi', 'addon_liliang', 'addon_naili', 'addon_minjie', 'addon_lingli', 'addon_moli',
                        'agg_added_attrs', 'gem_value', 'gem_level', 'special_skill', 'special_effect', 'suit_effect',
                        'large_equip_desc',
                        # çµé¥°ç‰¹å¾æå–å™¨éœ€è¦çš„å­—æ®µ
                        'damage', 'defense', 'magic_damage', 'magic_defense', 'fengyin', 'anti_fengyin', 'speed',
                        # å¬å”¤å…½è£…å¤‡ç‰¹å¾æå–å™¨éœ€è¦çš„å­—æ®µ
                        'fangyu', 'qixue', 'addon_fali', 'xiang_qian_level', 'addon_status',
                        # åŸºç¡€å­—æ®µ
                        'equip_sn', 'price', 'server_name', 'update_time'
                    ]
                    
                    for equipment_tuple in equipments:
                        equipment_dict = {}
                        for i, value in enumerate(equipment_tuple):
                            field_name = field_names[i]
                            if hasattr(value, 'isoformat'):  # datetimeå¯¹è±¡
                                equipment_dict[field_name] = value.isoformat()
                            else:
                                equipment_dict[field_name] = value
                        batch_data.append(equipment_dict)
                    
                    all_data.extend(batch_data)
                    self._refresh_processed_records += len(batch_data)
                    
                    progress_percentage = (self._refresh_processed_records / total_count) * 100
                    print(f"å·²å¤„ç† {self._refresh_processed_records}/{total_count} æ¡æ•°æ® ({progress_percentage:.1f}%)")
                    
                    offset += batch_size
                    
                    # æ¯å¤„ç†å‡ æ‰¹å°±å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼Œé‡Šæ”¾å†…å­˜
                    if batch_num % 5 == 0:
                        import gc
                        gc.collect()
                        
                except Exception as e:
                    self.logger.error(f"å¤„ç†ç¬¬ {batch_num + 1} æ‰¹æ•°æ®å¤±è´¥: {e}")
                    continue
            
            if not all_data:
                print("æœªæ‰¾åˆ°è£…å¤‡æ•°æ®")
                self._refresh_status = "error"
                self._refresh_message = "æœªæ‰¾åˆ°è£…å¤‡æ•°æ®"
                return False
            
            # è½¬æ¢ä¸ºDataFrame
            self._refresh_message = "æ„å»ºæ•°æ®ç»“æ„..."
            self._refresh_progress = 92
            
            df = pd.DataFrame(all_data)
            print(f"æ€»å…±åŠ è½½ {len(df)} æ¡è£…å¤‡æ•°æ®")
            
            # å­˜å‚¨åˆ°Redisåˆ†å—ç¼“å­˜
            self._refresh_message = "ä¿å­˜åˆ°Redisç¼“å­˜..."
            self._refresh_progress = 95
            
            chunk_size = 500  # å‡å°å—å¤§å°ï¼Œé¿å…è¶…æ—¶
            ttl_seconds = None if self._cache_ttl_hours == -1 else self._cache_ttl_hours * 3600
            
            print(f"å‡†å¤‡å­˜å‚¨åˆ°Redisï¼Œæ•°æ®é‡: {len(df)} æ¡ï¼Œå—å¤§å°: {chunk_size}")
            
            # æ— ç¼æ›´æ–°ç­–ç•¥ï¼šå…ˆå­˜å‚¨æ–°æ•°æ®ï¼Œå†æ¸…ç†æ—§æ•°æ®
            # ä½¿ç”¨ä¸´æ—¶é”®åå­˜å‚¨æ–°æ•°æ®ï¼Œé¿å…è¦†ç›–ç°æœ‰æ•°æ®
            temp_cache_key = f"{self._full_cache_key}_temp_{int(time.time())}"
            print(f"ä½¿ç”¨ä¸´æ—¶é”®åå­˜å‚¨æ–°æ•°æ®: {temp_cache_key}")
            
            # é‡è¯•æœºåˆ¶ - å…ˆå­˜å‚¨åˆ°ä¸´æ—¶é”®
            success = False
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    print(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•å­˜å‚¨æ–°æ•°æ®åˆ°ä¸´æ—¶é”®...")
                    success = self.redis_cache.set_chunked_data(
                        base_key=temp_cache_key,
                        data=df,
                        chunk_size=chunk_size,
                        ttl=ttl_seconds
                    )
                    if success:
                        print("æ–°æ•°æ®å­˜å‚¨åˆ°ä¸´æ—¶é”®æˆåŠŸï¼")
                        break
                    else:
                        print(f"ç¬¬ {attempt + 1} æ¬¡å­˜å‚¨å¤±è´¥ï¼Œå‡†å¤‡é‡è¯•...")
                except Exception as e:
                    print(f"ç¬¬ {attempt + 1} æ¬¡å­˜å‚¨å¼‚å¸¸: {e}")
                    if attempt < max_retries - 1:
                        import time
                        time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
                    else:
                        print("æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")
            
            if success:
                # æ–°æ•°æ®å­˜å‚¨æˆåŠŸï¼Œå¼€å§‹æ— ç¼åˆ‡æ¢
                print("ğŸ”„ å¼€å§‹æ— ç¼åˆ‡æ¢ï¼šå°†ä¸´æ—¶æ•°æ®åˆ‡æ¢ä¸ºæ­£å¼æ•°æ®...")
                
                # 1. å…ˆæ¸…ç†æ—§çš„æ­£å¼ç¼“å­˜æ•°æ®
                print("æ¸…ç†æ—§çš„æ­£å¼ç¼“å­˜æ•°æ®...")
                old_cleared_count = self.redis_cache.clear_pattern(f"{self._full_cache_key}:*")
                if old_cleared_count > 0:
                    print(f"å·²æ¸…ç† {old_cleared_count} ä¸ªæ—§æ­£å¼ç¼“å­˜é”®")
                else:
                    print("æ²¡æœ‰æ‰¾åˆ°æ—§çš„æ­£å¼ç¼“å­˜æ•°æ®")
                
                # 2. ç›´æ¥é‡æ–°å­˜å‚¨åˆ°æ­£å¼é”®ï¼ˆæ›´ç®€å•å¯é çš„æ–¹å¼ï¼‰
                print("å°†ä¸´æ—¶æ•°æ®å¤åˆ¶åˆ°æ­£å¼é”®...")
                copy_success = self._copy_temp_cache_to_official(temp_cache_key, self._full_cache_key, df, chunk_size, ttl_seconds)
                
                if copy_success:
                    print(" æ— ç¼åˆ‡æ¢å®Œæˆï¼æ–°æ•°æ®å·²ç”Ÿæ•ˆ")
                    elapsed_time = time.time() - start_time
                    cache_info = "æ°¸ä¸è¿‡æœŸï¼ˆä»…æ‰‹åŠ¨åˆ·æ–°ï¼‰" if self._cache_ttl_hours == -1 else f"{self._cache_ttl_hours}å°æ—¶"
                    print(f"å…¨é‡è£…å¤‡æ•°æ®å·²ç¼“å­˜åˆ°Redisï¼Œç¼“å­˜ç­–ç•¥: {cache_info}ï¼Œæ€»è€—æ—¶: {elapsed_time:.2f}ç§’")
                    self._full_data_cache = df  # åŒæ—¶ç¼“å­˜åˆ°å†…å­˜
                    
                    # å®Œæˆè¿›åº¦è·Ÿè¸ª
                    self._refresh_status = "completed"
                    self._refresh_progress = 100
                    self._refresh_message = "è£…å¤‡æ•°æ®åŠ è½½å®Œæˆï¼"
                    
                    # æ¸…ç†ä¸´æ—¶æ•°æ®
                    print("æ¸…ç†ä¸´æ—¶æ•°æ®...")
                    self.redis_cache.clear_pattern(f"{temp_cache_key}:*")
                    
                    return True
                else:
                    print(" æ— ç¼åˆ‡æ¢å¤±è´¥ï¼Œæ¸…ç†ä¸´æ—¶æ•°æ®...")
                    self.redis_cache.clear_pattern(f"{temp_cache_key}:*")
                    self._refresh_status = "error"
                    self._refresh_message = "æ— ç¼åˆ‡æ¢å¤±è´¥"
                    return False
            else:
                print(" æ–°æ•°æ®å­˜å‚¨å¤±è´¥ï¼Œæ¸…ç†ä¸´æ—¶æ•°æ®...")
                self.redis_cache.clear_pattern(f"{temp_cache_key}:*")
                self._refresh_status = "error"
                self._refresh_message = "æ–°æ•°æ®å­˜å‚¨å¤±è´¥"
                return False
                
        except Exception as e:
            self.logger.error(f"åŠ è½½å…¨é‡æ•°æ®åˆ°Rediså¤±è´¥: {e}")
            print(f"åŠ è½½å…¨é‡æ•°æ®å¤±è´¥: {e}")
            self._refresh_status = "error"
            self._refresh_message = f"åŠ è½½å¤±è´¥: {str(e)}"
            return False

    def _get_full_data_from_redis(self) -> Optional[pd.DataFrame]:
        """ä»Redisè·å–å…¨é‡è£…å¤‡æ•°æ®"""
        if not self.redis_cache:
            return None
            
        try:
            # å…ˆæ£€æŸ¥å†…å­˜ç¼“å­˜
            if self._full_data_cache is not None and not self._full_data_cache.empty:
                print(f"ä»å†…å­˜ç¼“å­˜è·å–å…¨é‡æ•°æ®: {len(self._full_data_cache)} æ¡")
                return self._full_data_cache
            
            # ä»Redisè·å–åˆ†å—æ•°æ®
            cached_data = self.redis_cache.get_chunked_data(self._full_cache_key)
            
            if cached_data is not None and not cached_data.empty:
                print(f"ä»Redisåˆ†å—ç¼“å­˜è·å–å…¨é‡æ•°æ®: {len(cached_data)} æ¡")
                self._full_data_cache = cached_data  # ç¼“å­˜åˆ°å†…å­˜
                return cached_data
            else:
                print("Rediså…¨é‡ç¼“å­˜æœªå‘½ä¸­")
                return None
                
        except Exception as e:
            self.logger.warning(f"ä»Redisè·å–å…¨é‡æ•°æ®å¤±è´¥: {e}")
            return None

    def _filter_data_from_full_cache(self, full_data: pd.DataFrame, **filters) -> pd.DataFrame:
        """
        ä»Rediså…¨é‡æ•°æ®ä¸­è¿›è¡Œç­›é€‰ - ä½¿ç”¨pandasé«˜æ•ˆç­›é€‰
        
        Args:
            full_data: å…¨é‡è£…å¤‡æ•°æ®
            **filters: ç­›é€‰æ¡ä»¶
            
        Returns:
            ç­›é€‰åçš„DataFrame
        """
        try:
            # å®šä¹‰å®‰å…¨çš„å­—ç¬¦ä¸²åŒ…å«æ£€æŸ¥å‡½æ•°ï¼Œé¿å…æ­£åˆ™è¡¨è¾¾å¼é”™è¯¯
            def safe_contains(series, pattern):
                """å®‰å…¨çš„å­—ç¬¦ä¸²åŒ…å«æ£€æŸ¥ï¼Œé¿å…æ­£åˆ™è¡¨è¾¾å¼é”™è¯¯"""
                try:
                    return series.str.contains(pattern, na=False, regex=False)
                except Exception:
                    # å¦‚æœstr.containså¤±è´¥ï¼Œä½¿ç”¨çº¯å­—ç¬¦ä¸²åŒ¹é…
                    return series.astype(str).str.find(pattern) >= 0
            
            filtered_df = full_data.copy()
            
            # åŸºç¡€ç­›é€‰æ¡ä»¶
            kindid = filters.get('kindid')
            level_range = filters.get('level_range')
            price_range = filters.get('price_range')
            server = filters.get('server')
            special_skill = filters.get('special_skill')
            suit_effect = filters.get('suit_effect')
            special_effect = filters.get('special_effect')
            exclude_special_effect = filters.get('exclude_special_effect')
            exclude_suit_effect = filters.get('exclude_suit_effect')
            exclude_high_value_simple_equips = filters.get('exclude_high_value_simple_equips', False)
            require_high_value_suits = filters.get('require_high_value_suits', False)
            exclude_high_value_special_skills = filters.get('exclude_high_value_special_skills', False)
            limit = filters.get('limit', 1000)
            
            print(f"å¼€å§‹ä» {len(filtered_df)} æ¡å…¨é‡æ•°æ®ä¸­ç­›é€‰...")
            
            # 1. è£…å¤‡ç±»å‹ç­›é€‰
            if kindid is not None:
                filtered_df = filtered_df[filtered_df['kindid'] == kindid]
                print(f"æŒ‰è£…å¤‡ç±»å‹({kindid})ç­›é€‰å: {len(filtered_df)} æ¡")
            
            # 2. ç­‰çº§èŒƒå›´ç­›é€‰
            if level_range:
                min_level, max_level = level_range
                filtered_df = filtered_df[
                    (filtered_df['equip_level'] >= min_level) & 
                    (filtered_df['equip_level'] <= max_level)
                ]
                print(f"æŒ‰ç­‰çº§èŒƒå›´({min_level}-{max_level})ç­›é€‰å: {len(filtered_df)} æ¡")
            
            # 3. ä»·æ ¼èŒƒå›´ç­›é€‰
            if price_range:
                min_price, max_price = price_range
                filtered_df = filtered_df[
                    (filtered_df['price'] >= min_price) & 
                    (filtered_df['price'] <= max_price)
                ]
                print(f"æŒ‰ä»·æ ¼èŒƒå›´({min_price}-{max_price})ç­›é€‰å: {len(filtered_df)} æ¡")
            
            # 4. æœåŠ¡å™¨ç­›é€‰
            if server:
                filtered_df = filtered_df[filtered_df['server_name'] == server]
                print(f"æŒ‰æœåŠ¡å™¨({server})ç­›é€‰å: {len(filtered_df)} æ¡")
            
            # 5. ç‰¹æŠ€ç­›é€‰
            if special_skill is not None:
                if not exclude_high_value_special_skills:
                    filtered_df = filtered_df[filtered_df['special_skill'] == special_skill]
                    print(f"æŒ‰ç‰¹æŠ€({special_skill})ç­›é€‰å: {len(filtered_df)} æ¡")
            
            # 6. æ’é™¤é«˜ä»·å€¼ç‰¹æŠ€è£…å¤‡
            if exclude_high_value_special_skills:
                low_value_condition = (
                    (filtered_df['special_skill'] == 0) |
                    (filtered_df['special_skill'].isna()) |
                    (filtered_df['special_skill'].isin(LOW_VALUE_SPECIAL_SKILLS))
                )
                filtered_df = filtered_df[low_value_condition]
                print(f"æ’é™¤é«˜ä»·å€¼ç‰¹æŠ€å: {len(filtered_df)} æ¡")
            
            # 7. å¥—è£…æ•ˆæœç­›é€‰
            if suit_effect is not None:
                try:
                    suit_effect_num = int(suit_effect) if suit_effect is not None else 0
                    if suit_effect_num > 0:
                        filtered_df = filtered_df[filtered_df['suit_effect'] == suit_effect_num]
                        print(f"æŒ‰å¥—è£…æ•ˆæœ({suit_effect_num})ç­›é€‰å: {len(filtered_df)} æ¡")
                except (ValueError, TypeError):
                    if suit_effect and str(suit_effect).strip():
                        filtered_df = filtered_df[filtered_df['suit_effect'] == suit_effect]
                        print(f"æŒ‰å¥—è£…æ•ˆæœ({suit_effect})ç­›é€‰å: {len(filtered_df)} æ¡")
            
            # 8. å¼ºåˆ¶åŒ…å«é«˜ä»·å€¼å¥—è£…
            if require_high_value_suits:
                high_value_suits = HIGH_VALUE_SUITS
                if high_value_suits:
                    filtered_df = filtered_df[filtered_df['suit_effect'].isin(high_value_suits)]
                    print(f"å¼ºåˆ¶åŒ…å«é«˜ä»·å€¼å¥—è£…å: {len(filtered_df)} æ¡")
            
            # 9. ç‰¹æ•ˆç­›é€‰ï¼ˆJSONæ•°ç»„æ ¼å¼ï¼‰
            if special_effect is not None and special_effect and len(special_effect) > 0:
                effect_mask = pd.Series([False] * len(filtered_df))
                for effect in special_effect:
                    if effect not in LOW_VALUE_EFFECTS:
                        # ä½¿ç”¨ç²¾ç¡®çš„JSONæ•°ç»„åŒ¹é…ï¼Œé¿å…æ•°å­—åŒ…å«å…³ç³»çš„è¯¯åŒ¹é…
                        # æ¯”å¦‚effectä¸º6ï¼ŒåŒ¹é…æ¨¡å¼ï¼š[6], [6,x], [x,6], [x,6,y] ç­‰ï¼Œä½†ä¸åŒ¹é…16, 26ç­‰åŒ…å«6çš„æ•°å­—
                        effect_condition = (
                            safe_contains(filtered_df['special_effect'], f'[{effect}]') |
                            safe_contains(filtered_df['special_effect'], f'[{effect},') |
                            safe_contains(filtered_df['special_effect'], f',{effect},') |
                            safe_contains(filtered_df['special_effect'], f',{effect}]')
                        )
                        effect_mask = effect_mask | effect_condition
                
                filtered_df = filtered_df[effect_mask]
                print(f"æŒ‰ç‰¹æ•ˆç­›é€‰å: {len(filtered_df)} æ¡")
            
            # 10. æ’é™¤ç‰¹æ•ˆç­›é€‰
            if exclude_special_effect is not None and exclude_special_effect and len(exclude_special_effect) > 0:
                # ä½¿ç”¨ä¸SQLiteç‰ˆæœ¬å®Œå…¨ç›¸åŒçš„é€»è¾‘
                # SQLiteç‰ˆæœ¬ï¼šå¯¹æ¯ä¸ªç‰¹æ•ˆåˆ›å»ºæ’é™¤æ¡ä»¶ï¼Œç„¶åç”¨ANDè¿æ¥
                exclude_conditions = []
                for effect in exclude_special_effect:
                    # å¯¹æ¯ä¸ªç‰¹æ•ˆï¼Œåˆ›å»ºæ’é™¤æ¡ä»¶ï¼ˆä¸åŒ…å«è¯¥ç‰¹æ•ˆçš„è£…å¤‡ï¼‰
                    effect_exclude_condition = ~(
                        safe_contains(filtered_df['special_effect'], f'[{effect}]') |  # åªæœ‰è¿™ä¸€ä¸ªç‰¹æ•ˆï¼š[1]
                        safe_contains(filtered_df['special_effect'], f'[{effect},') |  # åœ¨å¼€å¤´ï¼š[1,x,...]
                        safe_contains(filtered_df['special_effect'], f',{effect},') |  # åœ¨ä¸­é—´ï¼š[x,1,y,...]
                        safe_contains(filtered_df['special_effect'], f',{effect}]')    # åœ¨ç»“å°¾ï¼š[x,y,1]
                    )
                    exclude_conditions.append(effect_exclude_condition)
                
                # ä½¿ç”¨ANDè¿æ¥æ‰€æœ‰æ’é™¤æ¡ä»¶ï¼ˆä¸SQLiteçš„and_(*exclude_conditions)ç›¸åŒï¼‰
                if exclude_conditions:
                    final_exclude_mask = exclude_conditions[0]
                    for condition in exclude_conditions[1:]:
                        final_exclude_mask = final_exclude_mask & condition
                    filtered_df = filtered_df[final_exclude_mask]
                
                print(f"æ’é™¤ç‰¹æ•ˆå: {len(filtered_df)} æ¡")
            
            # 11. æ’é™¤å¥—è£…æ•ˆæœ
            if exclude_suit_effect is not None and exclude_suit_effect and len(exclude_suit_effect) > 0:
                filtered_df = filtered_df[~filtered_df['suit_effect'].isin(exclude_suit_effect)]
                print(f"æ’é™¤å¥—è£…æ•ˆæœå: {len(filtered_df)} æ¡")
            
            # 12. æ’é™¤é«˜ä»·å€¼ç®€æ˜“è£…å¤‡
            if exclude_high_value_simple_equips:
                high_value_levels = HIGH_VALUE_EQUIP_LEVELS
                simple_effect_id = SIMPLE_EFFECT_ID
                
                # æ’é™¤æŒ‡å®šç­‰çº§ä¸”æœ‰ç®€æ˜“ç‰¹æ•ˆçš„è£…å¤‡
                exclude_condition = pd.Series([False] * len(filtered_df))
                for level in high_value_levels:
                    level_condition = (filtered_df['equip_level'] == level)
                    simple_condition = (
                        safe_contains(filtered_df['special_effect'], f'[{simple_effect_id}]') |
                        safe_contains(filtered_df['special_effect'], f'[{simple_effect_id},') |
                        safe_contains(filtered_df['special_effect'], f',{simple_effect_id},') |
                        safe_contains(filtered_df['special_effect'], f',{simple_effect_id}]')
                    )
                    exclude_condition = exclude_condition | (level_condition & simple_condition)
                
                filtered_df = filtered_df[~exclude_condition]
                print(f"æ’é™¤é«˜ä»·å€¼ç®€æ˜“è£…å¤‡å: {len(filtered_df)} æ¡")
            
            # 13. æŒ‰æ›´æ–°æ—¶é—´æ’åºå¹¶é™åˆ¶æ•°é‡
            if 'update_time' in filtered_df.columns:
                # ç¡®ä¿update_timeæ˜¯datetimeç±»å‹
                if filtered_df['update_time'].dtype == 'object':
                    filtered_df['update_time'] = pd.to_datetime(filtered_df['update_time'])
                
                filtered_df = filtered_df.sort_values('update_time', ascending=False)
            
            # 14. é™åˆ¶è¿”å›æ•°é‡
            if len(filtered_df) > limit:
                filtered_df = filtered_df.head(limit)
                print(f"é™åˆ¶è¿”å›æ•°é‡åˆ°: {limit} æ¡")
            
            return filtered_df
            
        except Exception as e:
            self.logger.error(f"ä»Rediså…¨é‡æ•°æ®ç­›é€‰å¤±è´¥: {e}")
            print(f"Redisç­›é€‰å¼‚å¸¸: {e}")
            return pd.DataFrame()

    def get_market_data(self,
                        kindid: Optional[int] = None,
                        level_range: Optional[Tuple[int, int]] = None,
                        price_range: Optional[Tuple[float, float]] = None,
                        server: Optional[str] = None,
                        special_skill: Optional[int] = None,
                        suit_effect: Optional[int] = None,
                        special_effect: Optional[List[str]] = None,
                        exclude_special_effect: Optional[List[int]] = None,
                        exclude_suit_effect: Optional[List[int]] = None,
                        exclude_high_value_simple_equips: bool = False,
                        require_high_value_suits: bool = False,
                        exclude_high_value_special_skills: bool = False,
                        limit: int = 1000,
                        use_redis_cache: bool = True) -> pd.DataFrame:
        """
        è·å–å¸‚åœºè£…å¤‡æ•°æ® - ä¼˜å…ˆä»Rediså…¨é‡ç¼“å­˜è·å–å¹¶ç­›é€‰

        Args:
            kindid: è£…å¤‡ç±»å‹IDç­›é€‰
            level_range: ç­‰çº§èŒƒå›´ (min_level, max_level)
            price_range: ä»·æ ¼èŒƒå›´ (min_price, max_price)
            server: æœåŠ¡å™¨ç­›é€‰
            special_effect: ç‰¹æ•ˆç­›é€‰ï¼ˆé«˜ä»·å€¼å¿…é¡»åŒ…å«ï¼‰
            require_high_value_suits: å¼ºåˆ¶åŒ…å«é«˜ä»·å€¼å¥—è£…è£…å¤‡ï¼ˆé­”åŠ›å¥—å’Œæ•æ·å¥—ï¼‰
            special_skill: ç‰¹æŠ€ç­›é€‰ï¼ˆå¿…é¡»å®Œå…¨ä¸€è‡´ï¼‰
            exclude_special_effect: æ’é™¤ç‰¹æ•ˆç­›é€‰ï¼ˆæ’é™¤å…·æœ‰è¿™äº›ç‰¹æ•ˆçš„è£…å¤‡ï¼‰
            exclude_suit_effect: æ’é™¤å¥—è£…æ•ˆæœç­›é€‰ï¼ˆæ’é™¤å…·æœ‰è¿™äº›å¥—è£…æ•ˆæœçš„è£…å¤‡ï¼‰
            exclude_high_value_simple_equips: æ’é™¤é«˜ä»·å€¼ç®€æ˜“è£…å¤‡ï¼ˆ70/90/110/130çº§çš„ç®€æ˜“è£…å¤‡ï¼‰
            exclude_high_value_special_skills: æ’é™¤é«˜ä»·å€¼ç‰¹æŠ€è£…å¤‡ï¼ˆåªæœç´¢æ— ç‰¹æŠ€æˆ–ä½ä»·å€¼ç‰¹æŠ€è£…å¤‡ï¼‰
            suit_effect: å¥—è£…æ•ˆæœæœ‰ä¸‰ç§ï¼Œå¦‚æœæ˜¯"é™„åŠ çŠ¶æ€"/"è¿½åŠ æ³•æœ¯"ï¼Œåˆ™ä¼ å…¥è¿‡æ»¤ï¼Œ"å˜èº«æœ¯"/"å˜åŒ–å’’"ï¼Œåˆ™ä¸ä¼ å…¥è¿‡æ»¤ï¼Œåœ¨é”šå®šçš„æ—¶å€™åšèšç±»
            limit: è¿”å›æ•°æ®æ¡æ•°é™åˆ¶
            use_redis_cache: æ˜¯å¦ä½¿ç”¨Redisç¼“å­˜

        Returns:
            è£…å¤‡å¸‚åœºæ•°æ®DataFrame
            
        """
        try:
            import time
            start_time = time.time()
            
            # ä¼˜å…ˆä»Rediså…¨é‡ç¼“å­˜è·å–æ•°æ®
            if use_redis_cache and self.redis_cache:
                full_data = self._get_full_data_from_redis()
                
                if full_data is None or full_data.empty:
                    # ç¼“å­˜æœªå‘½ä¸­ï¼Œå°è¯•åŠ è½½å…¨é‡æ•°æ®åˆ°Redis
                    print("Redisç¼“å­˜æœªå‘½ä¸­ï¼Œå¼€å§‹åŠ è½½å…¨é‡æ•°æ®...")
                    if self._load_full_data_to_redis():
                        full_data = self._get_full_data_from_redis()
                
                if full_data is not None and not full_data.empty:
                    # ä»Rediså…¨é‡æ•°æ®ä¸­è¿›è¡Œç­›é€‰
                    filtered_data = self._filter_data_from_full_cache(
                        full_data=full_data,
                        kindid=kindid,
                        level_range=level_range,
                        price_range=price_range,
                        server=server,
                        special_skill=special_skill,
                        suit_effect=suit_effect,
                        special_effect=special_effect,
                        exclude_special_effect=exclude_special_effect,
                        exclude_suit_effect=exclude_suit_effect,
                        exclude_high_value_simple_equips=exclude_high_value_simple_equips,
                        require_high_value_suits=require_high_value_suits,
                        exclude_high_value_special_skills=exclude_high_value_special_skills,
                        limit=limit
                    )
                    
                    elapsed_time = time.time() - start_time
                    print(f"ä»Rediså…¨é‡ç¼“å­˜ç­›é€‰å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.3f}ç§’ï¼Œè¿”å›: {len(filtered_data)} æ¡æ•°æ®")
                    return filtered_data
            
            # é™çº§åˆ°MySQLæŸ¥è¯¢
            print("ä½¿ç”¨MySQLæ•°æ®åº“æŸ¥è¯¢ï¼ˆé™çº§æ¨¡å¼ï¼‰...")
            return self._get_market_data_from_mysql(
                kindid=kindid,
                level_range=level_range,
                price_range=price_range,
                server=server,
                special_skill=special_skill,
                suit_effect=suit_effect,
                special_effect=special_effect,
                exclude_special_effect=exclude_special_effect,
                exclude_suit_effect=exclude_suit_effect,
                exclude_high_value_simple_equips=exclude_high_value_simple_equips,
                require_high_value_suits=require_high_value_suits,
                exclude_high_value_special_skills=exclude_high_value_special_skills,
                limit=limit
            )

        except Exception as e:
            self.logger.error(f"è·å–å¸‚åœºæ•°æ®å¤±è´¥: {e}")
            print(f"æŸ¥è¯¢å¼‚å¸¸: {e}")
            return pd.DataFrame()

    def _get_market_data_from_mysql(self,
                                   kindid: Optional[int] = None,
                                   level_range: Optional[Tuple[int, int]] = None,
                                   price_range: Optional[Tuple[float, float]] = None,
                                   server: Optional[str] = None,
                                   special_skill: Optional[int] = None,
                                   suit_effect: Optional[int] = None,
                                   special_effect: Optional[List[str]] = None,
                                   exclude_special_effect: Optional[List[int]] = None,
                                   exclude_suit_effect: Optional[List[int]] = None,
                                   exclude_high_value_simple_equips: bool = False,
                                   require_high_value_suits: bool = False,
                                   exclude_high_value_special_skills: bool = False,
                                   limit: int = 1000) -> pd.DataFrame:
        """
        ä»MySQLæ•°æ®åº“è·å–è£…å¤‡æ•°æ®ï¼ˆåŸå§‹æŸ¥è¯¢é€»è¾‘ï¼‰
        """
        try:
            from src.database import db
            from src.models.equipment import Equipment
            from flask import current_app
            from src.app import create_app
            
            # ç¡®ä¿åœ¨Flaskåº”ç”¨ä¸Šä¸‹æ–‡ä¸­
            if not current_app:
                # åˆ›å»ºåº”ç”¨ä¸Šä¸‹æ–‡
                app = create_app()
                with app.app_context():
                    return self._get_market_data_from_mysql(
                        kindid=kindid,
                        level_range=level_range,
                        price_range=price_range,
                        server=server,
                        special_skill=special_skill,
                        suit_effect=suit_effect,
                        special_effect=special_effect,
                        exclude_special_effect=exclude_special_effect,
                        exclude_suit_effect=exclude_suit_effect,
                        exclude_high_value_simple_equips=exclude_high_value_simple_equips,
                        require_high_value_suits=require_high_value_suits,
                        exclude_high_value_special_skills=exclude_high_value_special_skills,
                        limit=limit
                    )
            
            # æ„å»ºSQLAlchemyæŸ¥è¯¢ - åªæŸ¥è¯¢ç‰¹å¾æå–å™¨éœ€è¦çš„å­—æ®µ
            # æ ¹æ®ç‰¹å¾æå–å™¨ç»Ÿè®¡ï¼Œéœ€è¦ä»¥ä¸‹å­—æ®µï¼ˆæ’é™¤iTypeå’ŒcDescï¼‰ï¼š
            # å¢åŠ çµé¥°ç‰¹å¾æå–å™¨éœ€è¦çš„å­—æ®µï¼š'damage', 'defense', 'magic_damage', 'magic_defense', 'fengyin', 'anti_fengyin', 'speed'
            required_fields = [
                Equipment.equip_level,
                Equipment.kindid,
                Equipment.init_damage,
                Equipment.init_damage_raw,
                Equipment.all_damage,
                Equipment.init_wakan,
                Equipment.init_defense,
                Equipment.init_hp,
                Equipment.init_dex,
                Equipment.mingzhong,
                Equipment.shanghai,
                Equipment.addon_tizhi,
                Equipment.addon_liliang,
                Equipment.addon_naili,
                Equipment.addon_minjie,
                Equipment.addon_lingli,
                Equipment.addon_moli,
                Equipment.agg_added_attrs,
                Equipment.gem_value,
                Equipment.gem_level,
                Equipment.special_skill,
                Equipment.special_effect,
                Equipment.suit_effect,
                Equipment.large_equip_desc,
                # çµé¥°ç‰¹å¾æå–å™¨éœ€è¦çš„å­—æ®µ
                Equipment.damage,
                Equipment.defense,
                Equipment.magic_damage,
                Equipment.magic_defense,
                Equipment.fengyin,
                Equipment.anti_fengyin,
                Equipment.speed,
                # ä¿ç•™ä¸€äº›å¿…è¦çš„å…ƒæ•°æ®å­—æ®µ
                Equipment.equip_sn,
                Equipment.price,
                Equipment.server_name,
                Equipment.update_time
            ]
            query = db.session.query(*required_fields)

            if kindid is not None:
                query = query.filter(Equipment.kindid == kindid)

            if special_skill is not None:
                # åªæœ‰å½“ä¸éœ€è¦æ’é™¤é«˜ä»·å€¼ç‰¹æŠ€æ—¶ï¼Œæ‰æ·»åŠ å…·ä½“çš„ç‰¹æŠ€ç­›é€‰æ¡ä»¶
                if not (exclude_high_value_special_skills or not require_high_value_suits):
                    query = query.filter(Equipment.special_skill == special_skill)
                else:
                    print(f"è·³è¿‡å…·ä½“ç‰¹æŠ€ç­›é€‰ï¼Œå› ä¸ºéœ€è¦æ’é™¤é«˜ä»·å€¼ç‰¹æŠ€")

            # ç‰¹æŠ€ç­›é€‰é€»è¾‘ï¼šå¦‚æœç›®æ ‡è£…å¤‡æ˜¯ä½ä»·å€¼ç‰¹æŠ€ï¼Œåˆ™æ’é™¤é«˜ä»·å€¼ç‰¹æŠ€è£…å¤‡
            if exclude_high_value_special_skills:
                # åªèƒ½æœç´¢æ— ç‰¹æŠ€æˆ–ä½ä»·å€¼ç‰¹æŠ€çš„è£…å¤‡
                query = query.filter(
                    or_(
                        Equipment.special_skill == 0,
                        Equipment.special_skill.is_(None),
                        Equipment.special_skill.in_(LOW_VALUE_SPECIAL_SKILLS)
                    )
                )
                print(f"ç‰¹æŠ€ç­›é€‰ï¼šåªæœç´¢æ— ç‰¹æŠ€æˆ–ä½ä»·å€¼ç‰¹æŠ€è£…å¤‡")

            if suit_effect is not None:
                # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•°å­—åå†æ¯”è¾ƒï¼ˆpet_equipé™¤å¤–ï¼Œå…¶ä»–éƒ½æ˜¯æ•°å­—å­—ç¬¦ä¸²ï¼‰
                try:
                    suit_effect_num = int(suit_effect) if suit_effect is not None else 0
                    if suit_effect_num > 0:
                        query = query.filter(Equipment.suit_effect == suit_effect_num)
                except (ValueError, TypeError):
                    # å¦‚æœè½¬æ¢å¤±è´¥ï¼ˆå¯èƒ½æ˜¯pet_equipçš„å­—ç¬¦ä¸²å¥—è£…ï¼‰ï¼Œç›´æ¥ä½¿ç”¨åŸå€¼
                    if suit_effect and str(suit_effect).strip():
                        query = query.filter(Equipment.suit_effect == suit_effect)

            if require_high_value_suits:
                # å¼ºåˆ¶åŒ…å«é«˜ä»·å€¼å¥—è£…ï¼šåªæœç´¢é­”åŠ›å¥—å’Œæ•æ·å¥—è£…å¤‡
                high_value_suits = HIGH_VALUE_SUITS
                if high_value_suits:
                    query = query.filter(Equipment.suit_effect.in_(high_value_suits))
                    print(f"å¼ºåˆ¶åŒ…å«é«˜ä»·å€¼å¥—è£…ï¼šåªæœç´¢é­”åŠ›å¥—å’Œæ•æ·å¥—è£…å¤‡")

            if special_effect is not None:
                # ç‰¹æ•ˆç­›é€‰ï¼ˆå¤šé€‰ï¼ŒJSONæ•°ç»„æ ¼å¼ï¼‰
                if special_effect and len(special_effect) > 0:
                    effect_conditions = []
                    for effect in special_effect:
                        effect_conditions.append(
                            or_(
                                Equipment.special_effect.like(f'[{effect}]'),
                                Equipment.special_effect.like(f'[{effect},%'),
                                Equipment.special_effect.like(f'%,{effect},%'),
                                Equipment.special_effect.like(f'%,{effect}]')
                            )
                        )
                    
                    if effect_conditions:
                        query = query.filter(or_(*effect_conditions))

            if exclude_special_effect is not None:
                # æ’é™¤ç‰¹æ•ˆç­›é€‰ï¼šæ’é™¤å…·æœ‰æŒ‡å®šç‰¹æ•ˆçš„è£…å¤‡
                if exclude_special_effect and len(exclude_special_effect) > 0:
                    exclude_conditions = []
                    for effect in exclude_special_effect:
                        exclude_conditions.append(
                            ~or_(
                                Equipment.special_effect.like(f'[{effect}]'),
                                Equipment.special_effect.like(f'[{effect},%'),
                                Equipment.special_effect.like(f'%,{effect},%'),
                                Equipment.special_effect.like(f'%,{effect}]')
                            )
                        )
                    
                    if exclude_conditions:
                        query = query.filter(and_(*exclude_conditions))

            if exclude_suit_effect is not None:
                # æ’é™¤å¥—è£…æ•ˆæœç­›é€‰ï¼šæ’é™¤å…·æœ‰æŒ‡å®šå¥—è£…æ•ˆæœçš„è£…å¤‡
                if exclude_suit_effect and len(exclude_suit_effect) > 0:
                    query = query.filter(~Equipment.suit_effect.in_(exclude_suit_effect))

            if exclude_high_value_simple_equips:
                # æ’é™¤é«˜ä»·å€¼ç®€æ˜“è£…å¤‡ï¼šæ’é™¤70çº§/90çº§/110çº§/130çº§ä¸”æœ‰ç®€æ˜“ç‰¹æ•ˆ(2)çš„è£…å¤‡
                high_value_levels = HIGH_VALUE_EQUIP_LEVELS
                simple_effect_conditions = []
                
                for level in high_value_levels:
                    # å¯¹äºæ¯ä¸ªé«˜ä»·å€¼ç­‰çº§ï¼Œæ’é™¤è¯¥ç­‰çº§ä¸”æœ‰ç®€æ˜“ç‰¹æ•ˆçš„è£…å¤‡
                    simple_conditions = or_(
                        Equipment.special_effect.like(f'[{SIMPLE_EFFECT_ID}]'),
                        Equipment.special_effect.like(f'[{SIMPLE_EFFECT_ID},%'),
                        Equipment.special_effect.like(f'%,{SIMPLE_EFFECT_ID},%'),
                        Equipment.special_effect.like(f'%,{SIMPLE_EFFECT_ID}]')
                    )
                    simple_effect_conditions.append(
                        ~(Equipment.equip_level == level) | ~simple_conditions
                    )
                
                if simple_effect_conditions:
                    query = query.filter(and_(*simple_effect_conditions))
                    print(f"æ’é™¤é«˜ä»·å€¼ç®€æ˜“è£…å¤‡ï¼š70çº§/90çº§/110çº§/130çº§ä¸”æœ‰ç®€æ˜“ç‰¹æ•ˆçš„è£…å¤‡")

            if level_range:
                query = query.filter(Equipment.equip_level.between(level_range[0], level_range[1]))

            if price_range:
                query = query.filter(Equipment.price.between(price_range[0], price_range[1]))

            if server:
                query = query.filter(Equipment.server_name == server)

            # æ’åºå’Œé™åˆ¶
            query = query.order_by(Equipment.update_time.desc()).limit(limit)

            # æ‰§è¡ŒæŸ¥è¯¢å¹¶è½¬æ¢ä¸ºDataFrame
            equipments = query.all()
            
            if equipments:
                # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨ - ç°åœ¨æŸ¥è¯¢è¿”å›çš„æ˜¯å…ƒç»„
                data_list = []
                field_names = [
                    'equip_level', 'kindid', 'init_damage', 'init_damage_raw', 'all_damage',
                    'init_wakan', 'init_defense', 'init_hp', 'init_dex', 'mingzhong', 'shanghai',
                    'addon_tizhi', 'addon_liliang', 'addon_naili', 'addon_minjie', 'addon_lingli', 'addon_moli',
                    'agg_added_attrs', 'gem_value', 'gem_level', 'special_skill', 'special_effect', 'suit_effect',
                    'large_equip_desc',
                    # çµé¥°ç‰¹å¾æå–å™¨éœ€è¦çš„å­—æ®µ
                    'damage', 'defense', 'magic_damage', 'magic_defense', 'fengyin', 'anti_fengyin', 'speed',
                    # åŸºç¡€å­—æ®µ
                    'equip_sn', 'price', 'server_name', 'update_time'
                ]
                
                for equipment_tuple in equipments:
                    equipment_dict = {}
                    for i, value in enumerate(equipment_tuple):
                        field_name = field_names[i]
                        if hasattr(value, 'isoformat'):  # datetimeå¯¹è±¡
                            equipment_dict[field_name] = value.isoformat()
                        else:
                            equipment_dict[field_name] = value
                    data_list.append(equipment_dict)
                
                df = pd.DataFrame(data_list)
                print(f"ä»MySQLæ•°æ®åº“åŠ è½½äº† {len(df)} æ¡è£…å¤‡æ•°æ®")
                return df
            else:
                print(f"ä»MySQLæ•°æ®åº“æŸ¥è¯¢åˆ°0æ¡æ•°æ®")
                return pd.DataFrame()

        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢è£…å¤‡æ•°æ®å¤±è´¥: {e}")
            print(f"SQLæ‰§è¡Œå¼‚å¸¸: {e}")
            return pd.DataFrame()

    def get_market_data_for_similarity(self,
                                       target_features: Dict[str, Any]) -> pd.DataFrame:
        """
        è·å–ç”¨äºç›¸ä¼¼åº¦è®¡ç®—çš„å¸‚åœºæ•°æ®

        ä¸“é—¨ä¸ºç›¸ä¼¼åº¦è®¡ç®—ä¼˜åŒ–çš„æ•°æ®è·å–æ–¹æ³•ï¼ŒåŒ…å«ä»¥ä¸‹ç‰¹æ®Šé€»è¾‘ï¼š
        1. é«˜ä»·å€¼ç‰¹æ•ˆçš„å…¬å¹³æ€§ç­›é€‰
        2. é«˜ä»·å€¼å¥—è£…çš„å…¬å¹³æ€§ç­›é€‰
        3. ç›¸ä¼¼åº¦è®¡ç®—ç›¸å…³çš„ç‰¹æ•ˆç­›é€‰
        4. é€‚å½“çš„æ•°æ®é‡æ§åˆ¶

        Args:
            target_features: ç›®æ ‡è£…å¤‡ç‰¹å¾

        Returns:
            å¸‚åœºæ•°æ®DataFrame
        """
        try:
            # æå–åŸºç¡€ç­›é€‰æ¡ä»¶
            level_range = target_features.get('equip_level_range', None)
            kindid = target_features.get('kindid', None)
            special_skill = target_features.get('special_skill', 0)
            special_effect = target_features.get('special_effect', [])
            suit_effect = target_features.get('suit_effect', 0)

            # å¤„ç†ç‰¹æ•ˆç­›é€‰é€»è¾‘
            filtered_special_effect = None
            exclude_special_effect = None
            exclude_high_value_simple_equips = False

            # ä½¿ç”¨é¢„å®šä¹‰çš„ç‰¹æ•ˆå’Œç­‰çº§å¸¸é‡
            high_value_effects = HIGH_VALUE_EFFECTS
            important_effects = IMPORTANT_EFFECTS

            # ç®€æ˜“è£…å¤‡ç‰¹æ®Šé€»è¾‘å¤„ç†
            target_equip_level = target_features.get('equip_level', 0)
            simple_effect_id = SIMPLE_EFFECT_ID
            high_value_equip_levels = HIGH_VALUE_EQUIP_LEVELS

            # æ£€æŸ¥ç›®æ ‡è£…å¤‡æ˜¯å¦åŒ…å«é«˜ä»·å€¼ç‰¹æ•ˆ
            target_has_high_value_effects = False
            target_has_simple_effect = False
            
            if special_effect and len(special_effect) > 0:
                # ç­›é€‰å‡ºé‡è¦ç‰¹æ•ˆç”¨äºç›¸ä¼¼åº¦è®¡ç®—
                filtered_effects = []
                for effect in special_effect:
                    if effect in important_effects:
                        # å¦‚æœç‰¹æ•ˆæ˜¯ SIMPLE_EFFECT_IDï¼Œåˆ™ç­‰çº§è¦ç¬¦åˆ HIGH_VALUE_EQUIP_LEVELS
                        if effect != simple_effect_id or (effect == simple_effect_id and target_equip_level in high_value_equip_levels):
                            filtered_effects.append(effect)
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«åŸºç¡€é«˜ä»·å€¼ç‰¹æ•ˆï¼ˆæ— çº§åˆ«ã€æ„¤æ€’ã€æ°¸ä¸ç£¨æŸï¼‰
                    if effect in high_value_effects:
                        target_has_high_value_effects = True
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«ç®€æ˜“ç‰¹æ•ˆ
                    if effect == simple_effect_id:
                        target_has_simple_effect = True

                if filtered_effects:
                    filtered_special_effect = filtered_effects

            # åˆ¤æ–­ç›®æ ‡è£…å¤‡æ˜¯å¦æœ‰é«˜ä»·å€¼ç®€æ˜“ç‰¹æ•ˆ
            if target_has_simple_effect and target_equip_level in high_value_equip_levels:
                target_has_high_value_effects = True
                print(f"ç›®æ ‡è£…å¤‡{target_equip_level}çº§ç®€æ˜“è£…å¤‡è§†ä¸ºé«˜ä»·å€¼ç‰¹æ•ˆ")

            # å…¬å¹³æ€§ç­›é€‰ï¼šå¦‚æœç›®æ ‡è£…å¤‡ä¸åŒ…å«é«˜ä»·å€¼ç‰¹æ•ˆï¼Œåˆ™æ’é™¤å…·æœ‰è¿™äº›ç‰¹æ•ˆçš„è£…å¤‡
            if not target_has_high_value_effects:
                # æ’é™¤åŸºç¡€é«˜ä»·å€¼ç‰¹æ•ˆ
                exclude_special_effect = high_value_effects
                # æ’é™¤é«˜ä»·å€¼ç®€æ˜“è£…å¤‡ï¼ˆ70/90/130çº§çš„ç®€æ˜“è£…å¤‡ï¼‰
                exclude_high_value_simple_equips = True
                print(
                    f"ç›®æ ‡è£…å¤‡ä¸åŒ…å«é«˜ä»·å€¼ç‰¹æ•ˆï¼Œå°†æ’é™¤åŸºç¡€é«˜ä»·å€¼ç‰¹æ•ˆ {high_value_effects} å’Œ70çº§/90çº§/130çº§ç®€æ˜“è£…å¤‡")

            # å¤„ç†å¥—è£…æ•ˆæœç­›é€‰é€»è¾‘
            exclude_suit_effect = []
            require_high_value_suits = False

            # ä½¿ç”¨é¢„å®šä¹‰çš„é«˜ä»·å€¼å¥—è£…å’Œç²¾ç¡®ç­›é€‰å¥—è£…
            high_value_suits = HIGH_VALUE_SUITS
            precise_filter_suits = PRECISE_FILTER_SUITS

            # æ£€æŸ¥ç›®æ ‡è£…å¤‡çš„å¥—è£…ç±»å‹
            target_has_high_value_suits = False
            target_has_precise_filter_suits = False

            # å¤„ç†suit_effectï¼šå°è¯•è½¬æ¢ä¸ºæ•°å­—ï¼Œå¦‚æœå¤±è´¥åˆ™ä¿æŒåŸå€¼
            suit_effect_value = None
            if suit_effect:
                try:
                    suit_effect_value = int(suit_effect)
                except (ValueError, TypeError):
                    # è½¬æ¢å¤±è´¥ï¼ˆå¯èƒ½æ˜¯pet_equipçš„å­—ç¬¦ä¸²å¥—è£…ï¼‰ï¼Œä¿æŒåŸå€¼
                    suit_effect_value = suit_effect

            if suit_effect_value:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«é«˜ä»·å€¼å¥—è£…ï¼ˆåªå¯¹æ•°å­—å¥—è£…æœ‰æ•ˆï¼‰
                if isinstance(suit_effect_value, int) and suit_effect_value in high_value_suits:
                    target_has_high_value_suits = True
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ç²¾ç¡®ç­›é€‰å¥—è£…ï¼ˆåªå¯¹æ•°å­—å¥—è£…æœ‰æ•ˆï¼‰TODO: éœ€è¦ä¼˜åŒ–éœ€è¦ä¼˜åŒ–éœ€è¦ä¼˜åŒ–éœ€è¦ä¼˜åŒ–éœ€è¦ä¼˜åŒ–éœ€è¦ä¼˜åŒ–éœ€è¦ä¼˜åŒ–
                elif isinstance(suit_effect_value, int) and suit_effect_value in precise_filter_suits:
                    target_has_precise_filter_suits = True

            # å¥—è£…ç­›é€‰é€»è¾‘
            if target_has_high_value_suits:
                # æƒ…å†µ1ï¼šç›®æ ‡è£…å¤‡æœ‰é«˜ä»·å€¼å¥—è£… â†’ å¼ºåˆ¶åªæœç´¢é«˜ä»·å€¼å¥—è£…è£…å¤‡
                require_high_value_suits = True
                print(f"ç›®æ ‡è£…å¤‡åŒ…å«é«˜ä»·å€¼å¥—è£… {suit_effect}ï¼Œå¼ºåˆ¶åªæœç´¢é«˜ä»·å€¼å¥—è£…è£…å¤‡")
            elif target_has_precise_filter_suits:
                # æƒ…å†µ2ï¼šç›®æ ‡è£…å¤‡æœ‰ç²¾ç¡®ç­›é€‰å¥—è£… â†’ ç²¾ç¡®ç­›é€‰è¯¥å¥—è£…è£…å¤‡ï¼Œæ’é™¤å…¶ä»–é«˜ä»·å€¼å¥—è£…
                exclude_suit_effect = high_value_suits + \
                    [s for s in precise_filter_suits if s != suit_effect_value]
                print(f"ç›®æ ‡è£…å¤‡åŒ…å«ç²¾ç¡®ç­›é€‰å¥—è£… {suit_effect}ï¼Œå°†ç²¾ç¡®ç­›é€‰è¯¥å¥—è£…ï¼Œæ’é™¤å…¶ä»–ç²¾ç¡®ç­›é€‰å¥—è£…å’Œé«˜ä»·å€¼å¥—è£…")
            else:
                # æƒ…å†µ3ï¼šç›®æ ‡è£…å¤‡æ²¡æœ‰å¥—è£…æˆ–æœ‰å…¶ä»–å¥—è£… â†’ æ’é™¤é«˜ä»·å€¼å¥—è£…å’Œç²¾ç¡®ç­›é€‰å¥—è£…
                exclude_suit_effect = high_value_suits + precise_filter_suits
                print(f"ç›®æ ‡è£…å¤‡ä¸åŒ…å«é«˜ä»·å€¼å¥—è£…å’Œç²¾ç¡®ç­›é€‰å¥—è£…ï¼Œå°†æ’é™¤è¿™äº›å¥—è£…çš„è£…å¤‡")

            # ç‰¹æŠ€ç­›é€‰é€»è¾‘
            exclude_high_value_special_skills = False
            if require_high_value_suits or (special_effect and len(special_effect) > 0 and not target_has_high_value_effects):
                # å¦‚æœå¼ºåˆ¶é«˜ä»·å€¼å¥—è£…ï¼Œæˆ–è€…æœ‰ç‰¹æ•ˆä½†ä¸æ˜¯é«˜ä»·å€¼ç‰¹æ•ˆï¼Œåˆ™æ£€æŸ¥ç‰¹æŠ€
                if special_skill and special_skill in LOW_VALUE_SPECIAL_SKILLS:
                    # å¦‚æœç›®æ ‡è£…å¤‡æ˜¯ä½ä»·å€¼ç‰¹æŠ€ï¼Œåˆ™åªèƒ½æœç´¢æ— ç‰¹æŠ€æˆ–ä½ä»·å€¼ç‰¹æŠ€çš„è£…å¤‡
                    exclude_high_value_special_skills = True
                    print(f"ç›®æ ‡è£…å¤‡æ˜¯ä½ä»·å€¼ç‰¹æŠ€ {special_skill}ï¼Œå°†æ’é™¤é«˜ä»·å€¼ç‰¹æŠ€è£…å¤‡")

            print(
                f"ç›¸ä¼¼åº¦ç­›é€‰ - é‡è¦ç‰¹æ•ˆ: {filtered_special_effect}, æ’é™¤ç‰¹æ•ˆ: {exclude_special_effect}")
            print(
                f"ç›¸ä¼¼åº¦ç­›é€‰ - æ’é™¤å¥—è£…: {exclude_suit_effect}, å¼ºåˆ¶é«˜ä»·å€¼å¥—è£…: {require_high_value_suits}")

            # å±æ€§ç‚¹åŠ æˆåˆ†ç±»ï¼Œå±æ€§ç‚¹åŠ æˆç±»å‹ä¸€èˆ¬æˆå¯¹å‡ºç°ï¼›æœ‰ä¸‰ç§æƒ…å†µ:1ã€ç©ºç™½ï¼Œå³æ²¡æœ‰å±æ€§ç‚¹ï¼›2ã€ä¸€ä¸ªå±æ€§åŠ æˆï¼ˆæ­£/è´Ÿï¼‰ï¼›3ã€ä¸€å¯¹å±æ€§åŠ æˆï¼ˆä¸¤ä¸ªæ­£æ•°ï¼Œæˆ–è€…ä¸€æ­£ä¸€è´Ÿï¼‰ï¼›
            # å¦‚æœä¸¤ä¸ªéƒ½æ˜¯æ­£æ•°åˆ™æ˜¯ç»„åˆåŒåŠ ï¼Œå¦‚ä½“è´¨+10å’Œè€åŠ›+10éƒ½æ˜¯æ­£æ•°åˆ™æ˜¯ä½“è€ï¼›
            # å¦‚æœå•ç§å±æ€§æ­£æ•°ï¼Œå¦‚ä½“è´¨+10ï¼Œåˆ™æ˜¯ä½“è´¨ï¼›
            # å¦‚æœä¸€ä¸ªæ­£æ•°ä¸€ä¸ªè´Ÿæ•°ï¼Œå¦‚ä½“è´¨+15ï¼Œæ•æ·-2ï¼Œåˆ™æ˜¯ä½“è´¨ï¼›
            # åˆ†å››ä¸ªèšç±»
            # 1."ä½“è´¨", "ä½“è€",
            # 2."é­”åŠ›", "é­”ä½“","é­”è€","é­”æ•",
            # 3."è€åŠ›",
            # 4."æ•æ·","æ•ä½“","æ•è€",
            # 5."åŠ›é‡", "åŠ›ä½“","åŠ›é­”","åŠ›è€","åŠ›æ•"

            # è°ƒç”¨åŸºç¡€æ•°æ®è·å–æ–¹æ³•ï¼ˆåº”ç”¨ä¸šåŠ¡è§„åˆ™ï¼‰
            market_data = self.get_market_data_with_business_rules(
                target_features=target_features,
                special_effect=filtered_special_effect,
                exclude_special_effect=exclude_special_effect,
                exclude_suit_effect=exclude_suit_effect if exclude_suit_effect else None,
                exclude_high_value_simple_equips=exclude_high_value_simple_equips,
                require_high_value_suits=require_high_value_suits,
                exclude_high_value_special_skills=exclude_high_value_special_skills,
                limit=2000  # ç›¸ä¼¼åº¦è®¡ç®—éœ€è¦æ›´å¤šæ•°æ®
            )

            return market_data

        except Exception as e:
            self.logger.error(f"è·å–ç›¸ä¼¼åº¦è®¡ç®—å¸‚åœºæ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()

    def refresh_full_cache(self) -> bool:
        """æ‰‹åŠ¨åˆ·æ–°å…¨é‡ç¼“å­˜"""
        print("ğŸ”„ æ‰‹åŠ¨åˆ·æ–°è£…å¤‡å…¨é‡ç¼“å­˜...")
        self._full_data_cache = None  # æ¸…ç©ºå†…å­˜ç¼“å­˜
        return self._load_full_data_to_redis(force_refresh=True)
    
    def set_cache_expiry(self, hours: int):
        """
        è®¾ç½®ç¼“å­˜è¿‡æœŸæ—¶é—´
        
        Args:
            hours: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆå°æ—¶ï¼‰ï¼Œ-1è¡¨ç¤ºæ°¸ä¸è¿‡æœŸ
        """
        self._cache_ttl_hours = hours
        if hours == -1:
            print("è£…å¤‡ç¼“å­˜è®¾ç½®ä¸ºæ°¸ä¸è¿‡æœŸæ¨¡å¼ï¼ˆä»…æ‰‹åŠ¨åˆ·æ–°ï¼‰")
        else:
            print(f"è£…å¤‡ç¼“å­˜è®¾ç½®ä¸º {hours} å°æ—¶è‡ªåŠ¨è¿‡æœŸ")
    
    def manual_refresh(self) -> bool:
        """
        æ‰‹åŠ¨åˆ·æ–°ç¼“å­˜ï¼ˆæ˜¾å¼è°ƒç”¨ï¼‰
        """
        print(" ç”¨æˆ·æ‰‹åŠ¨åˆ·æ–°è£…å¤‡ç¼“å­˜")
        return self.refresh_full_cache()
    
    def incremental_update(self, last_update_time: Optional[datetime] = None) -> bool:
        """
        å¢é‡æ›´æ–°ç¼“å­˜æ•°æ® - å…ˆæ“ä½œå†…å­˜ç¼“å­˜ï¼Œå†åŒæ­¥åˆ°Redis
        
        Args:
            last_update_time: ä¸Šæ¬¡æ›´æ–°æ—¶é—´ï¼Œå¦‚æœä¸ºNoneåˆ™ä»ç¼“å­˜å…ƒæ•°æ®è·å–
            
        Returns:
            bool: æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        try:
            print("ğŸ”„ å¼€å§‹å¢é‡æ›´æ–°è£…å¤‡ç¼“å­˜...")
            
            if not self.redis_cache:
                print(" Redisä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œå¢é‡æ›´æ–°")
                return False
            
            # è·å–ä¸Šæ¬¡æ›´æ–°æ—¶é—´
            if last_update_time is None:
                last_update_time = self._get_last_cache_update_time()
                if last_update_time is None:
                    print(" æ— æ³•è·å–ä¸Šæ¬¡æ›´æ–°æ—¶é—´ï¼Œå°†è¿›è¡Œå…¨é‡åˆ·æ–°")
                    return self.refresh_full_cache()
            
            print(f"ğŸ“… ä¸Šæ¬¡æ›´æ–°æ—¶é—´: {last_update_time}")
            
            # æŸ¥è¯¢æ–°å¢æˆ–æ›´æ–°çš„æ•°æ®
            new_data = self._get_incremental_data_from_mysql(last_update_time)
            
            if new_data.empty:
                print(" æ²¡æœ‰æ–°æ•°æ®éœ€è¦æ›´æ–°")
                return True
            
            print(f" å‘ç° {len(new_data)} æ¡æ–°æ•°æ®")
            
            # ä¼˜å…ˆä»å†…å­˜ç¼“å­˜è·å–ç°æœ‰æ•°æ®
            existing_data = self._get_existing_data_from_memory()
            if existing_data is None or existing_data.empty:
                print(" å†…å­˜ç¼“å­˜ä¸ºç©ºï¼Œå°è¯•ä»Redisè·å–")
                existing_data = self._get_full_data_from_redis()
                if existing_data is None or existing_data.empty:
                    print(" Redisç¼“å­˜ä¹Ÿä¸ºç©ºï¼Œå°†è¿›è¡Œå…¨é‡åˆ·æ–°")
                    return self.refresh_full_cache()
                # å°†Redisæ•°æ®åŠ è½½åˆ°å†…å­˜ç¼“å­˜
                self._full_data_cache = existing_data
                print(" å·²å°†Redisæ•°æ®åŠ è½½åˆ°å†…å­˜ç¼“å­˜")
            
            print(f" ç°æœ‰å†…å­˜ç¼“å­˜æ•°æ®: {len(existing_data)} æ¡")
            
            # åˆå¹¶æ•°æ®åˆ°å†…å­˜ç¼“å­˜
            merged_data = self._merge_incremental_data(existing_data, new_data)
            
            # æ›´æ–°å†…å­˜ç¼“å­˜
            self._full_data_cache = merged_data
            print(f" å†…å­˜ç¼“å­˜æ›´æ–°å®Œæˆï¼Œæ€»æ•°æ®é‡: {len(merged_data)} æ¡")
            
            # åŒæ­¥åˆ°Redisç¼“å­˜
            success = self._sync_memory_cache_to_redis(merged_data)
            
            if success:
                print(f" å¢é‡æ›´æ–°å®Œæˆï¼Œæ•°æ®å·²åŒæ­¥åˆ°Redis")
                # æ›´æ–°ç¼“å­˜å…ƒæ•°æ®ä¸­çš„æœ€åæ›´æ–°æ—¶é—´
                self._update_cache_metadata(merged_data)
                return True
            else:
                print(" RedisåŒæ­¥å¤±è´¥ï¼Œä½†å†…å­˜ç¼“å­˜å·²æ›´æ–°")
                return False
                
        except Exception as e:
            self.logger.error(f"å¢é‡æ›´æ–°å¤±è´¥: {e}")
            print(f" å¢é‡æ›´æ–°å¼‚å¸¸: {e}")
            return False
    
    def _get_last_cache_update_time(self) -> Optional[datetime]:
        """
        è·å–ç¼“å­˜ä¸­è®°å½•çš„æœ€åæ›´æ–°æ—¶é—´ - ä¼˜å…ˆä»å†…å­˜ç¼“å­˜è·å–
        
        Returns:
            Optional[datetime]: æœ€åæ›´æ–°æ—¶é—´ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        try:
            # ä¼˜å…ˆä»å†…å­˜ç¼“å­˜è·å–æœ€åæ›´æ–°æ—¶é—´
            memory_data = self._get_existing_data_from_memory()
            if memory_data is not None and not memory_data.empty and 'update_time' in memory_data.columns:
                # ç¡®ä¿update_timeæ˜¯datetimeç±»å‹
                if memory_data['update_time'].dtype == 'object':
                    memory_data['update_time'] = pd.to_datetime(memory_data['update_time'])
                
                max_time = memory_data['update_time'].max()
                if pd.notna(max_time):
                    print(f"ğŸ“… ä»å†…å­˜ç¼“å­˜è·å–æœ€åæ›´æ–°æ—¶é—´: {max_time}")
                    return max_time.to_pydatetime()
            
            # å¦‚æœå†…å­˜ç¼“å­˜ä¸­æ²¡æœ‰æ•°æ®ï¼Œå°è¯•ä»Redisè·å–
            if not self.redis_cache:
                return None
            
            print("ğŸ“… å†…å­˜ç¼“å­˜ä¸ºç©ºï¼Œå°è¯•ä»Redisè·å–æœ€åæ›´æ–°æ—¶é—´...")
            
            # ä»ç¼“å­˜å…ƒæ•°æ®è·å–æœ€åæ›´æ–°æ—¶é—´
            metadata = self.redis_cache.get(f"{self._full_cache_key}:meta")
            if metadata and 'last_update_time' in metadata:
                last_time = datetime.fromisoformat(metadata['last_update_time'])
                print(f"ğŸ“… ä»Rediså…ƒæ•°æ®è·å–æœ€åæ›´æ–°æ—¶é—´: {last_time}")
                return last_time
            
            # å¦‚æœå…ƒæ•°æ®ä¸­æ²¡æœ‰ï¼Œå°è¯•ä»æ•°æ®ä¸­è·å–æœ€æ–°çš„update_time
            cached_data = self.redis_cache.get_chunked_data(self._full_cache_key)
            if cached_data is not None and not cached_data.empty and 'update_time' in cached_data.columns:
                # ç¡®ä¿update_timeæ˜¯datetimeç±»å‹
                if cached_data['update_time'].dtype == 'object':
                    cached_data['update_time'] = pd.to_datetime(cached_data['update_time'])
                
                max_time = cached_data['update_time'].max()
                if pd.notna(max_time):
                    print(f"ğŸ“… ä»Redisæ•°æ®è·å–æœ€åæ›´æ–°æ—¶é—´: {max_time}")
                    return max_time.to_pydatetime()
            
            print("ğŸ“… æœªæ‰¾åˆ°æœ€åæ›´æ–°æ—¶é—´")
            return None
            
        except Exception as e:
            self.logger.warning(f"è·å–æœ€åæ›´æ–°æ—¶é—´å¤±è´¥: {e}")
            return None
    
    def _get_incremental_data_from_mysql(self, last_update_time: datetime) -> pd.DataFrame:
        """
        ä»MySQLè·å–å¢é‡æ•°æ®
        
        Args:
            last_update_time: ä¸Šæ¬¡æ›´æ–°æ—¶é—´
            
        Returns:
            pd.DataFrame: å¢é‡æ•°æ®
        """
        try:
            from src.database import db
            from src.models.equipment import Equipment
            from flask import current_app
            from src.app import create_app
            
            # ç¡®ä¿åœ¨Flaskåº”ç”¨ä¸Šä¸‹æ–‡ä¸­
            if not current_app:
                app = create_app()
                with app.app_context():
                    return self._get_incremental_data_from_mysql(last_update_time)
            
            # æŸ¥è¯¢è‡ªä¸Šæ¬¡æ›´æ–°ä»¥æ¥çš„æ–°æ•°æ®
            required_fields = [
                Equipment.equip_level, Equipment.kindid, Equipment.init_damage, Equipment.init_damage_raw,
                Equipment.all_damage, Equipment.init_wakan, Equipment.init_defense, Equipment.init_hp,
                Equipment.init_dex, Equipment.mingzhong, Equipment.shanghai, Equipment.addon_tizhi,
                Equipment.addon_liliang, Equipment.addon_naili, Equipment.addon_minjie, Equipment.addon_lingli,
                Equipment.addon_moli, Equipment.agg_added_attrs, Equipment.gem_value, Equipment.gem_level,
                Equipment.special_skill, Equipment.special_effect, Equipment.suit_effect, Equipment.large_equip_desc,
                # çµé¥°ç‰¹å¾æå–å™¨éœ€è¦çš„å­—æ®µ
                Equipment.damage, Equipment.defense, Equipment.magic_damage, Equipment.magic_defense,
                Equipment.fengyin, Equipment.anti_fengyin, Equipment.speed,
                # å¬å”¤å…½è£…å¤‡ç‰¹å¾æå–å™¨éœ€è¦çš„å­—æ®µ
                Equipment.fangyu, Equipment.qixue, Equipment.addon_fali, Equipment.xiang_qian_level, Equipment.addon_status,
                # åŸºç¡€å­—æ®µ
                Equipment.equip_sn, Equipment.price, Equipment.server_name, Equipment.update_time
            ]
            
            query = db.session.query(*required_fields).filter(
                Equipment.update_time > last_update_time
            ).order_by(Equipment.update_time.desc())
            
            equipments = query.all()
            
            if not equipments:
                return pd.DataFrame()
            
            # è½¬æ¢ä¸ºDataFrame
            field_names = [
                'equip_level', 'kindid', 'init_damage', 'init_damage_raw', 'all_damage',
                'init_wakan', 'init_defense', 'init_hp', 'init_dex', 'mingzhong', 'shanghai',
                'addon_tizhi', 'addon_liliang', 'addon_naili', 'addon_minjie', 'addon_lingli', 'addon_moli',
                'agg_added_attrs', 'gem_value', 'gem_level', 'special_skill', 'special_effect', 'suit_effect',
                'large_equip_desc',
                # çµé¥°ç‰¹å¾æå–å™¨éœ€è¦çš„å­—æ®µ
                'damage', 'defense', 'magic_damage', 'magic_defense', 'fengyin', 'anti_fengyin', 'speed',
                # å¬å”¤å…½è£…å¤‡ç‰¹å¾æå–å™¨éœ€è¦çš„å­—æ®µ
                'fangyu', 'qixue', 'addon_fali', 'xiang_qian_level', 'addon_status',
                # åŸºç¡€å­—æ®µ
                'equip_sn', 'price', 'server_name', 'update_time'
            ]
            
            data_list = []
            for equipment_tuple in equipments:
                equipment_dict = {}
                for i, value in enumerate(equipment_tuple):
                    field_name = field_names[i]
                    if hasattr(value, 'isoformat'):  # datetimeå¯¹è±¡
                        equipment_dict[field_name] = value.isoformat()
                    else:
                        equipment_dict[field_name] = value
                data_list.append(equipment_dict)
            
            df = pd.DataFrame(data_list)
            print(f"ä»MySQLè·å–å¢é‡æ•°æ®: {len(df)} æ¡")
            return df
            
        except Exception as e:
            self.logger.error(f"è·å–å¢é‡æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _merge_incremental_data(self, existing_data: pd.DataFrame, new_data: pd.DataFrame) -> pd.DataFrame:
        """
        åˆå¹¶ç°æœ‰æ•°æ®å’Œå¢é‡æ•°æ®
        
        Args:
            existing_data: ç°æœ‰ç¼“å­˜æ•°æ®
            new_data: æ–°å¢æ•°æ®
            
        Returns:
            pd.DataFrame: åˆå¹¶åçš„æ•°æ®
        """
        try:
            # ç¡®ä¿ä¸¤ä¸ªDataFrameçš„åˆ—é¡ºåºä¸€è‡´
            if not existing_data.empty and not new_data.empty:
                # ç¡®ä¿åˆ—é¡ºåºä¸€è‡´
                new_data = new_data.reindex(columns=existing_data.columns, fill_value=None)
            
            # åˆå¹¶æ•°æ®ï¼Œæ–°æ•°æ®ä¼šè¦†ç›–ç›¸åŒequip_snçš„æ—§æ•°æ®
            if existing_data.empty:
                merged_data = new_data
            elif new_data.empty:
                merged_data = existing_data
            else:
                # ä½¿ç”¨equip_snä½œä¸ºä¸»é”®è¿›è¡Œåˆå¹¶
                # å…ˆåˆ é™¤ç°æœ‰æ•°æ®ä¸­ä¸æ–°æ•°æ®é‡å¤çš„è®°å½•
                existing_data = existing_data[~existing_data['equip_sn'].isin(new_data['equip_sn'])]
                # ç„¶ååˆå¹¶
                merged_data = pd.concat([existing_data, new_data], ignore_index=True)
            
            print(f"æ•°æ®åˆå¹¶å®Œæˆ: ç°æœ‰ {len(existing_data)} æ¡ + æ–°å¢ {len(new_data)} æ¡ = æ€»è®¡ {len(merged_data)} æ¡")
            return merged_data
            
        except Exception as e:
            self.logger.error(f"åˆå¹¶æ•°æ®å¤±è´¥: {e}")
            return existing_data
    
    def _get_existing_data_from_memory(self) -> Optional[pd.DataFrame]:
        """
        ä»å†…å­˜ç¼“å­˜è·å–ç°æœ‰æ•°æ®
        
        Returns:
            Optional[pd.DataFrame]: å†…å­˜ç¼“å­˜ä¸­çš„æ•°æ®ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        try:
            if self._full_data_cache is not None and not self._full_data_cache.empty:
                return self._full_data_cache
            return None
        except Exception as e:
            self.logger.warning(f"è·å–å†…å­˜ç¼“å­˜æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _sync_memory_cache_to_redis(self, data: pd.DataFrame) -> bool:
        """
        å°†å†…å­˜ç¼“å­˜æ•°æ®åŒæ­¥åˆ°Redis
        
        Args:
            data: è¦åŒæ­¥çš„æ•°æ®
            
        Returns:
            bool: æ˜¯å¦åŒæ­¥æˆåŠŸ
        """
        try:
            if not self.redis_cache or data.empty:
                print(" Redisä¸å¯ç”¨æˆ–æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡åŒæ­¥")
                return True
            
            # æ›´æ–°Redisç¼“å­˜
            chunk_size = 500
            ttl_seconds = None if self._cache_ttl_hours == -1 else self._cache_ttl_hours * 3600
            
            success = self.redis_cache.set_chunked_data(
                base_key=self._full_cache_key,
                data=data,
                chunk_size=chunk_size,
                ttl=ttl_seconds
            )
            
            if success:
                print(" å†…å­˜ç¼“å­˜å·²åŒæ­¥åˆ°Redis")
                return True
            else:
                print(" åŒæ­¥åˆ°Rediså¤±è´¥")
                return False
                
        except Exception as e:
            self.logger.error(f"åŒæ­¥åˆ°Rediså¤±è´¥: {e}")
            return False

    def _update_cache_with_merged_data(self, merged_data: pd.DataFrame) -> bool:
        """
        ä½¿ç”¨åˆå¹¶åçš„æ•°æ®æ›´æ–°ç¼“å­˜ï¼ˆå…¼å®¹æ—§æ–¹æ³•ï¼‰
        
        Args:
            merged_data: åˆå¹¶åçš„æ•°æ®
            
        Returns:
            bool: æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        try:
            if merged_data.empty:
                print(" åˆå¹¶åæ•°æ®ä¸ºç©ºï¼Œè·³è¿‡ç¼“å­˜æ›´æ–°")
                return True
            
            # æ›´æ–°å†…å­˜ç¼“å­˜
            self._full_data_cache = merged_data
            
            # åŒæ­¥åˆ°Redisç¼“å­˜
            return self._sync_memory_cache_to_redis(merged_data)
                
        except Exception as e:
            self.logger.error(f"æ›´æ–°ç¼“å­˜å¤±è´¥: {e}")
            return False
    
    def _update_cache_metadata(self, merged_data: pd.DataFrame):
        """
        æ›´æ–°ç¼“å­˜å…ƒæ•°æ®
        
        Args:
            merged_data: åˆå¹¶åçš„æ•°æ®
        """
        try:
            if not self.redis_cache or merged_data.empty:
                return
            
            # æ›´æ–°å…ƒæ•°æ®
            metadata = {
                'total_rows': len(merged_data),
                'created_at': datetime.now().isoformat(),
                'last_update_time': datetime.now().isoformat(),
                'total_chunks': (len(merged_data) + 499) // 500,  # å‡è®¾chunk_size=500
                'chunk_size': 500
            }
            
            self.redis_cache.set(f"{self._full_cache_key}:meta", metadata)
            print(" ç¼“å­˜å…ƒæ•°æ®æ›´æ–°æˆåŠŸ")
            
        except Exception as e:
            self.logger.warning(f"æ›´æ–°ç¼“å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
    
    def get_incremental_update_status(self) -> Dict[str, Any]:
        """
        è·å–å¢é‡æ›´æ–°çŠ¶æ€ä¿¡æ¯ - ä¼˜å…ˆä»å†…å­˜ç¼“å­˜è·å–
        
        Returns:
            Dict: å¢é‡æ›´æ–°çŠ¶æ€ä¿¡æ¯
        """
        try:
            status = {
                'last_update_time': None,
                'mysql_latest_time': None,
                'has_new_data': False,
                'new_data_count': 0,
                'memory_cache_size': 0,
                'data_source': 'unknown'
            }
            
            # ä¼˜å…ˆä»å†…å­˜ç¼“å­˜è·å–çŠ¶æ€ä¿¡æ¯
            memory_data = self._get_existing_data_from_memory()
            if memory_data is not None and not memory_data.empty:
                status['memory_cache_size'] = len(memory_data)
                status['data_source'] = 'memory'
                print(f" ä»å†…å­˜ç¼“å­˜è·å–çŠ¶æ€ä¿¡æ¯ï¼Œæ•°æ®é‡: {len(memory_data)} æ¡")
            else:
                status['data_source'] = 'redis'
                print(" å†…å­˜ç¼“å­˜ä¸ºç©ºï¼Œä»Redisè·å–çŠ¶æ€ä¿¡æ¯")
            
            # è·å–ç¼“å­˜ä¸­çš„æœ€åæ›´æ–°æ—¶é—´ï¼ˆç°åœ¨ä¼šä¼˜å…ˆä»å†…å­˜ç¼“å­˜è·å–ï¼‰
            last_update_time = self._get_last_cache_update_time()
            if last_update_time:
                status['last_update_time'] = last_update_time.isoformat()
            
            # è·å–MySQLä¸­çš„æœ€æ–°æ—¶é—´
            mysql_latest_time = self._get_mysql_latest_update_time()
            if mysql_latest_time:
                status['mysql_latest_time'] = mysql_latest_time.isoformat()
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ•°æ®
                if last_update_time and mysql_latest_time > last_update_time:
                    status['has_new_data'] = True
                    # è·å–æ–°æ•°æ®æ•°é‡ï¼ˆè¿™é‡Œåªæ˜¯ä¼°ç®—ï¼Œå®é™…æ•°é‡éœ€è¦æŸ¥è¯¢ï¼‰
                    status['new_data_count'] = self._estimate_new_data_count(last_update_time)
                    print(f" æ£€æµ‹åˆ°æ–°æ•°æ®: {status['new_data_count']} æ¡")
                else:
                    print(" æ²¡æœ‰æ–°æ•°æ®éœ€è¦æ›´æ–°")
            
            return status
            
        except Exception as e:
            self.logger.error(f"è·å–å¢é‡æ›´æ–°çŠ¶æ€å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _get_mysql_latest_update_time(self) -> Optional[datetime]:
        """
        è·å–MySQLä¸­çš„æœ€æ–°æ›´æ–°æ—¶é—´
        
        Returns:
            Optional[datetime]: æœ€æ–°æ›´æ–°æ—¶é—´
        """
        try:
            from src.database import db
            from src.models.equipment import Equipment
            from flask import current_app
            from src.app import create_app
            
            # ç¡®ä¿åœ¨Flaskåº”ç”¨ä¸Šä¸‹æ–‡ä¸­
            if not current_app:
                app = create_app()
                with app.app_context():
                    return self._get_mysql_latest_update_time()
            
            # æŸ¥è¯¢æœ€æ–°çš„update_time
            latest_time = db.session.query(func.max(Equipment.update_time)).scalar()
            return latest_time
            
        except Exception as e:
            self.logger.error(f"è·å–MySQLæœ€æ–°æ—¶é—´å¤±è´¥: {e}")
            return None
    
    def _estimate_new_data_count(self, last_update_time: datetime) -> int:
        """
        ä¼°ç®—æ–°æ•°æ®æ•°é‡
        
        Args:
            last_update_time: ä¸Šæ¬¡æ›´æ–°æ—¶é—´
            
        Returns:
            int: ä¼°ç®—çš„æ–°æ•°æ®æ•°é‡
        """
        try:
            from src.database import db
            from src.models.equipment import Equipment
            from flask import current_app
            from src.app import create_app
            
            # ç¡®ä¿åœ¨Flaskåº”ç”¨ä¸Šä¸‹æ–‡ä¸­
            if not current_app:
                app = create_app()
                with app.app_context():
                    return self._estimate_new_data_count(last_update_time)
            
            # æŸ¥è¯¢æ–°æ•°æ®æ•°é‡
            count = db.session.query(Equipment).filter(
                Equipment.update_time > last_update_time
            ).count()
            
            return count
            
        except Exception as e:
            self.logger.error(f"ä¼°ç®—æ–°æ•°æ®æ•°é‡å¤±è´¥: {e}")
            return 0
    
    
    def auto_incremental_update(self) -> bool:
        """
        è‡ªåŠ¨æ£€æµ‹å¹¶æ‰§è¡Œå¢é‡æ›´æ–°
        
        Returns:
            bool: æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        try:
            print("ğŸ¤– å¼€å§‹è‡ªåŠ¨å¢é‡æ›´æ–°æ£€æµ‹...")
            
            # è·å–å¢é‡æ›´æ–°çŠ¶æ€
            status = self.get_incremental_update_status()
            
            if 'error' in status:
                print(f" è·å–å¢é‡æ›´æ–°çŠ¶æ€å¤±è´¥: {status['error']}")
                return False
            
            if not status.get('has_new_data', False):
                print(" æ²¡æœ‰æ–°æ•°æ®éœ€è¦æ›´æ–°")
                return True
            
            new_data_count = status.get('new_data_count', 0)
            print(f" æ£€æµ‹åˆ° {new_data_count} æ¡æ–°æ•°æ®ï¼Œå¼€å§‹å¢é‡æ›´æ–°...")
            
            # æ‰§è¡Œå¢é‡æ›´æ–°
            return self.incremental_update()
            
        except Exception as e:
            self.logger.error(f"è‡ªåŠ¨å¢é‡æ›´æ–°å¤±è´¥: {e}")
            print(f" è‡ªåŠ¨å¢é‡æ›´æ–°å¼‚å¸¸: {e}")
            return False
    
    def force_incremental_update(self) -> bool:
        """
        å¼ºåˆ¶å¢é‡æ›´æ–°ï¼ˆå¿½ç•¥ç¼“å­˜çŠ¶æ€æ£€æŸ¥ï¼‰
        
        Returns:
            bool: æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        try:
            print("ğŸ’ª å¼€å§‹å¼ºåˆ¶å¢é‡æ›´æ–°...")
            
            # è·å–MySQLä¸­çš„æœ€æ–°æ—¶é—´
            mysql_latest_time = self._get_mysql_latest_update_time()
            if mysql_latest_time is None:
                print(" æ— æ³•è·å–MySQLæœ€æ–°æ—¶é—´")
                return False
            
            # è·å–ç¼“å­˜ä¸­çš„æœ€åæ›´æ–°æ—¶é—´
            last_update_time = self._get_last_cache_update_time()
            if last_update_time is None:
                print(" æ— æ³•è·å–ç¼“å­˜æœ€åæ›´æ–°æ—¶é—´ï¼Œå°†è¿›è¡Œå…¨é‡åˆ·æ–°")
                return self.refresh_full_cache()
            
            # å¦‚æœMySQLæ—¶é—´æ—©äºç¼“å­˜æ—¶é—´ï¼Œè¯´æ˜æ²¡æœ‰æ–°æ•°æ®
            if mysql_latest_time <= last_update_time:
                print(" MySQLæ•°æ®æ²¡æœ‰æ›´æ–°ï¼Œæ— éœ€å¢é‡æ›´æ–°")
                return True
            
            # æ‰§è¡Œå¢é‡æ›´æ–°
            return self.incremental_update(last_update_time)
            
        except Exception as e:
            self.logger.error(f"å¼ºåˆ¶å¢é‡æ›´æ–°å¤±è´¥: {e}")
            print(f" å¼ºåˆ¶å¢é‡æ›´æ–°å¼‚å¸¸: {e}")
            return False

    def get_cache_status(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜çŠ¶æ€ä¿¡æ¯"""
        try:
            # è·å–MySQLè£…å¤‡æ•°æ®æ€»æ•°
            mysql_count = self._get_mysql_equipments_count()
            
            status = {
                'redis_available': False,
                'full_cache_exists': False,
                'full_cache_size': 0,
                'memory_cache_size': 0,
                'cache_key': self._full_cache_key,
                'cache_ttl_hours': self._cache_ttl_hours,
                'cache_never_expires': self._cache_ttl_hours == -1,
                'refresh_mode': 'manual_only' if self._cache_ttl_hours == -1 else 'auto_expire',
                'mysql_data_count': mysql_count
            }
            
            if self.redis_cache and self.redis_cache.is_available():
                status['redis_available'] = True
                
                # æ£€æŸ¥Redisä¸­çš„å…¨é‡ç¼“å­˜
                try:
                    metadata = self.redis_cache.get(f"{self._full_cache_key}:meta")
                    if metadata:
                        status['full_cache_exists'] = True
                        status['full_cache_size'] = metadata.get('total_rows', 0)
                        status['cache_created_at'] = metadata.get('created_at')
                        status['chunk_info'] = {
                            'total_chunks': metadata.get('total_chunks', 0),
                            'chunk_size': metadata.get('chunk_size', 0)
                        }
                except Exception as e:
                    self.logger.debug(f"æ£€æŸ¥Redisç¼“å­˜çŠ¶æ€å¤±è´¥: {e}")
            
            # æ£€æŸ¥å†…å­˜ç¼“å­˜
            if self._full_data_cache is not None:
                status['memory_cache_size'] = len(self._full_data_cache)
            
            return status
            
        except Exception as e:
            self.logger.error(f"è·å–ç¼“å­˜çŠ¶æ€å¤±è´¥: {e}")
            return {'error': str(e)}

    def get_refresh_status(self) -> Dict[str, Any]:
        """
        è·å–åˆ·æ–°è¿›åº¦çŠ¶æ€
        
        Returns:
            Dict: åŒ…å«è¿›åº¦ä¿¡æ¯çš„å­—å…¸
        """
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        print(f"ğŸ” è·å–åˆ·æ–°çŠ¶æ€ - å®ä¾‹ID: {id(self)}")
        print(f"ğŸ” å½“å‰çŠ¶æ€: {self._refresh_status}, è¿›åº¦: {self._refresh_progress}%")
        print(f"ğŸ” å†…å­˜ç¼“å­˜çŠ¶æ€: {self._full_data_cache is not None and not self._full_data_cache.empty if self._full_data_cache is not None else False}")
        
        status_info = {
            "status": self._refresh_status,
            "progress": self._refresh_progress,
            "message": self._refresh_message,
            "processed_records": self._refresh_processed_records,
            "total_records": self._refresh_total_records,
            "current_batch": self._refresh_current_batch,
            "total_batches": self._refresh_total_batches,
            "start_time": self._refresh_start_time.isoformat() if self._refresh_start_time else None,
            "elapsed_seconds": int((datetime.now() - self._refresh_start_time).total_seconds()) if self._refresh_start_time else 0
        }
        
        return status_info

    @classmethod
    def clear_cache(cls):
        """æ¸…ç©ºå•ä¾‹å®ä¾‹çš„ç¼“å­˜"""
        with cls._lock:
            if cls._instance and hasattr(cls._instance, '_full_data_cache'):
                cls._instance._full_data_cache = None
                cls._instance._refresh_status = "idle"
                cls._instance._refresh_progress = 0
                cls._instance._refresh_message = ""
                print("å·²æ¸…ç©ºè£…å¤‡æ•°æ®ç¼“å­˜")

    @classmethod
    def get_cache_status_static(cls) -> Dict[str, Any]:
        """è·å–å•ä¾‹å®ä¾‹çš„ç¼“å­˜çŠ¶æ€"""
        with cls._lock:
            if cls._instance:
                return cls._instance.get_cache_status()
            else:
                return {
                    'redis_available': False,
                    'full_cache_exists': False,
                    'full_cache_size': 0,
                    'memory_cache_size': 0
                }

    @classmethod
    def get_refresh_status_static(cls) -> Dict[str, Any]:
        """è·å–å•ä¾‹å®ä¾‹çš„åˆ·æ–°çŠ¶æ€"""
        with cls._lock:
            if cls._instance:
                return cls._instance.get_refresh_status()
            else:
                return {
                    "status": "idle",
                    "progress": 0,
                    "message": "",
                    "processed_records": 0,
                    "total_records": 0,
                    "current_batch": 0,
                    "total_batches": 0,
                    "start_time": None,
                    "elapsed_seconds": 0
                }

    def _should_filter_suit_effect(self, suit_effect: Union[int, str]) -> bool:
        """
        åˆ¤æ–­å¥—è£…æ•ˆæœæ˜¯å¦åº”è¯¥è¢«ç”¨äºç­›é€‰

        ä¸šåŠ¡è§„åˆ™ï¼šåªæœ‰ç‰¹å®šçš„é«˜ä»·å€¼å¥—è£…æ•ˆæœæ‰ç”¨äºç²¾ç¡®ç­›é€‰ï¼Œ
        å…¶ä»–å¥—è£…æ•ˆæœåœ¨ç›¸ä¼¼åº¦è®¡ç®—æ—¶è¿›è¡Œèšç±»å¤„ç†

        Args:
            suit_effect: å¥—è£…æ•ˆæœIDï¼ˆæ•°å­—æˆ–æ•°å­—å­—ç¬¦ä¸²ï¼Œpet_equipå¯èƒ½æ˜¯çº¯å­—ç¬¦ä¸²ï¼‰

        Returns:
            bool: æ˜¯å¦åº”è¯¥ç­›é€‰æ­¤å¥—è£…æ•ˆæœ
        """
        # å…è®¸ç²¾ç¡®ç­›é€‰çš„å¥—è£…æ•ˆæœï¼šå®šå¿ƒæœ¯ã€å˜èº«æœ¯ã€ç¢æ˜Ÿè¯€ã€å¤©ç¥æŠ¤ä½“ã€æ»¡å¤©èŠ±é›¨ã€æµªæ¶Œ
        allowed_suit_effects = PRECISE_FILTER_SUITS

        # å°è¯•è½¬æ¢ä¸ºæ•°å­—è¿›è¡Œæ¯”è¾ƒ
        try:
            suit_effect_num = int(suit_effect)
            return suit_effect_num in allowed_suit_effects
        except (ValueError, TypeError):
            # è½¬æ¢å¤±è´¥ï¼ˆå¯èƒ½æ˜¯pet_equipçš„å­—ç¬¦ä¸²å¥—è£…ï¼‰ï¼Œä¸è¿›è¡Œç²¾ç¡®ç­›é€‰
            return False

    def get_market_data_with_business_rules(self,
                                            target_features: Dict[str, Any],
                                            **kwargs) -> pd.DataFrame:
        """
        åº”ç”¨ä¸šåŠ¡è§„åˆ™è·å–å¸‚åœºæ•°æ®

        è¿™ä¸ªæ–¹æ³•åœ¨åŸºç¡€æŸ¥è¯¢çš„åŸºç¡€ä¸Šåº”ç”¨ç‰¹å®šçš„ä¸šåŠ¡è§„åˆ™ï¼Œ
        æ¯”å¦‚å¥—è£…æ•ˆæœçš„ç­›é€‰ç­–ç•¥ç­‰

        Args:
            target_features: ç›®æ ‡è£…å¤‡ç‰¹å¾
            **kwargs: å…¶ä»–æŸ¥è¯¢å‚æ•°

        Returns:
            å¸‚åœºæ•°æ®DataFrame
        """
        try:
            # å¤„ç†å¥—è£…æ•ˆæœä¸šåŠ¡è§„åˆ™
            suit_effect = target_features.get('suit_effect', 0)
            filtered_suit_effect = None

            if suit_effect and self._should_filter_suit_effect(suit_effect):
                filtered_suit_effect = suit_effect
                print(f"å¥—è£…æ•ˆæœ {suit_effect} å°†ç”¨äºç²¾ç¡®ç­›é€‰")
            else:
                print(f"å¥—è£…æ•ˆæœ {suit_effect} å°†åœ¨ç›¸ä¼¼åº¦è®¡ç®—æ—¶å¤„ç†")

            # åˆå¹¶å‚æ•°
            query_params = {
                'kindid': target_features.get('kindid'),
                'level_range': target_features.get('equip_level_range'),
                'special_skill': target_features.get('special_skill', 0),
                'suit_effect': filtered_suit_effect,
                **kwargs
            }

            return self.get_market_data(**query_params)

        except Exception as e:
            self.logger.error(f"åº”ç”¨ä¸šåŠ¡è§„åˆ™è·å–å¸‚åœºæ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()

    def _classify_addon_attributes(self, addon_minjie: int = 0, addon_liliang: int = 0,
                                   addon_naili: int = 0, addon_tizhi: int = 0,
                                   addon_moli: int = 0) -> str:
        """
        æ ¹æ®è£…å¤‡çš„é™„åŠ å±æ€§åˆ†ç±»
        #å±æ€§ç‚¹åŠ æˆåˆ†ç±»ï¼Œå±æ€§ç‚¹åŠ æˆç±»å‹ä¸€èˆ¬æˆå¯¹å‡ºç°ï¼›
        # å¦‚æœä¸¤ä¸ªéƒ½æ˜¯æ­£æ•°åˆ™æ˜¯ç»„åˆåŒåŠ ï¼Œå¦‚ä½“è´¨+10å’Œè€åŠ›+10éƒ½æ˜¯æ­£æ•°åˆ™æ˜¯ä½“è€ï¼›
        # å¦‚æœå•ç§å±æ€§æ­£æ•°ï¼Œå¦‚ä½“è´¨+10ï¼Œåˆ™æ˜¯ä½“è´¨ï¼›
        # å¦‚æœä¸€ä¸ªæ­£æ•°ä¸€ä¸ªè´Ÿæ•°ï¼Œå¦‚ä½“è´¨+15ï¼Œæ•æ·-2ï¼Œåˆ™æ˜¯ä½“è´¨ï¼›
        # åˆ†å››ä¸ªèšç±»
        # 1."ä½“è´¨", "ä½“è€",
        # 2."é­”åŠ›", "é­”ä½“","é­”è€","é­”æ•",
        # 3."è€åŠ›",
        # 4."æ•æ·","æ•ä½“","æ•è€",
        # 5."åŠ›é‡", "åŠ›ä½“","åŠ›é­”","åŠ›è€","åŠ›æ•"

        Args:
            addon_minjie: æ•æ·åŠ æˆ
            addon_liliang: åŠ›é‡åŠ æˆ
            addon_naili: è€åŠ›åŠ æˆ
            addon_tizhi: ä½“è´¨åŠ æˆ
            addon_moli: é­”åŠ›åŠ æˆ

        Returns:
            str: å±æ€§åˆ†ç±»ç±»å‹
        """
        # ç»Ÿè®¡æ­£æ•°å±æ€§
        positive_attrs = {}
        if addon_minjie > 0:
            positive_attrs['æ•æ·'] = addon_minjie
        if addon_liliang > 0:
            positive_attrs['åŠ›é‡'] = addon_liliang
        if addon_naili > 0:
            positive_attrs['è€åŠ›'] = addon_naili
        if addon_tizhi > 0:
            positive_attrs['ä½“è´¨'] = addon_tizhi
        if addon_moli > 0:
            positive_attrs['é­”åŠ›'] = addon_moli

        # å¦‚æœæ²¡æœ‰æ­£æ•°å±æ€§
        if not positive_attrs:
            return "æ— å±æ€§"

        # å¦‚æœåªæœ‰ä¸€ä¸ªæ­£æ•°å±æ€§
        if len(positive_attrs) == 1:
            attr_name = list(positive_attrs.keys())[0]
            return attr_name

        # å¦‚æœæœ‰ä¸¤ä¸ªæ­£æ•°å±æ€§ï¼ŒæŒ‰ç»„åˆè§„åˆ™åˆ†ç±»
        if len(positive_attrs) == 2:
            attr_names = sorted(positive_attrs.keys())

            # ä½“è´¨ç›¸å…³ç»„åˆ
            if 'ä½“è´¨' in attr_names and 'è€åŠ›' in attr_names:
                return "ä½“è€"
            elif 'ä½“è´¨' in attr_names and 'æ•æ·' in attr_names:
                return "æ•ä½“"
            elif 'ä½“è´¨' in attr_names and 'åŠ›é‡' in attr_names:
                return "åŠ›ä½“"
            elif 'ä½“è´¨' in attr_names and 'é­”åŠ›' in attr_names:
                return "é­”ä½“"

            # é­”åŠ›ç›¸å…³ç»„åˆ
            elif 'é­”åŠ›' in attr_names and 'è€åŠ›' in attr_names:
                return "é­”è€"
            elif 'é­”åŠ›' in attr_names and 'æ•æ·' in attr_names:
                return "é­”æ•"
            elif 'é­”åŠ›' in attr_names and 'åŠ›é‡' in attr_names:
                return "åŠ›é­”"

            # æ•æ·ç›¸å…³ç»„åˆ
            elif 'æ•æ·' in attr_names and 'è€åŠ›' in attr_names:
                return "æ•è€"
            elif 'æ•æ·' in attr_names and 'åŠ›é‡' in attr_names:
                return "åŠ›æ•"

            # åŠ›é‡è€åŠ›ç»„åˆ
            elif 'åŠ›é‡' in attr_names and 'è€åŠ›' in attr_names:
                return "åŠ›è€"

        # å¤šäºä¸¤ä¸ªå±æ€§æˆ–å…¶ä»–æƒ…å†µï¼Œè¿”å›ä¸»å±æ€§ï¼ˆæ•°å€¼æœ€å¤§çš„ï¼‰
        if positive_attrs:
            main_attr = max(positive_attrs.items(), key=lambda x: x[1])[0]
            return main_attr

        return "æ— å±æ€§"

    def _copy_temp_cache_to_official(self, temp_key: str, official_key: str, df: pd.DataFrame, chunk_size: int, ttl_seconds: Optional[int]) -> bool:
        """
        å°†ä¸´æ—¶ç¼“å­˜å¤åˆ¶åˆ°æ­£å¼ç¼“å­˜ï¼ˆæ— ç¼åˆ‡æ¢ï¼‰
        
        Args:
            temp_key: ä¸´æ—¶ç¼“å­˜é”®å
            official_key: æ­£å¼ç¼“å­˜é”®å
            df: æ•°æ®DataFrame
            chunk_size: å—å¤§å°
            ttl_seconds: TTLç§’æ•°
            
        Returns:
            bool: æ˜¯å¦å¤åˆ¶æˆåŠŸ
        """
        try:
            print(f"å¼€å§‹å¤åˆ¶ä¸´æ—¶ç¼“å­˜ {temp_key} åˆ°æ­£å¼ç¼“å­˜ {official_key}...")
            
            # ç›´æ¥ä½¿ç”¨set_chunked_dataé‡æ–°å­˜å‚¨åˆ°æ­£å¼é”®
            success = self.redis_cache.set_chunked_data(
                base_key=official_key,
                data=df,
                chunk_size=chunk_size,
                ttl=ttl_seconds
            )
            
            if success:
                print(" ä¸´æ—¶ç¼“å­˜å¤åˆ¶åˆ°æ­£å¼ç¼“å­˜æˆåŠŸ")
                return True
            else:
                print(" ä¸´æ—¶ç¼“å­˜å¤åˆ¶åˆ°æ­£å¼ç¼“å­˜å¤±è´¥")
                return False
                
        except Exception as e:
            print(f" å¤åˆ¶ä¸´æ—¶ç¼“å­˜å¤±è´¥: {e}")
            return False

    def _rename_temp_cache_to_official(self, temp_key: str, official_key: str) -> bool:
        """
        å°†ä¸´æ—¶ç¼“å­˜é‡å‘½åä¸ºæ­£å¼ç¼“å­˜ï¼ˆæ— ç¼åˆ‡æ¢ï¼‰
        
        Args:
            temp_key: ä¸´æ—¶ç¼“å­˜é”®å
            official_key: æ­£å¼ç¼“å­˜é”®å
            
        Returns:
            bool: æ˜¯å¦é‡å‘½åæˆåŠŸ
        """
        try:
            # è·å–ä¸´æ—¶ç¼“å­˜çš„æ‰€æœ‰é”®
            temp_keys = self.redis_cache.client.keys(f"{temp_key}:*")
            if not temp_keys:
                print(f" ä¸´æ—¶ç¼“å­˜é”®ä¸å­˜åœ¨: {temp_key}")
                return False
            
            print(f"æ‰¾åˆ° {len(temp_keys)} ä¸ªä¸´æ—¶ç¼“å­˜é”®ï¼Œå¼€å§‹é‡å‘½å...")
            
            # ä½¿ç”¨Redisç®¡é“æ‰¹é‡é‡å‘½å
            pipe = self.redis_cache.client.pipeline()
            
            for temp_key_full in temp_keys:
                # å°†ä¸´æ—¶é”®åæ›¿æ¢ä¸ºæ­£å¼é”®å
                official_key_full = temp_key_full.decode('utf-8').replace(temp_key, official_key)
                
                # è·å–ä¸´æ—¶é”®çš„å€¼
                value = self.redis_cache.client.get(temp_key_full)
                if value is not None:
                    # è®¾ç½®åˆ°æ­£å¼é”®
                    pipe.set(official_key_full, value)
                    # åˆ é™¤ä¸´æ—¶é”®
                    pipe.delete(temp_key_full)
                    print(f"é‡å‘½å: {temp_key_full.decode('utf-8')} -> {official_key_full}")
            
            # æ‰§è¡Œç®¡é“æ“ä½œ
            results = pipe.execute()
            
            # æ£€æŸ¥ç»“æœ
            success_count = sum(1 for result in results if result is True)
            total_operations = len(results) // 2  # æ¯ä¸ªé”®æœ‰setå’Œdeleteä¸¤ä¸ªæ“ä½œ
            
            print(f"é‡å‘½åå®Œæˆ: {success_count}/{total_operations} ä¸ªé”®æ“ä½œæˆåŠŸ")
            
            if success_count == total_operations:
                print(" æ‰€æœ‰ä¸´æ—¶ç¼“å­˜é”®é‡å‘½åæˆåŠŸ")
                return True
            else:
                print(f" éƒ¨åˆ†é‡å‘½åå¤±è´¥: {success_count}/{total_operations}")
                return False
                
        except Exception as e:
            print(f" é‡å‘½åä¸´æ—¶ç¼“å­˜å¤±è´¥: {e}")
            return False

    def _get_target_addon_classification(self, target_features: Dict[str, Any]) -> str:
        """
        è·å–ç›®æ ‡è£…å¤‡çš„å±æ€§åˆ†ç±»

        Args:
            target_features: ç›®æ ‡è£…å¤‡ç‰¹å¾

        Returns:
            str: å±æ€§åˆ†ç±»ç±»å‹
        """
        addon_minjie = target_features.get('addon_minjie', 0)
        addon_liliang = target_features.get('addon_liliang', 0)
        addon_naili = target_features.get('addon_naili', 0)
        addon_tizhi = target_features.get('addon_tizhi', 0)
        addon_moli = target_features.get('addon_moli', 0)

        return self._classify_addon_attributes(
            addon_minjie, addon_liliang, addon_naili, addon_tizhi, addon_moli
        )

    def get_market_data_with_addon_classification(self,
                                                  target_features: Dict[str, Any],
                                                  **kwargs) -> pd.DataFrame:
        """
        æ ¹æ®å±æ€§åŠ æˆåˆ†ç±»è·å–å¸‚åœºæ•°æ®

        è‡ªåŠ¨æŒ‰ç…§è£…å¤‡å±æ€§åˆ†ç±»è¿‡æ»¤å¸‚åœºæ•°æ®ï¼Œåªä¿ç•™åŒç±»å±æ€§è£…å¤‡

        Args:
            target_features: ç›®æ ‡è£…å¤‡ç‰¹å¾
            **kwargs: å…¶ä»–æŸ¥è¯¢å‚æ•°

        Returns:
            å¸‚åœºæ•°æ®DataFrameï¼ŒåŒ…å«å±æ€§åˆ†ç±»ä¿¡æ¯ï¼Œå·²æŒ‰åŒç±»å±æ€§è¿‡æ»¤
        """
        try:
            # è·å–åŸºç¡€å¸‚åœºæ•°æ®
            market_data = self.get_market_data_for_similarity(target_features)
            if market_data.empty:
                return market_data

            # è·å–ç›®æ ‡è£…å¤‡çš„å±æ€§åˆ†ç±»
            target_classification = self._get_target_addon_classification(
                target_features)
            print(f"ç›®æ ‡è£…å¤‡å±æ€§åˆ†ç±»: {target_classification}")

            # ä¸ºå¸‚åœºæ•°æ®æ·»åŠ å±æ€§åˆ†ç±»
            market_data['addon_classification'] = market_data.apply(
                lambda row: self._classify_addon_attributes(
                    row.get('addon_minjie', 0),
                    row.get('addon_liliang', 0),
                    row.get('addon_naili', 0),
                    row.get('addon_tizhi', 0),
                    row.get('addon_moli', 0)
                ), axis=1
            )

            # å§‹ç»ˆæŒ‰å±æ€§åˆ†ç±»è¿‡æ»¤ï¼ˆé™¤éç›®æ ‡è£…å¤‡æ˜¯æ— å±æ€§ï¼‰
            if target_classification != "æ— å±æ€§":
                # å®šä¹‰åŒç±»å±æ€§åˆ†ç»„
                classification_groups = {
                    "ä½“è´¨ç³»": ["ä½“è´¨", "ä½“è€"],
                    "é­”åŠ›ç³»": ["é­”åŠ›", "é­”ä½“", "é­”è€", "é­”æ•"],
                    "è€åŠ›ç³»": ["è€åŠ›"],
                    "æ•æ·ç³»": ["æ•æ·", "æ•ä½“", "æ•è€"],
                    "åŠ›é‡ç³»": ["åŠ›é‡", "åŠ›ä½“", "åŠ›é­”", "åŠ›è€", "åŠ›æ•"]
                }

                # æ‰¾åˆ°ç›®æ ‡è£…å¤‡æ‰€å±çš„åˆ†ç»„
                target_group = None
                for group_name, classifications in classification_groups.items():
                    if target_classification in classifications:
                        target_group = classifications
                        break

                if target_group:
                    # è¿‡æ»¤åŒç±»å±æ€§è£…å¤‡ï¼ˆä¿ç•™æ— å±æ€§è£…å¤‡ä½œä¸ºå‚è€ƒï¼‰
                    before_filter_count = len(market_data)
                    market_data = market_data[
                        (market_data['addon_classification'].isin(target_group)) |
                        (market_data['addon_classification'] == "æ— å±æ€§")
                    ]
                    after_filter_count = len(market_data)

                    print(
                        f"å±æ€§åˆ†ç±»è¿‡æ»¤: {target_classification} -> åŒç±»å±æ€§ {target_group}")
                    print(
                        f"è¿‡æ»¤ç»“æœ: {before_filter_count} -> {after_filter_count} æ¡æ•°æ®")
                else:
                    print(f"æœªæ‰¾åˆ°å±æ€§åˆ†ç±» {target_classification} å¯¹åº”çš„åˆ†ç»„ï¼Œä¸è¿›è¡Œè¿‡æ»¤")
            else:
                print(f"ç›®æ ‡è£…å¤‡æ— å±æ€§ï¼Œä¸è¿›è¡Œå±æ€§åˆ†ç±»è¿‡æ»¤")

            return market_data

        except Exception as e:
            self.logger.error(f"æŒ‰å±æ€§åˆ†ç±»è·å–å¸‚åœºæ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
