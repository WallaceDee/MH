import sys
import os
import threading
import concurrent.futures

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œè§£å†³æ¨¡å—å¯¼å…¥é—®é¢˜
from src.utils.project_path import get_project_root
project_root = get_project_root()
sys.path.insert(0, project_root)

import pandas as pd
import numpy as np
import json
import logging
import time
import hashlib
from typing import Dict, Any, List, Optional, Union
from datetime import datetime, timedelta
from flask import current_app

try:
    from .feature_extractor.feature_extractor import FeatureExtractor
except ImportError:
    from src.evaluator.feature_extractor.feature_extractor import FeatureExtractor


class MarketDataCollector:
    """å¸‚åœºæ•°æ®é‡‡é›†å™¨ - ä»MySQLæ•°æ®åº“è·å–ç©ºè§’è‰²æ•°æ®ä½œä¸ºé”šç‚¹ï¼Œæ”¯æŒå•ä¾‹æ¨¡å¼çš„æ•°æ®ç¼“å­˜å…±äº«"""
    
    _instance = None  # å•ä¾‹å®ä¾‹
    _lock = threading.Lock()  # çº¿ç¨‹é”ï¼Œç¡®ä¿çº¿ç¨‹å®‰å…¨
    
    def __new__(cls):
        """å•ä¾‹æ¨¡å¼å®ç°"""
        with cls._lock:
            if cls._instance is None:
                instance = super(MarketDataCollector, cls).__new__(cls)
                cls._instance = instance
                # æ ‡è®°å®ä¾‹æ˜¯å¦å·²åˆå§‹åŒ–ï¼Œé¿å…é‡å¤åˆå§‹åŒ–
                instance._initialized = False
                print("åˆ›å»ºæ–°çš„ MarketDataCollector å•ä¾‹å®ä¾‹")
            else:
                print("ä½¿ç”¨ç°æœ‰çš„ MarketDataCollector å•ä¾‹å®ä¾‹")
            
            return cls._instance
    
    def __init__(self):
        """åˆå§‹åŒ–å¸‚åœºæ•°æ®é‡‡é›†å™¨ï¼ˆå•ä¾‹æ¨¡å¼ä¸‹åªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰"""
        # é¿å…é‡å¤åˆå§‹åŒ–
        if hasattr(self, '_initialized') and self._initialized:
            return
        
        self.logger = logging.getLogger(__name__)
        self.feature_extractor = FeatureExtractor()
        self.market_data = pd.DataFrame()
        
        # æ•°æ®ç¼“å­˜ç›¸å…³å±æ€§
        self._data_loaded = False
        self._last_refresh_time = None
        self._cache_expiry_hours = -1  # ç¼“å­˜æ°¸ä¸è¿‡æœŸï¼ˆ-1è¡¨ç¤ºæ°¸ä¸è¿‡æœŸï¼‰
        
        # Flask-Caching ç¼“å­˜å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self._cache = None
        
        # è¿›åº¦è·Ÿè¸ªç›¸å…³å±æ€§
        self._refresh_status = "idle"  # idle, running, completed, error
        self._refresh_progress = 0  # 0-100
        self._refresh_message = ""
        self._refresh_start_time = None
        self._refresh_total_records = 0
        self._refresh_processed_records = 0
        self._refresh_current_batch = 0
        self._refresh_total_batches = 0
        
        self._initialized = True
        print("å¸‚åœºæ•°æ®é‡‡é›†å™¨å•ä¾‹åˆå§‹åŒ–å®Œæˆï¼Œé»˜è®¤è·å–ç©ºè§’è‰²æ•°æ®ä½œä¸ºé”šç‚¹")
        print("ğŸ’¾ ç¼“å­˜ç­–ç•¥: æ•°æ®æ°¸ä¸è¿‡æœŸï¼Œåªèƒ½é€šè¿‡force_refresh=Trueæˆ–æ‰‹åŠ¨åˆ·æ–°æ›´æ–°")
    
    def _get_cache(self):
        """è·å–Flask-Cachingå®ä¾‹"""
        if self._cache is None:
            try:
                # è·å–å½“å‰åº”ç”¨çš„ç¼“å­˜å®ä¾‹
                from flask_caching import Cache
                
                # æ£€æŸ¥æ˜¯å¦åœ¨åº”ç”¨ä¸Šä¸‹æ–‡ä¸­
                if current_app:
                    # ä»åº”ç”¨æ‰©å±•ä¸­è·å–ç¼“å­˜å®ä¾‹
                    extensions = getattr(current_app, 'extensions', {})
                    
                    # æŸ¥æ‰¾Cacheå®ä¾‹
                    cache_instance = None
                    for ext_name, ext_instance in extensions.items():
                        if isinstance(ext_instance, Cache):
                            cache_instance = ext_instance
                            self.logger.info("æˆåŠŸè·å–Flask-Cachingå®ä¾‹")
                            break
                    
                    if cache_instance is None:
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä»åº”ç”¨ä¸­ç›´æ¥è·å–
                        if hasattr(current_app, 'cache'):
                            cache_instance = current_app.cache
                            self.logger.info("ä»åº”ç”¨å±æ€§è·å–Flask-Cachingå®ä¾‹")
                        else:
                            # åˆ›å»ºæ–°çš„å®ä¾‹
                            cache_instance = Cache()
                            cache_instance.init_app(current_app)
                            self.logger.info("åˆ›å»ºæ–°çš„Flask-Cachingå®ä¾‹")
                    
                    self._cache = cache_instance
                        
                else:
                    self.logger.warning("æœªåœ¨Flaskåº”ç”¨ä¸Šä¸‹æ–‡ä¸­ï¼Œæ— æ³•ä½¿ç”¨ç¼“å­˜")
                    
            except Exception as e:
                self.logger.warning(f"è·å–Flask-Cachingå®ä¾‹å¤±è´¥: {e}")
                self._cache = None
                
        return self._cache
    
    def _generate_cache_key(self, filters: Optional[Dict[str, Any]] = None, max_records: int = 9999) -> str:
        """
        ç”Ÿæˆç¼“å­˜é”® - åªåŸºäºç­›é€‰æ¡ä»¶ï¼Œä¸åŒ…å«max_records
        è¿™æ ·å¯ä»¥è®©ä¸åŒçš„max_recordså€¼å¤ç”¨åŒä¸€ä¸ªç¼“å­˜
        """
        # åˆ›å»ºå”¯ä¸€çš„ç¼“å­˜é”®ï¼ŒåªåŸºäºç­›é€‰æ¡ä»¶
        cache_data = {
            'filters': filters or {},
            'version': '1.0'  # ç‰ˆæœ¬å·ï¼Œç”¨äºç¼“å­˜å¤±æ•ˆ
        }
        
        # ç”Ÿæˆå“ˆå¸Œ
        cache_str = json.dumps(cache_data, sort_keys=True, ensure_ascii=False)
        cache_hash = hashlib.md5(cache_str.encode('utf-8')).hexdigest()[:16]
        
        return f"market_data:{cache_hash}"
    
    def _get_cached_data(self, filters: Optional[Dict[str, Any]] = None, max_records: int = 9999) -> Optional[pd.DataFrame]:
        """ä»Flask-Cachingè·å–å¸‚åœºæ•°æ®ï¼Œæ”¯æŒæ™ºèƒ½æˆªå–"""
        cache = self._get_cache()
        if not cache:
            return None
        
        try:
            cache_key = self._generate_cache_key(filters, max_records)
            cached_data = cache.get(cache_key)
            
            if cached_data is not None:
                df = None
                
                # ä»JSONæ ¼å¼è¿˜åŸDataFrame
                if isinstance(cached_data, dict) and 'data' in cached_data:
                    df = pd.DataFrame(cached_data['data'])
                    # å¦‚æœæœ‰ç´¢å¼•ä¿¡æ¯ï¼Œè®¾ç½®ç´¢å¼•
                    if 'index' in cached_data and cached_data['index'] and len(df) > 0:
                        if cached_data['index'] in df.columns:
                            df.set_index(cached_data['index'], inplace=True)
                elif isinstance(cached_data, pd.DataFrame):
                    # ç›´æ¥æ˜¯DataFrameï¼ˆå‘åå…¼å®¹ï¼‰
                    df = cached_data
                else:
                    # å…¶ä»–æ ¼å¼ï¼Œå°è¯•è½¬æ¢
                    self.logger.warning(f"ä»Flask-Cachingè·å–åˆ°æœªçŸ¥æ ¼å¼æ•°æ®: {type(cached_data)}")
                    return None
                
                if df is not None:
                    # æ™ºèƒ½æˆªå–ï¼šå¦‚æœç¼“å­˜æ•°æ®é‡å¤§äºè¯·æ±‚é‡ï¼Œæˆªå–å‰Næ¡
                    original_len = len(df)
                    if original_len > max_records:
                        df = df.head(max_records)
                        self.logger.info(f"ä»Flask-Cachingè·å–å¸‚åœºæ•°æ®å¹¶æˆªå–ï¼ŒåŸå§‹: {original_len} æ¡ï¼Œæˆªå–: {len(df)} æ¡")
                    else:
                        self.logger.info(f"ä»Flask-Cachingè·å–å¸‚åœºæ•°æ®ï¼Œæ•°æ®é‡: {len(df)} æ¡ï¼ˆæ»¡è¶³è¯·æ±‚é‡ {max_records}ï¼‰")
                    
                    return df
                    
        except Exception as e:
            self.logger.warning(f"ä»Flask-Cachingè·å–æ•°æ®å¤±è´¥: {e}")
        
        return None
    
    def _set_cached_data(self, filters: Optional[Dict[str, Any]], max_records: int, data: pd.DataFrame) -> bool:
        """
        è®¾ç½®Flask-Cachingç¼“å­˜æ•°æ®
        æ™ºèƒ½ç¼“å­˜ç­–ç•¥ï¼šä¼˜å…ˆç¼“å­˜æ›´å¤§çš„æ•°æ®é›†ï¼Œä»¥ä¾¿åç»­ä¸åŒmax_recordsçš„è¯·æ±‚éƒ½èƒ½å¤ç”¨
        """
        cache = self._get_cache()
        if not cache:
            self.logger.warning("ç¼“å­˜å®ä¾‹ä¸å¯ç”¨")
            return False
            
        if data.empty:
            self.logger.warning("æ•°æ®ä¸ºç©ºï¼Œä¸è¿›è¡Œç¼“å­˜")
            return False
        
        try:
            cache_key = self._generate_cache_key(filters, max_records)
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜
            existing_cached = cache.get(cache_key)
            if existing_cached and isinstance(existing_cached, dict):
                existing_count = existing_cached.get('record_count', 0)
                current_count = len(data)
                
                # å¦‚æœå½“å‰æ•°æ®é‡å°äºæˆ–ç­‰äºå·²ç¼“å­˜çš„æ•°æ®é‡ï¼Œä¸æ›´æ–°ç¼“å­˜
                if current_count <= existing_count:
                    self.logger.info(f"è·³è¿‡ç¼“å­˜æ›´æ–°ï¼Œå½“å‰æ•°æ®é‡ {current_count} <= å·²ç¼“å­˜æ•°æ®é‡ {existing_count}")
                    return True
                else:
                    self.logger.info(f"æ›´æ–°ç¼“å­˜ï¼Œå½“å‰æ•°æ®é‡ {current_count} > å·²ç¼“å­˜æ•°æ®é‡ {existing_count}")
            
            # å°†DataFrameè½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
            cache_data = {
                'data': data.reset_index().to_dict(orient='records'),
                'index': data.index.name if hasattr(data.index, 'name') and data.index.name else 'eid',
                'cached_at': datetime.now().isoformat(),
                'record_count': len(data),
                'max_records_used': max_records  # è®°å½•ç”¨äºç”Ÿæˆæ­¤ç¼“å­˜çš„max_recordsï¼Œç”¨äºè°ƒè¯•
            }
            
            # è®¾ç½®ç¼“å­˜ï¼Œä½¿ç”¨6å°æ—¶TTL
            result = cache.set(cache_key, cache_data, timeout=6*3600)
            
            # Flask-Caching çš„ set æ–¹æ³•è¿”å› True/False æˆ– None
            if result is True:
                self.logger.info(f"å¸‚åœºæ•°æ®å·²ç¼“å­˜åˆ°Flask-Cachingï¼Œæ•°æ®é‡: {len(data)}ï¼Œæ”¯æŒmax_records <= {len(data)}")
                return True
            elif result is False:
                self.logger.warning("Flask-Caching.set() è¿”å›False")
                return False
            elif result is None:
                # æœ‰äº›ç¼“å­˜åç«¯è¿”å›Noneè¡¨ç¤ºæˆåŠŸ
                self.logger.info(f"å¸‚åœºæ•°æ®å·²ç¼“å­˜åˆ°Flask-Cachingï¼ˆè¿”å›Noneï¼‰ï¼Œæ•°æ®é‡: {len(data)}ï¼Œæ”¯æŒmax_records <= {len(data)}")
                return True
            else:
                self.logger.warning(f"Flask-Caching.set() è¿”å›æœªé¢„æœŸå€¼: {result}")
                return False
            
        except Exception as e:
            self.logger.warning(f"è®¾ç½®Flask-Cachingå¤±è´¥: {str(e)}")
            return False
    
    
    def refresh_market_data(self, 
                       filters: Optional[Dict[str, Any]] = None,
                       max_records: int = 99999,
                       use_cache: bool = True,
                       force_refresh: bool = False,
                       batch_size: int = 100) -> pd.DataFrame:
        """
        åˆ·æ–°å¸‚åœºæ•°æ® - ä¼˜å…ˆä»Rediså…¨é‡ç¼“å­˜è·å–ï¼Œæ”¯æŒç­›é€‰ã€åˆ†é¡µå’Œè¯¦ç»†è¿›åº¦è·Ÿè¸ª
        
        Args:
            filters: ç­›é€‰æ¡ä»¶å­—å…¸ï¼Œä¾‹å¦‚ {'level_min': 109, 'price_max': 10000}
            max_records: æœ€å¤§è®°å½•æ•°
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°å…¨é‡ç¼“å­˜
            batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆä»…åœ¨ä»æ•°æ®åº“åŠ è½½æ—¶ä½¿ç”¨ï¼Œå»ºè®®100-800ä¹‹é—´ï¼‰
            
        Returns:
            pd.DataFrame: å¤„ç†åçš„å¸‚åœºæ•°æ®
        """
        try:
            start_time = time.time()
            
            # åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ª
            self._refresh_status = "running"
            self._refresh_progress = 0
            self._refresh_message = "å¼€å§‹åˆ·æ–°æ•°æ®..."
            self._refresh_start_time = datetime.now()
            self._refresh_processed_records = 0
            self._refresh_current_batch = 0
            self._refresh_total_batches = 0
            self._refresh_total_records = 0
            
            # å¦‚æœä½¿ç”¨ç¼“å­˜ä¸”ä¸å¼ºåˆ¶åˆ·æ–°ï¼Œå°è¯•ä»Rediså…¨é‡ç¼“å­˜è·å–
            if use_cache and not force_refresh:
                self._refresh_message = "æ£€æŸ¥Rediså…¨é‡ç¼“å­˜..."
                self._refresh_progress = 5
                
                cached_data = self._get_full_cached_data()
                if cached_data is not None and not cached_data.empty:
                    self._refresh_message = "ä»ç¼“å­˜åº”ç”¨ç­›é€‰æ¡ä»¶..."
                    self._refresh_progress = 50
                    
                    # åº”ç”¨ç­›é€‰æ¡ä»¶
                    filtered_data = self._apply_filters(cached_data, filters, max_records)
                    
                    self.market_data = filtered_data
                    self._data_loaded = True
                    self._last_refresh_time = datetime.now()
                    
                    # æ›´æ–°è¿›åº¦çŠ¶æ€
                    self._refresh_status = "completed"
                    self._refresh_progress = 100
                    self._refresh_message = "ä»ç¼“å­˜è·å–å®Œæˆï¼"
                    self._refresh_processed_records = len(filtered_data)
                    self._refresh_total_records = len(filtered_data)
                    
                    elapsed_time = time.time() - start_time
                    print(f"ä»Rediså…¨é‡ç¼“å­˜è·å–å¸‚åœºæ•°æ®æˆåŠŸï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
                    print(f"å…¨é‡ç¼“å­˜æ•°æ®: {len(cached_data)} æ¡ï¼Œç­›é€‰å: {len(filtered_data)} æ¡")
                    print(f"ç‰¹å¾ç»´åº¦: {len(filtered_data.columns)}")
                    if not filtered_data.empty:
                        print(f"ä»·æ ¼èŒƒå›´: {filtered_data['price'].min():.1f} - {filtered_data['price'].max():.1f}")
                    
                    return self.market_data
                else:
                    print("Rediså…¨é‡ç¼“å­˜æœªå‘½ä¸­æˆ–å·²è¿‡æœŸï¼Œå°†ä»MySQLé‡æ–°åŠ è½½å…¨é‡æ•°æ®")
                    self._refresh_message = "ç¼“å­˜æœªå‘½ä¸­ï¼Œå‡†å¤‡ä»æ•°æ®åº“åŠ è½½..."
                    self._refresh_progress = 10
            
            self._refresh_message = "ä»æ•°æ®åº“åŠ è½½å…¨é‡æ•°æ®..."
            self._refresh_progress = 15
            
            # å¯¼å…¥MySQLè¿æ¥ç›¸å…³æ¨¡å—
            from sqlalchemy import create_engine, text
            from src.app import create_app
            
            # åˆ›å»ºFlaskåº”ç”¨ä¸Šä¸‹æ–‡è·å–æ•°æ®åº“é…ç½®
            app = create_app()
            
            with app.app_context():
                # è·å–æ•°æ®åº“é…ç½®
                db_config = app.config.get('SQLALCHEMY_DATABASE_URI')
                if not db_config:
                    raise ValueError("æœªæ‰¾åˆ°æ•°æ®åº“é…ç½®")
                
                self._refresh_message = "è¿æ¥æ•°æ®åº“..."
                self._refresh_progress = 20
                print(f"è¿æ¥MySQLæ•°æ®åº“: {db_config}")
                
                # åˆ›å»ºä¼˜åŒ–çš„æ•°æ®åº“è¿æ¥ - ä½¿ç”¨è¿æ¥æ± 
                connection_config = self._get_optimized_connection_config(db_config)
                engine = create_engine(db_config, **connection_config)
                
                # è¾“å‡ºæ•°æ®åº“ç´¢å¼•ä¼˜åŒ–å»ºè®®
                self._optimize_database_indexes()
                
                self._refresh_message = "åˆ†ææ•°æ®é‡..."
                self._refresh_progress = 25
                
                # é¦–å…ˆè·å–æ€»è®°å½•æ•°
                count_query = """
                    SELECT COUNT(*) as total_count
                    FROM roles c
                    WHERE c.role_type = 'empty' AND c.price > 0
                """
                
                with engine.connect() as conn:
                    count_result = conn.execute(text(count_query))
                    total_count = count_result.fetchone()[0]
                
                print(f"æ€»è®°å½•æ•°: {total_count}")
                
                # åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°ï¼ˆä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½ï¼Œå‡å°‘å•æ¬¡æŸ¥è¯¢æ—¶é—´ï¼‰
                # è€ƒè™‘å¤æ‚SQLæŸ¥è¯¢ï¼ˆLEFT JOIN + å¤§é‡å­—æ®µï¼‰ï¼Œä½¿ç”¨æ›´å°çš„æ‰¹æ¬¡å¤§å°
                if total_count > 50000:
                    actual_batch_size = min(batch_size, 300)   # å¤§æ•°æ®é›†ï¼šå°æ‰¹æ¬¡é¿å…æŸ¥è¯¢è¶…æ—¶å’Œå†…å­˜æº¢å‡º
                elif total_count > 20000:
                    actual_batch_size = min(batch_size, 500)   # ä¸­ç­‰æ•°æ®é›†ï¼šé€‚ä¸­æ‰¹æ¬¡
                elif total_count > 5000:
                    actual_batch_size = min(batch_size, 800)   # ä¸­å°æ•°æ®é›†ï¼šè¾ƒå¤§æ‰¹æ¬¡
                else:
                    actual_batch_size = min(batch_size, 300)   # å°æ•°æ®é›†ï¼šå°æ‰¹æ¬¡ä¿è¯å¿«é€Ÿå“åº”
                
                total_batches = (total_count + actual_batch_size - 1) // actual_batch_size
                
                # æ›´æ–°è¿›åº¦è·Ÿè¸ªä¿¡æ¯
                self._refresh_total_records = total_count
                self._refresh_total_batches = total_batches
                self._refresh_message = f"å‡†å¤‡åˆ†æ‰¹å¤„ç†: {total_batches} æ‰¹ï¼Œæ¯æ‰¹ {actual_batch_size} æ¡"
                self._refresh_progress = 30
                
                print(f"å°†åˆ† {total_batches} æ‰¹å¤„ç†ï¼Œæ¯æ‰¹ {actual_batch_size} æ¡")
                
                # æ€§èƒ½ä¼˜åŒ–æç¤º
                if actual_batch_size > 1000:
                    print(f"âš ï¸  æ‰¹æ¬¡å¤§å°è¾ƒå¤§({actual_batch_size})ï¼Œå¦‚æœæŸ¥è¯¢ç¼“æ…¢ï¼Œå»ºè®®è®¾ç½®æ›´å°çš„batch_sizeå‚æ•°(100-800)")
                elif actual_batch_size < batch_size:
                    print(f"ğŸ’¡ å·²è‡ªåŠ¨è°ƒæ•´æ‰¹æ¬¡å¤§å°: {batch_size} -> {actual_batch_size} (ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½)")
                
                # ä¼˜åŒ–çš„SQLæŸ¥è¯¢ - åªé€‰æ‹©å¿…è¦å­—æ®µï¼Œå‡å°‘æ•°æ®ä¼ è¾“
                base_query = """
                    SELECT 
                        c.eid, c.serverid, c.level, c.school, c.price, c.collect_num,
                        c.yushoushu_skill, c.school_skills, c.life_skills,
                        l.sum_exp, l.three_fly_lv, l.all_new_point, l.jiyuan_amount, 
                        l.packet_page, l.xianyu_amount, l.sum_amount,
                        l.expt_ski1, l.expt_ski2, l.expt_ski3, l.expt_ski4, l.expt_ski5,
                        l.beast_ski1, l.beast_ski2, l.beast_ski3, l.beast_ski4,
                        l.changesch_json, l.ex_avt_json, l.huge_horse_json, l.shenqi_json,
                        l.all_equip_json, l.all_summon_json, l.all_rider_json
                    FROM roles c
                    LEFT JOIN large_equip_desc_data l ON c.eid = l.eid
                    WHERE c.role_type = 'empty' AND c.price > 0
                    ORDER BY c.price ASC
                    LIMIT {batch_size} OFFSET {offset}
                """
                
                # åˆ†æ‰¹å¤„ç†æ•°æ®
                full_market_data = []
                processed_count = 0
                
                with engine.connect() as conn:
                    for batch_num in range(total_batches):
                        offset = batch_num * actual_batch_size
                        current_batch_query = base_query.format(batch_size=actual_batch_size, offset=offset)
                        
                        # æ›´æ–°å½“å‰æ‰¹æ¬¡è¿›åº¦
                        self._refresh_current_batch = batch_num + 1
                        batch_progress = 30 + int(((batch_num + 1) / total_batches) * 60)  # 30-90%çš„è¿›åº¦èŒƒå›´
                        self._refresh_progress = min(batch_progress, 90)
                        self._refresh_message = f"å¤„ç†ç¬¬ {batch_num + 1}/{total_batches} æ‰¹æ•°æ®..."
                        
                        print(f"å¤„ç†ç¬¬ {batch_num + 1}/{total_batches} æ‰¹ï¼Œåç§»é‡: {offset}")
                        
                        try:
                            # æ‰§è¡Œæ‰¹æ¬¡æŸ¥è¯¢
                            batch_result = conn.execute(text(current_batch_query))
                            batch_columns = batch_result.keys()
                            batch_rows = batch_result.fetchall()
                            
                            if not batch_rows:
                                print(f"ç¬¬ {batch_num + 1} æ‰¹æ— æ•°æ®ï¼Œè·³è¿‡")
                                continue
                            
                            # ä½¿ç”¨å¹¶è¡Œå¤„ç†å½“å‰æ‰¹æ¬¡çš„æ•°æ®
                            batch_data = self._process_batch_data_parallel(batch_rows, batch_columns, batch_num + 1)
                            full_market_data.extend(batch_data)
                            
                            processed_count += len(batch_data)
                            self._refresh_processed_records = processed_count
                            
                            progress_percentage = (processed_count / total_count) * 100
                            print(f"å·²å¤„ç† {processed_count}/{total_count} æ¡æ•°æ® ({progress_percentage:.1f}%)")
                            
                            # æ¯å¤„ç†å‡ æ‰¹å°±å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼Œé‡Šæ”¾å†…å­˜
                            if batch_num % 5 == 0:
                                import gc
                                gc.collect()
                                
                        except Exception as e:
                            self.logger.error(f"å¤„ç†ç¬¬ {batch_num + 1} æ‰¹æ•°æ®å¤±è´¥: {e}")
                            continue
                
                # è½¬æ¢ä¸ºDataFrame
                self._refresh_message = "æ„å»ºæ•°æ®ç»“æ„..."
                self._refresh_progress = 92
                
                full_data_df = pd.DataFrame(full_market_data)
                
                if not full_data_df.empty:
                    full_data_df.set_index('eid', inplace=True)
                    
                    elapsed_time = time.time() - start_time
                    print(f"å…¨é‡å¸‚åœºæ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(full_data_df)} æ¡æœ‰æ•ˆæ•°æ®ï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
                    print(f"æ•°æ®ç‰¹å¾ç»´åº¦: {len(full_data_df.columns)}")
                    print(f"ä»·æ ¼èŒƒå›´: {full_data_df['price'].min():.1f} - {full_data_df['price'].max():.1f}")
                    
                    # ç¼“å­˜å…¨é‡æ•°æ®åˆ°Redis
                    if use_cache:
                        self._refresh_message = "ç¼“å­˜æ•°æ®åˆ°Redis..."
                        self._refresh_progress = 95
                        
                        cache_start = time.time()
                        if self._set_full_cached_data(full_data_df):
                            cache_time = time.time() - cache_start
                            print(f"å…¨é‡æ•°æ®å·²ç¼“å­˜åˆ°Redisï¼Œç¼“å­˜è€—æ—¶: {cache_time:.2f}ç§’")
                        else:
                            print("Rediså…¨é‡ç¼“å­˜è®¾ç½®å¤±è´¥ï¼Œä½†æ•°æ®è·å–æˆåŠŸ")
                    
                    # åº”ç”¨ç­›é€‰æ¡ä»¶å¹¶è¿”å›ç»“æœ
                    self._refresh_message = "åº”ç”¨ç­›é€‰æ¡ä»¶..."
                    self._refresh_progress = 98
                    
                    filtered_data = self._apply_filters(full_data_df, filters, max_records)
                    self.market_data = filtered_data
                    
                    # æ›´æ–°ç¼“å­˜çŠ¶æ€
                    self._data_loaded = True
                    self._last_refresh_time = datetime.now()
                    
                    # å®Œæˆè¿›åº¦è·Ÿè¸ª
                    self._refresh_status = "completed"
                    self._refresh_progress = 100
                    self._refresh_message = "æ•°æ®åˆ·æ–°å®Œæˆï¼"
                    self._refresh_processed_records = len(filtered_data)
                    
                    print(f"ç­›é€‰åæ•°æ®: {len(filtered_data)} æ¡")
                    
                else:
                    print("è­¦å‘Šï¼šæœªè·å–åˆ°æœ‰æ•ˆçš„å¸‚åœºæ•°æ®")
                    self.market_data = pd.DataFrame()
                    
                    # å®Œæˆè¿›åº¦è·Ÿè¸ªï¼ˆæ— æ•°æ®æƒ…å†µï¼‰
                    self._refresh_status = "completed"
                    self._refresh_progress = 100
                    self._refresh_message = "æœªè·å–åˆ°æœ‰æ•ˆæ•°æ®"
                    self._refresh_processed_records = 0
                
                return self.market_data
                
        except Exception as e:
            # é”™è¯¯å¤„ç†è¿›åº¦è·Ÿè¸ª
            self._refresh_status = "error"
            self._refresh_progress = 0
            self._refresh_message = f"åˆ·æ–°å¤±è´¥: {str(e)}"
            
            self.logger.error(f"åˆ·æ–°å¸‚åœºæ•°æ®å¤±è´¥: {e}")
            raise
    
    def get_refresh_status(self) -> Dict[str, Any]:
        """
        è·å–åˆ·æ–°è¿›åº¦çŠ¶æ€
        
        Returns:
            Dict: åŒ…å«è¿›åº¦ä¿¡æ¯çš„å­—å…¸
        """
        status_info = {
            "status": self._refresh_status,
            "progress": self._refresh_progress,
            "message": self._refresh_message,
            "processed_records": self._refresh_processed_records,
            "total_records": self._refresh_total_records,
            "current_batch": self._refresh_current_batch,
            "total_batches": self._refresh_total_batches,
            "start_time": self._refresh_start_time.isoformat() if self._refresh_start_time else None,
            "elapsed_seconds": int((datetime.now() - self._refresh_start_time).total_seconds()) if self._refresh_start_time else 0
        }
        
        return status_info
    
    def get_market_data(self, force_refresh: bool = False) -> pd.DataFrame:
        """
        è·å–å½“å‰çš„å¸‚åœºæ•°æ®ï¼Œæ”¯æŒæ™ºèƒ½ç¼“å­˜å’Œè¿‡æœŸæ£€æŸ¥
        
        Args:
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°æ•°æ®
            
        Returns:
            pd.DataFrame: å¸‚åœºæ•°æ®
        """
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°æ•°æ®
        need_refresh = (
            force_refresh or 
            not self._data_loaded or 
            self.market_data.empty or 
            self._is_cache_expired()
        )
        
        if need_refresh:
            if force_refresh:
                print("å¼ºåˆ¶åˆ·æ–°å¸‚åœºæ•°æ®...")
            elif not self._data_loaded:
                print("é¦–æ¬¡åŠ è½½å¸‚åœºæ•°æ®...")
            elif self.market_data.empty:
                print("å¸‚åœºæ•°æ®ä¸ºç©ºï¼Œæ­£åœ¨åˆ·æ–°...")
            elif self._is_cache_expired():
                if self._cache_expiry_hours == -1:
                    print("ç¼“å­˜æ°¸ä¸è¿‡æœŸæ¨¡å¼ï¼Œä½†å› å…¶ä»–åŸå› éœ€è¦åˆ·æ–°...")
                else:
                    print(f"ç¼“å­˜å·²è¿‡æœŸï¼ˆè¶…è¿‡{self._cache_expiry_hours}å°æ—¶ï¼‰ï¼Œæ­£åœ¨åˆ·æ–°...")
            
            self.refresh_market_data()
            self._data_loaded = True
            self._last_refresh_time = datetime.now()
        else:
            if self._cache_expiry_hours == -1:
                print(f"ä½¿ç”¨æ°¸ä¹…ç¼“å­˜çš„å¸‚åœºæ•°æ®ï¼Œä¸Šæ¬¡åˆ·æ–°æ—¶é—´: {self._last_refresh_time}, "
                      f"æ•°æ®é‡: {len(self.market_data)} ï¼ˆæ°¸ä¸è¿‡æœŸæ¨¡å¼ï¼‰")
            else:
                print(f"ä½¿ç”¨ç¼“å­˜çš„å¸‚åœºæ•°æ®ï¼Œä¸Šæ¬¡åˆ·æ–°æ—¶é—´: {self._last_refresh_time}, "
                      f"æ•°æ®é‡: {len(self.market_data)}")
        
        return self.market_data
    
    def _is_cache_expired(self) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ - æ°¸ä¸è¿‡æœŸæ¨¡å¼"""
        if not self._last_refresh_time:
            return True  # é¦–æ¬¡åŠ è½½æ—¶éœ€è¦åˆ·æ–°
        
        # å¦‚æœè®¾ç½®ä¸ºæ°¸ä¸è¿‡æœŸï¼ˆ-1ï¼‰ï¼Œåˆ™ç¼“å­˜æ°¸è¿œä¸ä¼šè¿‡æœŸ
        if self._cache_expiry_hours == -1:
            return False
        
        # æ­£å¸¸çš„è¿‡æœŸæ£€æŸ¥ï¼ˆå‘åå…¼å®¹ï¼‰
        elapsed_time = datetime.now() - self._last_refresh_time
        return elapsed_time > timedelta(hours=self._cache_expiry_hours)
    
    
    def filter_market_data(self, 
                          level_range: Optional[tuple] = None,
                          price_range: Optional[tuple] = None,
                          school: Optional[str] = None,
                          server: Optional[str] = None) -> pd.DataFrame:
        """
        ç­›é€‰å¸‚åœºæ•°æ®
        
        Args:
            level_range: ç­‰çº§èŒƒå›´ (min, max)
            price_range: ä»·æ ¼èŒƒå›´ (min, max)  
            school: é—¨æ´¾
            server: æœåŠ¡å™¨
            
        Returns:
            pd.DataFrame: ç­›é€‰åçš„æ•°æ®
        """
        df = self.get_market_data().copy()
        
        if level_range:
            df = df[(df['level'] >= level_range[0]) & (df['level'] <= level_range[1])]
        
        if price_range:
            df = df[(df['price'] >= price_range[0]) & (df['price'] <= price_range[1])]
        
        if school:
            df = df[df['school_desc'] == school]
        
        if server:
            df = df[df['server_name'] == server]
        
        return df
    
    def get_market_data_for_similarity(self, 
                                      filters: Optional[Dict[str, Any]] = None) -> pd.DataFrame:
        """
        è·å–ç”¨äºç›¸ä¼¼åº¦è®¡ç®—çš„å¸‚åœºæ•°æ®
        
        Args:
            filters: é¢„è¿‡æ»¤æ¡ä»¶ï¼Œç”¨äºå‡å°‘è®¡ç®—é‡
            
        Returns:
            pd.DataFrame: å¸‚åœºæ•°æ®
        """
        self.logger.info(f"[GET_SIMILARITY_DATA] å¼€å§‹è·å–ç›¸ä¼¼åº¦è®¡ç®—æ•°æ®ï¼Œfilters: {filters}")
        
        market_data = self.get_market_data()
        
        self.logger.info(f"[GET_SIMILARITY_DATA] åŸå§‹å¸‚åœºæ•°æ®å¤§å°: {len(market_data)}")
        
        if market_data.empty:
            self.logger.warning("[GET_SIMILARITY_DATA] å¸‚åœºæ•°æ®ä¸ºç©º")
            return market_data
        
        # åº”ç”¨é¢„è¿‡æ»¤æ¡ä»¶ä»¥æé«˜æ•ˆç‡
        if filters:
            self.logger.info(f"[GET_SIMILARITY_DATA] åº”ç”¨é¢„è¿‡æ»¤æ¡ä»¶: {filters}")
            filtered_data = market_data.copy()
            
            # ç­‰çº§è¿‡æ»¤
            if 'level_range' in filters and filters['level_range'] is not None:
                level_range = filters['level_range']
                self.logger.debug(f"[GET_SIMILARITY_DATA] ç­‰çº§è¿‡æ»¤ï¼šlevel_range={level_range}, ç±»å‹={type(level_range)}")
                if isinstance(level_range, (tuple, list)) and len(level_range) == 2:
                    self.logger.debug(f"[GET_SIMILARITY_DATA] å¼€å§‹è§£åŒ…ç­‰çº§èŒƒå›´...")
                    level_min, level_max = level_range
                    self.logger.debug(f"[GET_SIMILARITY_DATA] ç­‰çº§èŒƒå›´è§£åŒ…æˆåŠŸ: {level_min} - {level_max}")
                    filtered_data = filtered_data[
                        (filtered_data['level'] >= level_min) & 
                        (filtered_data['level'] <= level_max)
                    ]
            
            # æ€»ä¿®ç‚¼ç­‰çº§è¿‡æ»¤
            if 'total_cultivation_range' in filters and filters['total_cultivation_range'] is not None:
                cultivation_range = filters['total_cultivation_range']
                self.logger.debug(f"[GET_SIMILARITY_DATA] ä¿®ç‚¼è¿‡æ»¤ï¼šcultivation_range={cultivation_range}, ç±»å‹={type(cultivation_range)}")
                if isinstance(cultivation_range, (tuple, list)) and len(cultivation_range) == 2:
                    self.logger.debug(f"[GET_SIMILARITY_DATA] å¼€å§‹è§£åŒ…ä¿®ç‚¼èŒƒå›´...")
                    cult_min, cult_max = cultivation_range
                    self.logger.debug(f"[GET_SIMILARITY_DATA] ä¿®ç‚¼èŒƒå›´è§£åŒ…æˆåŠŸ: {cult_min} - {cult_max}")
                    filtered_data = filtered_data[
                        (filtered_data['total_cultivation'] >= cult_min) & 
                        (filtered_data['total_cultivation'] <= cult_max)
                    ]
            
            # å¬å”¤å…½æ§åˆ¶åŠ›æ€»å’Œè¿‡æ»¤
            if 'total_beast_cultivation_range' in filters and filters['total_beast_cultivation_range'] is not None:
                beast_range = filters['total_beast_cultivation_range']
                if isinstance(beast_range, (tuple, list)) and len(beast_range) == 2:
                    beast_min, beast_max = beast_range
                    filtered_data = filtered_data[
                        (filtered_data['total_beast_cultivation'] >= beast_min) & 
                        (filtered_data['total_beast_cultivation'] <= beast_max)
                    ]
            
            # å¸ˆé—¨æŠ€èƒ½å¹³å‡å€¼è¿‡æ»¤
            if 'avg_school_skills_range' in filters and filters['avg_school_skills_range'] is not None:
                skills_range = filters['avg_school_skills_range']
                if isinstance(skills_range, (tuple, list)) and len(skills_range) == 2:
                    skills_min, skills_max = skills_range
                    filtered_data = filtered_data[
                        (filtered_data['avg_school_skills'] >= skills_min) & 
                        (filtered_data['avg_school_skills'] <= skills_max)
                    ]
            
            # ä»·æ ¼è¿‡æ»¤
            if 'price_range' in filters and filters['price_range'] is not None:
                price_range = filters['price_range']
                if isinstance(price_range, (tuple, list)) and len(price_range) == 2:
                    price_min, price_max = price_range
                    filtered_data = filtered_data[
                        (filtered_data['price'] >= price_min) & 
                        (filtered_data['price'] <= price_max)
                    ]
            
            # å…¶ä»–èŒƒå›´è¿‡æ»¤æ¡ä»¶ï¼ˆé€šç”¨å¤„ç†ï¼‰
            range_keys = ['total_cultivation_range', 'total_beast_cultivation_range', 
                         'avg_school_skills_range', 'level_range', 'price_range']
            for key, value in filters.items():
                if key not in range_keys and key in filtered_data.columns:
                    if isinstance(value, (list, tuple)) and len(value) == 2:
                        # èŒƒå›´è¿‡æ»¤
                        filtered_data = filtered_data[
                            (filtered_data[key] >= value[0]) & 
                            (filtered_data[key] <= value[1])
                        ]
                    else:
                        # ç²¾ç¡®åŒ¹é…
                        filtered_data = filtered_data[filtered_data[key] == value]
            
            return filtered_data
        
        return market_data
    
    @classmethod
    def clear_cache(cls):
        """æ¸…ç©ºå•ä¾‹å®ä¾‹çš„ç¼“å­˜"""
        with cls._lock:
            if cls._instance and hasattr(cls._instance, 'market_data'):
                cls._instance.market_data = pd.DataFrame()
                cls._instance._data_loaded = False
                cls._instance._last_refresh_time = None
                print("å·²æ¸…ç©ºå¸‚åœºæ•°æ®ç¼“å­˜")
    
    @classmethod
    def get_cache_status(cls) -> Dict[str, Any]:
        """è·å–å•ä¾‹å®ä¾‹çš„ç¼“å­˜çŠ¶æ€"""
        with cls._lock:
            if cls._instance:
                return {
                    'data_loaded': getattr(cls._instance, '_data_loaded', False),
                    'last_refresh_time': getattr(cls._instance, '_last_refresh_time', None),
                    'cache_expired': cls._instance._is_cache_expired() if hasattr(cls._instance, '_is_cache_expired') else True,
                    'data_size': len(cls._instance.market_data) if hasattr(cls._instance, 'market_data') and not cls._instance.market_data.empty else 0,
                    'data_source': 'MySQL (empty roles)'
                }
            else:
                return {
                    'data_loaded': False,
                    'last_refresh_time': None,
                    'cache_expired': True,
                    'data_size': 0,
                    'data_source': 'MySQL (empty roles)'
                }
    
    def clear_instance_cache(self):
        """æ¸…ç©ºå½“å‰å®ä¾‹çš„ç¼“å­˜"""
        self.market_data = pd.DataFrame()
        self._data_loaded = False
        self._last_refresh_time = None
        print("å·²æ¸…ç©ºå¸‚åœºæ•°æ®ç¼“å­˜")
    
    def set_cache_expiry(self, hours: float):
        """
        è®¾ç½®ç¼“å­˜è¿‡æœŸæ—¶é—´
        
        Args:
            hours: è¿‡æœŸæ—¶é—´ï¼ˆå°æ—¶ï¼‰ï¼Œè®¾ç½®ä¸º-1è¡¨ç¤ºæ°¸ä¸è¿‡æœŸ
        """
        self._cache_expiry_hours = hours
        if hours == -1:
            print("ç¼“å­˜å·²è®¾ç½®ä¸ºæ°¸ä¸è¿‡æœŸæ¨¡å¼ï¼Œåªèƒ½æ‰‹åŠ¨åˆ·æ–°")
        else:
            print(f"ç¼“å­˜è¿‡æœŸæ—¶é—´å·²è®¾ç½®ä¸º {hours} å°æ—¶")
    
    def get_cache_info(self) -> Dict[str, Any]:
        """è·å–å½“å‰å®ä¾‹çš„ç¼“å­˜ä¿¡æ¯ï¼ˆåŒ…æ‹¬Flask-Cachingä¿¡æ¯ï¼‰"""
        info = {
            'data_loaded': self._data_loaded,
            'last_refresh_time': self._last_refresh_time,
            'cache_expired': self._is_cache_expired(),
            'data_size': len(self.market_data) if not self.market_data.empty else 0,
            'cache_expiry_hours': self._cache_expiry_hours,
            'cache_never_expires': self._cache_expiry_hours == -1,
            'refresh_mode': 'manual_only' if self._cache_expiry_hours == -1 else 'auto_expire',
            'data_source': 'MySQL (empty roles)',
            'cache_available': False,
            'cache_type': None,
            'cache_config': {}
        }
        
        # æ·»åŠ Flask-Cachingä¿¡æ¯
        cache = self._get_cache()
        if cache:
            info['cache_available'] = True
            try:
                # è·å–ç¼“å­˜é…ç½®ä¿¡æ¯
                if hasattr(cache, 'config') and cache.config:
                    cache_config = cache.config
                elif hasattr(current_app, 'config'):
                    cache_config = current_app.config
                else:
                    cache_config = {}
                
                info['cache_type'] = cache_config.get('CACHE_TYPE', 'Unknown')
                info['cache_config'] = {
                    'cache_type': cache_config.get('CACHE_TYPE'),
                    'default_timeout': cache_config.get('CACHE_DEFAULT_TIMEOUT'),
                    'key_prefix': cache_config.get('CACHE_KEY_PREFIX'),
                }
                
                # å¦‚æœæ˜¯Redisç¼“å­˜ï¼Œæ·»åŠ Redisç‰¹å®šä¿¡æ¯
                if info['cache_type'] == 'RedisCache':
                    info['cache_config'].update({
                        'redis_host': cache_config.get('CACHE_REDIS_HOST'),
                        'redis_port': cache_config.get('CACHE_REDIS_PORT'),
                        'redis_db': cache_config.get('CACHE_REDIS_DB')
                    })
                        
            except Exception as e:
                info['cache_error'] = str(e)
        
        return info

    def _get_full_cached_data(self) -> Optional[pd.DataFrame]:
        """ä»Redisè·å–å…¨é‡ç¼“å­˜æ•°æ® - æ”¯æŒåˆ†å—å­˜å‚¨"""
        try:
            from src.utils.redis_cache import get_redis_cache
            redis_cache = get_redis_cache()
            
            if not redis_cache or not redis_cache.is_available():
                print("Redisä¸å¯ç”¨")
                return None
            
            # ä½¿ç”¨å›ºå®šçš„å…¨é‡ç¼“å­˜é”®
            full_cache_key = "market_data_full_empty_roles"
            
            print("å°è¯•ä»Redisåˆ†å—ç¼“å­˜è·å–å…¨é‡æ•°æ®...")
            
            # å°è¯•è·å–åˆ†å—æ•°æ®
            cached_data = redis_cache.get_chunked_data(full_cache_key)
            
            if cached_data is not None and not cached_data.empty:
                print(f"ä»Redisåˆ†å—ç¼“å­˜è·å–æ•°æ®æˆåŠŸ: {len(cached_data)} æ¡")
                return cached_data
            else:
                print("Redisåˆ†å—ç¼“å­˜æœªå‘½ä¸­")
                return None
                
        except Exception as e:
            self.logger.warning(f"è·å–Redisåˆ†å—ç¼“å­˜å¤±è´¥: {str(e)}")
            print("è·å–åˆ†å—ç¼“å­˜å¤±è´¥")
            return None


    def _set_full_cached_data(self, data: pd.DataFrame) -> bool:
        """å°†å…¨é‡æ•°æ®ç¼“å­˜åˆ°Redis - ä½¿ç”¨åˆ†å—å­˜å‚¨å’ŒPipelineä¼˜åŒ–"""
        try:
            from src.utils.redis_cache import get_redis_cache
            redis_cache = get_redis_cache()
            
            if not redis_cache or not redis_cache.is_available():
                print("Redisä¸å¯ç”¨")
                return False
            
            # ä½¿ç”¨å›ºå®šçš„å…¨é‡ç¼“å­˜é”®
            full_cache_key = "market_data_full_empty_roles"
            
            # è®¾ç½®è¾ƒé•¿çš„ç¼“å­˜æ—¶é—´ï¼ˆ24å°æ—¶ï¼‰
            cache_hours = 24
            ttl_seconds = cache_hours * 3600
            
            print(f"å¼€å§‹ä½¿ç”¨åˆ†å—å­˜å‚¨å…¨é‡æ•°æ®: {len(data)} æ¡è®°å½•")
            
            # æ ¹æ®æ•°æ®å¤§å°åŠ¨æ€è°ƒæ•´å—å¤§å°
            if len(data) > 10000:
                chunk_size = 2000  # å¤§æ•°æ®é›†ä½¿ç”¨è¾ƒå¤§çš„å—
            elif len(data) > 5000:
                chunk_size = 1000  # ä¸­ç­‰æ•°æ®é›†
            else:
                chunk_size = 500   # å°æ•°æ®é›†
            
            # ä½¿ç”¨åˆ†å—å­˜å‚¨
            success = redis_cache.set_chunked_data(
                base_key=full_cache_key,
                data=data,
                chunk_size=chunk_size,
                ttl=ttl_seconds
            )
            
            if success:
                print(f"å…¨é‡æ•°æ®å·²åˆ†å—ç¼“å­˜åˆ°Redisï¼Œç¼“å­˜æ—¶é—´: {cache_hours}å°æ—¶ï¼Œå—å¤§å°: {chunk_size}")
                return True
            else:
                print("Redisåˆ†å—ç¼“å­˜è®¾ç½®å¤±è´¥")
                return False
                
        except Exception as e:
            self.logger.warning(f"è®¾ç½®Rediså…¨é‡ç¼“å­˜å¤±è´¥: {str(e)}")
            print("Redisç¼“å­˜å¤±è´¥")
            return False


    def _apply_filters(self, data: pd.DataFrame, filters: Optional[Dict[str, Any]], max_records: int) -> pd.DataFrame:
        """å¯¹æ•°æ®åº”ç”¨ç­›é€‰æ¡ä»¶"""
        if data.empty:
            return data
        
        try:
            filtered_data = data.copy()
            
            # åº”ç”¨ç­›é€‰æ¡ä»¶
            if filters:
                if 'level_min' in filters and 'level' in filtered_data.columns:
                    filtered_data = filtered_data[filtered_data['level'] >= filters['level_min']]
                
                if 'level_max' in filters and 'level' in filtered_data.columns:
                    filtered_data = filtered_data[filtered_data['level'] <= filters['level_max']]
                
                if 'price_min' in filters and 'price' in filtered_data.columns:
                    filtered_data = filtered_data[filtered_data['price'] >= filters['price_min']]
                
                if 'price_max' in filters and 'price' in filtered_data.columns:
                    filtered_data = filtered_data[filtered_data['price'] <= filters['price_max']]
                
                if 'school' in filters and 'school' in filtered_data.columns:
                    filtered_data = filtered_data[filtered_data['school'] == filters['school']]
                
                if 'serverid' in filters and 'serverid' in filtered_data.columns:
                    filtered_data = filtered_data[filtered_data['serverid'] == filters['serverid']]
            
            # åº”ç”¨è®°å½•æ•°é™åˆ¶
            if len(filtered_data) > max_records:
                # æŒ‰ä»·æ ¼æ’åºåå–å‰Næ¡
                if 'price' in filtered_data.columns:
                    filtered_data = filtered_data.sort_values('price').head(max_records)
                else:
                    filtered_data = filtered_data.head(max_records)
            
            return filtered_data
            
        except Exception as e:
            self.logger.warning(f"åº”ç”¨ç­›é€‰æ¡ä»¶å¤±è´¥: {str(e)}")
            return data.head(max_records) if len(data) > max_records else data

    def refresh_full_cache(self) -> bool:
        """
        æ‰‹åŠ¨åˆ·æ–°å…¨é‡ç¼“å­˜ - ä»MySQLé‡æ–°åŠ è½½æ‰€æœ‰emptyè§’è‰²æ•°æ®åˆ°Redis
        è¿™æ˜¯åœ¨æ°¸ä¸è¿‡æœŸæ¨¡å¼ä¸‹æ›´æ–°æ•°æ®çš„ä¸»è¦æ–¹æ³•
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸåˆ·æ–°ç¼“å­˜
        """
        try:
            print("ğŸ”„ å¼€å§‹æ‰‹åŠ¨åˆ·æ–°å…¨é‡ç¼“å­˜ï¼ˆæ°¸ä¸è¿‡æœŸæ¨¡å¼ï¼‰...")
            print("ğŸ“Š è¿™å°†ä»MySQLé‡æ–°åŠ è½½æ‰€æœ‰emptyè§’è‰²æ•°æ®")
            
            # å¼ºåˆ¶ä»æ•°æ®åº“åˆ·æ–°ï¼Œä¸ä½¿ç”¨ç°æœ‰ç¼“å­˜
            self.refresh_market_data(
                filters=None, 
                max_records=999999,  # è®¾ç½®å¾ˆå¤§çš„å€¼ä»¥è·å–å…¨éƒ¨æ•°æ®
                use_cache=True,
                force_refresh=True
            )
            
            print("âœ… å…¨é‡ç¼“å­˜æ‰‹åŠ¨åˆ·æ–°å®Œæˆ")
            print("ğŸ’¾ æ•°æ®å·²æ›´æ–°ä¸ºæœ€æ–°ç‰ˆæœ¬ï¼Œå°†æ°¸ä¹…ä¿æŒç›´åˆ°ä¸‹æ¬¡æ‰‹åŠ¨åˆ·æ–°")
            return True
            
        except Exception as e:
            self.logger.error(f"åˆ·æ–°å…¨é‡ç¼“å­˜å¤±è´¥: {e}")
            print(f"âŒ æ‰‹åŠ¨åˆ·æ–°å¤±è´¥: {e}")
            return False

    def manual_refresh(self, max_records: int = 999999, filters: Optional[Dict[str, Any]] = None) -> bool:
        """
        æ‰‹åŠ¨åˆ·æ–°å¸‚åœºæ•°æ® - ä¸“ä¸ºæ°¸ä¸è¿‡æœŸæ¨¡å¼è®¾è®¡çš„åˆ·æ–°æ–¹æ³•
        
        Args:
            max_records: æœ€å¤§è®°å½•æ•°
            filters: ç­›é€‰æ¡ä»¶
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸåˆ·æ–°
        """
        try:
            print("ğŸ”„ æ‰‹åŠ¨åˆ·æ–°å¸‚åœºæ•°æ®...")
            print("ğŸ“Š å¼ºåˆ¶ä»æ•°æ®åº“é‡æ–°åŠ è½½æ•°æ®")
            
            self.refresh_market_data(
                filters=filters,
                max_records=max_records,
                use_cache=True,
                force_refresh=True
            )
            
            print("âœ… æ‰‹åŠ¨åˆ·æ–°å®Œæˆ")
            print("ğŸ’¾ æ•°æ®å·²æ›´æ–°ï¼Œåœ¨æ°¸ä¸è¿‡æœŸæ¨¡å¼ä¸‹å°†ä¿æŒæœ€æ–°çŠ¶æ€")
            return True
            
        except Exception as e:
            self.logger.error(f"æ‰‹åŠ¨åˆ·æ–°å¤±è´¥: {e}")
            print(f"âŒ æ‰‹åŠ¨åˆ·æ–°å¤±è´¥: {e}")
            return False

    def get_cache_status(self) -> Dict[str, Any]:
        """
        è·å–ç¼“å­˜çŠ¶æ€ä¿¡æ¯ - æ”¯æŒåˆ†å—ç¼“å­˜
        
        Returns:
            Dict: ç¼“å­˜çŠ¶æ€ä¿¡æ¯
        """
        try:
            from src.utils.redis_cache import get_redis_cache
            redis_cache = get_redis_cache()
            
            status = {
                'redis_available': False,
                'full_cache_exists': False,
                'full_cache_size': 0,
                'full_cache_last_update': None,
                'cache_type': 'unknown',
                'chunk_info': {}
            }
            
            # æ£€æŸ¥Redisè¿æ¥
            if redis_cache and redis_cache.is_available():
                status['redis_available'] = True
                
                # æ£€æŸ¥åˆ†å—ç¼“å­˜
                full_cache_key = "market_data_full_empty_roles"
                
                # å…ˆæ£€æŸ¥åˆ†å—ç¼“å­˜çš„å…ƒæ•°æ®
                try:
                    metadata = redis_cache.get(f"{full_cache_key}:meta")
                    if metadata:
                        status['full_cache_exists'] = True
                        status['cache_type'] = 'chunked'
                        status['full_cache_size'] = metadata.get('total_rows', 0)
                        status['full_cache_last_update'] = metadata.get('created_at')
                        status['chunk_info'] = {
                            'total_chunks': metadata.get('total_chunks', 0),
                            'chunk_size': metadata.get('chunk_size', 0),
                            'columns_count': len(metadata.get('columns', []))
                        }
                        
                        # éªŒè¯æ‰€æœ‰åˆ†å—æ˜¯å¦å®Œæ•´
                        total_chunks = metadata.get('total_chunks', 0)
                        if total_chunks > 0:
                            chunk_keys = [f"{full_cache_key}:chunk_{i}" for i in range(total_chunks)]
                            existing_chunks = redis_cache.get_batch(chunk_keys)
                            status['chunk_info']['existing_chunks'] = len(existing_chunks)
                            status['chunk_info']['is_complete'] = len(existing_chunks) == total_chunks
                        
                        return status
                except Exception as e:
                    self.logger.debug(f"æ£€æŸ¥åˆ†å—ç¼“å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
                
            
            return status
            
        except Exception as e:
            self.logger.error(f"è·å–ç¼“å­˜çŠ¶æ€å¤±è´¥: {e}")
            return {
                'redis_available': False,
                'full_cache_exists': False,
                'full_cache_size': 0,
                'full_cache_last_update': None,
                'cache_type': 'error',
                'error': str(e)
            }

    def _process_batch_data(self, batch_rows: List, batch_columns: List, batch_num: int) -> List[Dict]:
        """
        æ‰¹é‡å¤„ç†æ•°æ® - ä¼˜åŒ–ç‰¹å¾æå–æ€§èƒ½
        
        Args:
            batch_rows: æ•°æ®åº“æŸ¥è¯¢ç»“æœè¡Œ
            batch_columns: åˆ—ååˆ—è¡¨
            batch_num: æ‰¹æ¬¡å·
            
        Returns:
            List[Dict]: å¤„ç†åçš„ç‰¹å¾æ•°æ®åˆ—è¡¨
        """
        try:
            batch_data = []
            
            # é¢„å…ˆè½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨ï¼Œå‡å°‘é‡å¤æ“ä½œ
            role_data_list = [dict(zip(batch_columns, row)) for row in batch_rows]
            
            # æ‰¹é‡æå–ç‰¹å¾
            for i, role_data in enumerate(role_data_list):
                try:
                    # æå–ç‰¹å¾
                    features = self.feature_extractor.extract_features(role_data)
                    
                    # æ·»åŠ åŸºæœ¬ä¿¡æ¯
                    features.update({
                        'eid': role_data.get('eid', ''),
                        'price': role_data.get('price', 0),
                        'school': role_data.get('school', 0),
                        'serverid': role_data.get('serverid', 0)
                    })
                    
                    batch_data.append(features)
                    
                except Exception as e:
                    self.logger.warning(f"å¤„ç†ç¬¬{batch_num}æ‰¹ç¬¬{i+1}æ¡æ•°æ®æ—¶å‡ºé”™: {e}")
                    continue
            
            print(f"ç¬¬{batch_num}æ‰¹å¤„ç†å®Œæˆ: {len(batch_data)}/{len(batch_rows)} æ¡æœ‰æ•ˆæ•°æ®")
            return batch_data
            
        except Exception as e:
            self.logger.error(f"æ‰¹é‡å¤„ç†ç¬¬{batch_num}æ‰¹æ•°æ®å¤±è´¥: {e}")
            return []

    def _optimize_database_indexes(self):
        """
        ä¼˜åŒ–æ•°æ®åº“ç´¢å¼•å»ºè®® - ä»…è¾“å‡ºå»ºè®®ï¼Œä¸æ‰§è¡Œ
        """
        index_suggestions = [
            "CREATE INDEX idx_roles_type_price ON roles(role_type, price);",
            "CREATE INDEX idx_roles_eid ON roles(eid);",
            "CREATE INDEX idx_large_equip_eid ON large_equip_desc_data(eid);",
            "CREATE INDEX idx_roles_level ON roles(level);",
            "CREATE INDEX idx_roles_serverid ON roles(serverid);",
            "CREATE INDEX idx_roles_school ON roles(school);"
        ]
        
        print("ğŸ” æ•°æ®åº“ç´¢å¼•ä¼˜åŒ–å»ºè®®:")
        for suggestion in index_suggestions:
            print(f"  {suggestion}")
        print("ğŸ“ è¯·åœ¨æ•°æ®åº“ä¸­æ‰‹åŠ¨æ‰§è¡Œè¿™äº›ç´¢å¼•åˆ›å»ºè¯­å¥ä»¥æå‡æŸ¥è¯¢æ€§èƒ½")

    def _get_optimized_connection_config(self, db_config: str) -> Dict:
        """
        è·å–ä¼˜åŒ–çš„æ•°æ®åº“è¿æ¥é…ç½®
        
        Args:
            db_config: åŸå§‹æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²
            
        Returns:
            Dict: ä¼˜åŒ–çš„è¿æ¥å‚æ•°
        """
        return {
            'pool_size': 10,           # è¿æ¥æ± å¤§å°
            'max_overflow': 20,        # æœ€å¤§æº¢å‡ºè¿æ¥æ•°
            'pool_timeout': 30,        # è·å–è¿æ¥è¶…æ—¶æ—¶é—´
            'pool_recycle': 3600,      # è¿æ¥å›æ”¶æ—¶é—´ï¼ˆ1å°æ—¶ï¼‰
            'pool_pre_ping': True,     # è¿æ¥å‰pingæµ‹è¯•
            'echo': False,             # ä¸æ‰“å°SQL
            'connect_args': {
                'charset': 'utf8mb4',
                'connect_timeout': 60,
                'read_timeout': 300,
                'write_timeout': 300,
                'autocommit': True
            }
        }

    def _process_batch_data_parallel(self, batch_rows: List, batch_columns: List, batch_num: int, max_workers: int = 4) -> List[Dict]:
        """
        å¹¶è¡Œæ‰¹é‡å¤„ç†æ•°æ® - ä½¿ç”¨å¤šçº¿ç¨‹ä¼˜åŒ–ç‰¹å¾æå–æ€§èƒ½
        
        Args:
            batch_rows: æ•°æ®åº“æŸ¥è¯¢ç»“æœè¡Œ
            batch_columns: åˆ—ååˆ—è¡¨
            batch_num: æ‰¹æ¬¡å·
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
            
        Returns:
            List[Dict]: å¤„ç†åçš„ç‰¹å¾æ•°æ®åˆ—è¡¨
        """
        try:
            if len(batch_rows) < 100:
                # å°æ‰¹æ¬¡æ•°æ®ç›´æ¥ä¸²è¡Œå¤„ç†ï¼Œé¿å…çº¿ç¨‹å¼€é”€
                return self._process_batch_data(batch_rows, batch_columns, batch_num)
            
            # é¢„å…ˆè½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            role_data_list = [dict(zip(batch_columns, row)) for row in batch_rows]
            
            # åˆ†å‰²æ•°æ®ä¾›å¤šçº¿ç¨‹å¤„ç†
            chunk_size = max(1, len(role_data_list) // max_workers)
            chunks = [role_data_list[i:i + chunk_size] for i in range(0, len(role_data_list), chunk_size)]
            
            batch_data = []
            
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤ä»»åŠ¡
                future_to_chunk = {
                    executor.submit(self._process_data_chunk, chunk, f"{batch_num}-{i+1}"): i 
                    for i, chunk in enumerate(chunks)
                }
                
                # æ”¶é›†ç»“æœ
                for future in concurrent.futures.as_completed(future_to_chunk):
                    chunk_index = future_to_chunk[future]
                    try:
                        chunk_result = future.result()
                        batch_data.extend(chunk_result)
                    except Exception as e:
                        self.logger.error(f"å¤„ç†ç¬¬{batch_num}æ‰¹ç¬¬{chunk_index+1}å—æ•°æ®å¤±è´¥: {e}")
                        continue
            
            print(f"ç¬¬{batch_num}æ‰¹å¹¶è¡Œå¤„ç†å®Œæˆ: {len(batch_data)}/{len(batch_rows)} æ¡æœ‰æ•ˆæ•°æ®")
            return batch_data
            
        except Exception as e:
            self.logger.error(f"å¹¶è¡Œå¤„ç†ç¬¬{batch_num}æ‰¹æ•°æ®å¤±è´¥: {e}")
            # é™çº§åˆ°ä¸²è¡Œå¤„ç†
            return self._process_batch_data(batch_rows, batch_columns, batch_num)

    def _process_data_chunk(self, role_data_chunk: List[Dict], chunk_id: str) -> List[Dict]:
        """
        å¤„ç†æ•°æ®å— - çº¿ç¨‹å®‰å…¨çš„ç‰¹å¾æå–
        
        Args:
            role_data_chunk: è§’è‰²æ•°æ®å—
            chunk_id: å—æ ‡è¯†
            
        Returns:
            List[Dict]: å¤„ç†åçš„ç‰¹å¾æ•°æ®
        """
        try:
            chunk_data = []
            
            for i, role_data in enumerate(role_data_chunk):
                try:
                    # æå–ç‰¹å¾
                    features = self.feature_extractor.extract_features(role_data)
                    
                    # æ·»åŠ åŸºæœ¬ä¿¡æ¯
                    features.update({
                        'eid': role_data.get('eid', ''),
                        'price': role_data.get('price', 0),
                        'school': role_data.get('school', 0),
                        'serverid': role_data.get('serverid', 0)
                    })
                    
                    chunk_data.append(features)
                    
                except Exception as e:
                    self.logger.warning(f"å¤„ç†å—{chunk_id}ç¬¬{i+1}æ¡æ•°æ®æ—¶å‡ºé”™: {e}")
                    continue
            
            return chunk_data
            
        except Exception as e:
            self.logger.error(f"å¤„ç†æ•°æ®å—{chunk_id}å¤±è´¥: {e}")
            return []

    def get_performance_stats(self) -> Dict[str, Any]:
        """
        è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict: æ€§èƒ½ç»Ÿè®¡æ•°æ®
        """
        try:
            stats = {
                'data_loaded': self._data_loaded,
                'last_refresh_time': self._last_refresh_time.isoformat() if self._last_refresh_time else None,
                'data_count': len(self.market_data) if not self.market_data.empty else 0,
                'memory_usage_mb': self.market_data.memory_usage(deep=True).sum() / 1024 / 1024 if not self.market_data.empty else 0,
                'cache_expiry_hours': self._cache_expiry_hours,
                'optimization_suggestions': self._get_optimization_suggestions()
            }
            
            # æ·»åŠ æ•°æ®åº“è¿æ¥æ± çŠ¶æ€
            try:
                from src.app import create_app
                app = create_app()
                with app.app_context():
                    db_config = app.config.get('SQLALCHEMY_DATABASE_URI')
                    if db_config:
                        stats['database_config'] = {
                            'connection_pool_enabled': True,
                            'suggested_indexes': self._get_index_suggestions()
                        }
            except:
                pass
                
            return stats
            
        except Exception as e:
            self.logger.error(f"è·å–æ€§èƒ½ç»Ÿè®¡å¤±è´¥: {e}")
            return {'error': str(e)}

    def _get_optimization_suggestions(self) -> List[str]:
        """è·å–ä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        if hasattr(self, 'market_data') and not self.market_data.empty:
            data_count = len(self.market_data)
            
            if data_count > 100000:
                suggestions.append("æ•°æ®é‡è¾ƒå¤§ï¼Œå»ºè®®ä½¿ç”¨åˆ†å—ç¼“å­˜ç­–ç•¥")
            
            if data_count > 50000:
                suggestions.append("å»ºè®®åœ¨æ•°æ®åº“ä¸­åˆ›å»ºå¤åˆç´¢å¼•ä»¥æå‡æŸ¥è¯¢æ€§èƒ½")
                
            memory_mb = self.market_data.memory_usage(deep=True).sum() / 1024 / 1024
            if memory_mb > 500:
                suggestions.append("å†…å­˜ä½¿ç”¨è¾ƒé«˜ï¼Œå»ºè®®ä¼˜åŒ–æ•°æ®ç±»å‹æˆ–ä½¿ç”¨åˆ†æ‰¹å¤„ç†")
        
        suggestions.extend([
            "å»ºè®®åœ¨æ•°æ®åº“ä¸­åˆ›å»ºæ¨èçš„ç´¢å¼•ä»¥æå‡æŸ¥è¯¢æ€§èƒ½",
            "ä½¿ç”¨Redisåˆ†å—ç¼“å­˜å¯ä»¥æ˜¾è‘—æå‡è¯»å–é€Ÿåº¦",
            "å¹¶è¡Œå¤„ç†å¯ä»¥åŠ é€Ÿç‰¹å¾æå–è¿‡ç¨‹"
        ])
        
        return suggestions

    def _get_index_suggestions(self) -> List[str]:
        """è·å–ç´¢å¼•å»ºè®®"""
        return [
            "CREATE INDEX idx_roles_type_price ON roles(role_type, price);",
            "CREATE INDEX idx_roles_eid ON roles(eid);", 
            "CREATE INDEX idx_large_equip_eid ON large_equip_desc_data(eid);",
            "CREATE INDEX idx_roles_level ON roles(level);",
            "CREATE INDEX idx_roles_serverid ON roles(serverid);"
        ]