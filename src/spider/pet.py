#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¢¦å¹»è¥¿æ¸¸è—å®é˜å¬å”¤å…½çˆ¬è™«æ¨¡å—
ä¸“é—¨ç”¨äºçˆ¬å–å¬å”¤å…½ï¼ˆå¬å”¤å…½ï¼‰æ•°æ®
"""

import os
import sys
import json
import time
import random
import logging
from datetime import datetime
from urllib.parse import urlencode
import asyncio
import pandas as pd
from playwright.async_api import async_playwright
import threading
from concurrent.futures import ThreadPoolExecutor

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
from src.utils.project_path import get_project_root
project_root = get_project_root()
sys.path.insert(0, project_root)

from src.tools.setup_requests_session import setup_session
from src.database import db
from src.models.pet import Pet
from src.tools.search_form_helper import (
    get_pet_search_params_sync,
    get_pet_search_params_async
)
from src.utils.cookie_manager import (
    setup_session_with_cookies, 
    get_playwright_cookies_for_context,
    verify_cookie_validity
)

# å¯¼å…¥å¬å”¤å…½æè¿°è§£æç›¸å…³æ¨¡å—
from src.spider.helper.decode_desc import parse_pet_info

# å¯¼å…¥å¬å”¤å…½ç±»å‹å¸¸é‡
from src.evaluator.constants.equipment_types import PET_CACHE_REQUIRED_FIELDS


class CBGPetSpider:
    def __init__(self):
        self.session = setup_session()
        self.base_url = 'https://xyq.cbg.163.com/cgi-bin/recommend.py'
        self.output_dir = self.create_output_dir()
        
        # é…ç½®ä¸“ç”¨çš„æ—¥å¿—å™¨ï¼Œé¿å…ä¸å…¶ä»–æ¨¡å—å†²çª
        self.logger = self._setup_logger()
        
        # åˆå§‹åŒ–å…¶ä»–ç»„ä»¶
        self.setup_session()
        self.retry_attempts = 1
        
        # åˆå§‹åŒ–çº¿ç¨‹æ± ï¼ˆç”¨äºå¼‚æ­¥æ•°æ®åº“æ“ä½œï¼‰
        self._executor = ThreadPoolExecutor(max_workers=3, thread_name_prefix='PetDB-')
        self.logger.info("çº¿ç¨‹æ± åˆå§‹åŒ–å®Œæˆï¼Œæœ€å¤§å¹¶å‘æ•°: 3")

    def _setup_logger(self):
        """è®¾ç½®ä¸“ç”¨çš„æ—¥å¿—å™¨"""
        # åˆ›å»ºä¸“ç”¨çš„æ—¥å¿—å™¨
        logger = logging.getLogger(f'CBGPetSpider_{id(self)}')
        logger.setLevel(logging.INFO)
        
        # æ¸…é™¤å¯èƒ½å­˜åœ¨çš„å¤„ç†å™¨ï¼Œé¿å…é‡å¤æ—¥å¿—
        if logger.handlers:
            logger.handlers.clear()
        
        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_file = os.path.join(self.output_dir, f'cbg_pet_spider_{timestamp}.log')
        file_handler = logging.FileHandler(log_file, encoding='utf-8', mode='w')
        file_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºæ ¼å¼å™¨
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        # æ·»åŠ å¤„ç†å™¨åˆ°æ—¥å¿—å™¨
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        
        # é˜²æ­¢æ—¥å¿—ä¼ æ’­åˆ°æ ¹æ—¥å¿—å™¨ï¼Œé¿å…é‡å¤è¾“å‡º
        logger.propagate = False
        
        # æµ‹è¯•æ—¥å¿—å†™å…¥
        logger.info(" CBGå¬å”¤å…½çˆ¬è™«æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        logger.info(f" æ—¥å¿—æ–‡ä»¶è·¯å¾„: {log_file}")
        
        return logger

    def create_output_dir(self):
        """åˆ›å»ºè¾“å‡ºç›®å½• - æŒ‰å¹´æœˆåˆ†ç»„"""
        current_date = datetime.now()
        year_month = current_date.strftime('%Y%m')  # 202506
        output_dir = os.path.join('output', year_month)
        os.makedirs(output_dir, exist_ok=True)
        return output_dir
    
    def setup_session(self):
        """è®¾ç½®è¯·æ±‚ä¼šè¯"""
        # ä½¿ç”¨ç»Ÿä¸€çš„Cookieç®¡ç†
        setup_session_with_cookies(
            self.session, 
            referer='https://xyq.cbg.163.com/cgi-bin/equipquery.py?act=show_overall_search_pet',
            logger=self.logger
        )
    

    def parse_jsonp_response(self, text):
        """è§£æJSONPå“åº”ï¼Œæå–å¬å”¤å…½æ•°æ®"""
        try:
            # æå–JSONéƒ¨åˆ†
            start = text.find('(') + 1
            end = text.rfind(')')
            if start <= 0 or end <= 0:
                self.logger.error("å“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONPæ ¼å¼")
                return None
                
            json_str = text[start:end]
            data = json.loads(json_str)
            
            if not isinstance(data, dict):
                self.logger.error("è§£æJSONPå“åº”å¤±è´¥ï¼šå“åº”ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„JSONå¯¹è±¡")
                return None

            equip_list = data.get('equip_list', [])
            
            if not equip_list:
                self.logger.warning(text)
                self.logger.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¬å”¤å…½æ•°æ®")
                return []
                
            pets = []
            for pet in equip_list:
                try:
                    # å¼•å…¥decode_desc.pyä¸­çš„decode_descå‡½æ•°ï¼Œè§£ædescå­—æ®µï¼ŒæŠŠè§£æçš„å­—æ®µå­˜å…¥æ•°æ®åº“ 
                    
                    # è·å–å¹¶è§£æå¬å”¤å…½æè¿°å­—æ®µ
                    raw_desc = pet.get('desc', '')
                    parsed_pet_attrs = {}
                    
                    # å¦‚æœå­˜åœ¨descå­—æ®µï¼Œåˆ™è§£æå¬å”¤å…½å±æ€§
                    if raw_desc:
                        try:
                            parsed_pet_attrs = parse_pet_info(raw_desc)
                            self.logger.debug(f"æˆåŠŸè§£æå¬å”¤å…½æè¿°ï¼Œè·å¾—{len(parsed_pet_attrs)}ä¸ªå±æ€§å­—æ®µ")
                        except Exception as e:
                            self.logger.warning(f"è§£æå¬å”¤å…½æè¿°å¤±è´¥: {e}")
                            parsed_pet_attrs = {}
                    
                    # ç›´æ¥ä¿å­˜æ‰€æœ‰åŸå§‹å­—æ®µï¼Œä¸åšè§£æ
                    pet_data = {
                        # åŸºæœ¬å­—æ®µç›´æ¥æ˜ å°„
                        'eid': pet.get('eid'),
                        'equipid': pet.get('equipid'),
                        'equip_sn': pet.get('equip_sn'),
                        'server_name': pet.get('server_name'),
                        'serverid': pet.get('serverid'),
                        'equip_server_sn': pet.get('equip_server_sn'),
                        'seller_nickname': pet.get('seller_nickname'),
                        'seller_roleid': pet.get('seller_roleid'),
                        'area_name': pet.get('area_name'),
                        'equip_name': pet.get('equip_name'),
                        'equip_type': pet.get('equip_type'),
                        'equip_type_name': pet.get('equip_type_name'),
                        'equip_type_desc': pet.get('equip_type_desc'),
                        'level': pet.get('level'),
                        'equip_level': pet.get('equip_level'),
                        'equip_level_desc': pet.get('equip_level_desc'),
                        'level_desc': pet.get('level_desc'),
                        'subtitle': pet.get('subtitle'),
                        'equip_pos': pet.get('equip_pos'),
                        'position': pet.get('position'),
                        'school': pet.get('school'),
                        'role_grade_limit': pet.get('role_grade_limit'),
                        'min_buyer_level': pet.get('min_buyer_level'),
                        'equip_count': pet.get('equip_count'),
                        'price': pet.get('price'),
                        'price_desc': pet.get('price_desc'),
                        'unit_price_desc': pet.get('unit_price_desc'),
                        'min_unit_price': pet.get('min_unit_price'),
                        'accept_bargain': 1 if pet.get('accept_bargain') else 0,
                        'equip_status': pet.get('equip_status'),
                        'equip_status_desc': pet.get('equip_status_desc'),
                        'status_desc': pet.get('status_desc'),
                        'onsale_expire_time_desc': pet.get('onsale_expire_time_desc'),
                        'time_left': pet.get('time_left'),
                        'expire_time': pet.get('expire_time'),
                        'create_time_equip': pet.get('create_time'),
                        'selling_time': pet.get('selling_time'),
                        'selling_time_ago_desc': pet.get('selling_time_ago_desc'),
                        'first_onsale_time': pet.get('first_onsale_time'),
                        'pass_fair_show': pet.get('pass_fair_show'),
                        'fair_show_time': pet.get('fair_show_time'),
                        'fair_show_end_time': pet.get('fair_show_end_time'),
                        'fair_show_end_time_left': pet.get('fair_show_end_time_left'),
                        'fair_show_poundage': pet.get('fair_show_poundage'),
                  
                        # å…¶ä»–ä¿¡æ¯
                        'collect_num': pet.get('collect_num'),
                        'has_collect': 1 if pet.get('has_collect') else 0,
                        'score': pet.get('score'),
                        'icon_index': pet.get('icon_index'),
                        'icon': pet.get('icon'),
                        'equip_face_img': pet.get('equip_face_img'),
                        'kindid': pet.get('kindid'),
                        'game_channel': pet.get('game_channel'),
                        
                        # è®¢å•ç›¸å…³
                        'game_ordersn': pet.get('game_ordersn'),
                        'whole_game_ordersn': pet.get('whole_game_ordersn'),
                        
                        # è·¨æœç›¸å…³
                        'allow_cross_buy': pet.get('allow_cross_buy'),
                        'cross_server_poundage': pet.get('cross_server_poundage'),
                        'cross_server_poundage_origin': pet.get('cross_server_poundage_origin'),
                        'cross_server_poundage_discount': pet.get('cross_server_poundage_discount'),
                        'cross_server_poundage_discount_label': pet.get('cross_server_poundage_discount_label'),
                        'cross_server_poundage_display_mode': pet.get('cross_server_poundage_display_mode'),
                        'cross_server_activity_conf_discount': pet.get('cross_server_activity_conf_discount'),
                        
                        # æ´»åŠ¨ç›¸å…³
                        'activity_type': pet.get('activity_type'),
                        'joined_seller_activity': 1 if pet.get('joined_seller_activity') else 0,
                        
                        # æ‹†åˆ†ç›¸å…³
                        'is_split_sale': 1 if pet.get('is_split_sale') else 0,
                        'is_split_main_role': 1 if pet.get('is_split_main_role') else 0,
                        'is_split_independent_role': 1 if pet.get('is_split_independent_role') else 0,
                        'is_split_independent_equip': 1 if pet.get('is_split_independent_equip') else 0,
                        'split_equip_sold_happen': 1 if pet.get('split_equip_sold_happen') else 0,
                        'show_split_equip_sold_remind': 1 if pet.get('show_split_equip_sold_remind') else 0,
                        
                        # ä¿æŠ¤ç›¸å…³
                        'is_onsale_protection_period': 1 if pet.get('is_onsale_protection_period') else 0,
                        'onsale_protection_end_time': pet.get('onsale_protection_end_time'),
                        'is_vip_protection': 1 if pet.get('is_vip_protection') else 0,
                        'is_time_lock': 1 if pet.get('is_time_lock') else 0,
                        
                        # æµ‹è¯•æœç›¸å…³
                        'equip_in_test_server': 1 if pet.get('equip_in_test_server') else 0,
                        'buyer_in_test_server': 1 if pet.get('buyer_in_test_server') else 0,
                        'equip_in_allow_take_away_server': 1 if pet.get('equip_in_allow_take_away_server') else 0,
                        
                        # å…¶ä»–æ ‡è¯†
                        'is_weijianding': 1 if pet.get('is_weijianding') else 0,
                        'is_show_alipay_privilege': 1 if pet.get('is_show_alipay_privilege') else 0,
                        'is_seller_redpacket_flag': 1 if pet.get('is_seller_redpacket_flag') else 0,
                        'is_show_expert_desc': pet.get('is_show_expert_desc'),
                        'is_show_special_highlight': 1 if pet.get('is_show_special_highlight') else 0,
                        'is_xyq_game_role_kunpeng_reach_limit': 1 if pet.get('is_xyq_game_role_kunpeng_reach_limit') else 0,
                        
                        # ç‰ˆæœ¬å’Œå­˜å‚¨ç›¸å…³
                        'equip_onsale_version': pet.get('equip_onsale_version'),
                        'storage_type': pet.get('storage_type'),
                        'agent_trans_time': pet.get('agent_trans_time'),
                        
                        # KOLç›¸å…³
                        'kol_article_id': pet.get('kol_article_id'),
                        'kol_share_id': pet.get('kol_share_id'),
                        'kol_share_time': pet.get('kol_share_time'),
                        'kol_share_status': pet.get('kol_share_status'),
                        
                        # æ¨èç›¸å…³
                        'reco_request_id': pet.get('reco_request_id'),
                        'appointed_roleid': pet.get('appointed_roleid'),
                        
                        # å›¢é˜Ÿç›¸å…³
                        'play_team_cnt': pet.get('play_team_cnt'),
                        
                        # éšæœºæŠ½å¥–ç›¸å…³
                        'random_draw_finish_time': pet.get('random_draw_finish_time'),
                        
                        # è¯¦ç»†æè¿°
                        'desc': raw_desc,  # ä½¿ç”¨è§£æè·å–çš„åŸå§‹descå­—æ®µ
                        'large_equip_desc': pet.get('large_equip_desc'),
                        'desc_sumup': pet.get('desc_sumup'),
                        'desc_sumup_short': pet.get('desc_sumup_short'),
                        'diy_desc': pet.get('diy_desc'),
                        'rec_desc': pet.get('rec_desc'),
                        
                        # æœç´¢ç›¸å…³
                        'search_type': pet.get('search_type'),
                        'tag': pet.get('tag'),
                        
                        # JSONæ ¼å¼å­—æ®µ
                        'price_explanation': json.dumps(pet.get('price_explanation'), ensure_ascii=False) if pet.get('price_explanation') else '',
                        'bargain_info': json.dumps(pet.get('bargain_info'), ensure_ascii=False) if pet.get('bargain_info') else '',
                        'diy_desc_pay_info': json.dumps(pet.get('diy_desc_pay_info'), ensure_ascii=False) if pet.get('diy_desc_pay_info') else '',
                        'other_info': pet.get('other_info', ''),
                        'video_info': json.dumps(pet.get('video_info'), ensure_ascii=False) if pet.get('video_info') else '',
                        'agg_added_attrs': json.dumps(pet.get('agg_added_attrs'), ensure_ascii=False) if pet.get('agg_added_attrs') else '',
                        'dynamic_tags': json.dumps(pet.get('dynamic_tags'), ensure_ascii=False) if pet.get('dynamic_tags') else '',
                        'highlight': json.dumps(pet.get('highlight'), ensure_ascii=False) if pet.get('highlight') else '',
                        'tag_key': pet.get('tag_key', ''),
                        
                        # åŸå§‹æ•°æ®
                        'raw_data_json': json.dumps(pet, ensure_ascii=False)
                    }
                    
                    # æ·»åŠ è§£æåçš„å¬å”¤å…½å±æ€§å­—æ®µ - ç›´æ¥ä½¿ç”¨åŸå§‹å­—æ®µå
                    if parsed_pet_attrs:
                        # ç›´æ¥å°†æ‰€æœ‰è§£æå‡ºçš„å­—æ®µæ·»åŠ åˆ°pet_dataä¸­
                        for field_name, field_value in parsed_pet_attrs.items():
                            # å¯¹äºå¤æ‚æ•°æ®ç±»å‹ï¼ˆåˆ—è¡¨å’Œå­—å…¸ï¼‰ï¼Œè½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
                            if isinstance(field_value, (list, dict)):
                                pet_data[field_name] = json.dumps(field_value, ensure_ascii=False)
                            else:
                                pet_data[field_name] = field_value
                    
                    pets.append(pet_data)
                    
                except Exception as e:
                    self.logger.error(f"è§£æå•ä¸ªå¬å”¤å…½æ•°æ®æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            return pets
            
        except Exception as e:
            self.logger.error(f"è§£æJSONPå“åº”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None

    def get_search_params(self, use_browser=False):
        """
        è·å–å¬å”¤å…½æœç´¢å‚æ•°
        - use_browser=True: å¯åŠ¨æµè§ˆå™¨æ‰‹åŠ¨è®¾ç½®å‚æ•°
        - use_browser=False: ä»æœ¬åœ°æ–‡ä»¶æˆ–é»˜è®¤é…ç½®åŠ è½½å‚æ•°
        """
        params_file = 'config/equip_params_pet.json'
        
        # å¼ºåˆ¶æµè§ˆå™¨æ¨¡å¼ï¼šå¦‚æœuse_browserä¸ºTrueï¼Œåˆ™åˆ é™¤æ—§å‚æ•°æ–‡ä»¶
        if use_browser and os.path.exists(params_file):
            self.logger.info(f"å¼ºåˆ¶æµè§ˆå™¨æ¨¡å¼ï¼Œåˆ é™¤æ—§çš„å‚æ•°æ–‡ä»¶: {params_file}")
            os.remove(params_file)

        # ä½¿ç”¨åŒæ­¥çš„å‚æ•°è·å–å‡½æ•°
        return get_pet_search_params_sync(use_browser=use_browser)

    def fetch_page_sync(self, page=1, search_params=None, search_type='overall_search_pet'):
        """
        åŒæ­¥è·å–å•é¡µå¬å”¤å…½æ•°æ®
        """
        if search_params is None:
            search_params = {}
        
        # åŸºç¡€å‚æ•°
        params = {
            'act': search_type,
            'page': page
        }
        
        # åˆå¹¶æœç´¢å‚æ•°
        params.update(search_params)
        
        # æ„å»ºURL
        url = 'https://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?' + urlencode(params)
        self.logger.info(f"æ­£åœ¨è¯·æ±‚URL: {url}")
        
        try:
            response = self.session.get(url, timeout=20)
            response.raise_for_status()
            
            # è§£æJSONPå“åº”
            pets = self.parse_jsonp_response(response.text)
            return pets

        except Exception as e:
            self.logger.error(f"è·å–å¬å”¤å…½æ•°æ®å¤±è´¥ (é¡µç : {page}): {e}")
            return None

    def save_pet_data(self, pets):
        """ä¿å­˜å¬å”¤å…½æ•°æ®åˆ°MySQLæ•°æ®åº“"""
        if not pets:
            self.logger.info("æ²¡æœ‰å¬å”¤å…½æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            # ç¡®ä¿åœ¨Flaskåº”ç”¨ä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œæ•°æ®åº“æ“ä½œ
            from flask import current_app
            from src.app import create_app
            
            # å¦‚æœå½“å‰æ²¡æœ‰åº”ç”¨ä¸Šä¸‹æ–‡ï¼Œåˆ›å»ºä¸€ä¸ª
            if not current_app:
                app = create_app()
                with app.app_context():
                    return self._save_pet_data_with_context(pets)
            else:
                return self._save_pet_data_with_context(pets)
                
        except Exception as e:
            self.logger.error(f"ä¿å­˜å¬å”¤å…½æ•°æ®åˆ°MySQLæ•°æ®åº“å¤±è´¥: {e}")
            return 0
    
    def _filter_pet_fields(self, pets):
        """
        è¿‡æ»¤å¬å”¤å…½å­—æ®µï¼Œåªä¿ç•™ç¼“å­˜æ‰€éœ€å­—æ®µï¼ˆä¼˜åŒ–å†…å­˜å ç”¨ï¼‰
        
        Args:
            pets: å¬å”¤å…½æ•°æ®åˆ—è¡¨
            
        Returns:
            list: è¿‡æ»¤åçš„å¬å”¤å…½æ•°æ®åˆ—è¡¨
        """
        # ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼ï¼Œæ›´é«˜æ•ˆ
        return [
            {k: v for k, v in pet.items() if k in PET_CACHE_REQUIRED_FIELDS}
            for pet in pets
        ]
    
    def _get_redis_total_count(self):
        """
        è·å–Redisä¸­çš„å¬å”¤å…½æ€»æ¡æ•°
        
        Returns:
            int: Redisä¸­çš„å¬å”¤å…½æ€»æ¡æ•°ï¼Œè·å–å¤±è´¥è¿”å›0
        """
        try:
            from src.evaluator.market_anchor.pet.pet_market_data_collector import PetMarketDataCollector
            from src.utils.redis_cache import get_redis_cache
            
            collector = PetMarketDataCollector()
            redis_cache = get_redis_cache()
            
            if redis_cache and redis_cache.is_available():
                # è·å–Redis Hashçš„æ€»æ¡æ•°
                full_key = redis_cache._make_key(collector._full_cache_key)
                total_count = redis_cache.client.hlen(full_key)
                self.logger.debug(f"Rediså¬å”¤å…½æ€»æ¡æ•°: {total_count}")
                return total_count
            else:
                self.logger.warning("Redisä¸å¯ç”¨ï¼Œæ— æ³•è·å–æ€»æ¡æ•°")
                return 0
                
        except Exception as e:
            self.logger.warning(f"è·å–Redisæ€»æ¡æ•°å¤±è´¥: {e}")
            return 0
    
    def _publish_dataframe_message(self, new_data_df, data_count):
        """
        å‘å¸ƒDataFrameæ¶ˆæ¯åˆ°Redisï¼ˆç«‹å³é€šçŸ¥å‰ç«¯ï¼Œåªæ›´æ–°å†…å­˜æ•°æ®ï¼Œä¸åŒ…å«æ€»æ•°ï¼‰
        
        Args:
            new_data_df: DataFrameæ•°æ®
            data_count: æœ¬æ¬¡æ–°å¢çš„æ•°æ®æ¡æ•°
            
        Returns:
            bool: å‘å¸ƒæ˜¯å¦æˆåŠŸ
        """
        try:
            from src.utils.redis_pubsub import get_redis_pubsub, MessageType, Channel
            
            pubsub = get_redis_pubsub()
            message = {
                'type': MessageType.PET_CACHE_UPDATED,
                'data_count': data_count,  # æœ¬æ¬¡æ•°æ®é‡
                'action': 'add_dataframe'  # åªç”¨äºç«‹å³æ›´æ–°å†…å­˜ç¼“å­˜
            }
            
            success = pubsub.publish_with_dataframe(Channel.PET_UPDATES, message, new_data_df)
            if success:
                self.logger.info(f"ğŸ“¢ å·²ç«‹å³å‘å¸ƒDataFrameæ¶ˆæ¯åˆ°Redis (æœ¬æ¬¡:{data_count}æ¡)")
            else:
                self.logger.warning("âš ï¸ å‘å¸ƒDataFrameæ¶ˆæ¯å¤±è´¥")
            
            return success
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ å‘å¸ƒDataFrameæ¶ˆæ¯å¤±è´¥: {e}")
            return False
    
    def _batch_save_to_mysql(self, pets):
        """
        æ‰¹é‡ä¿å­˜åˆ°MySQLï¼Œä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½
        
        Args:
            pets: å¬å”¤å…½æ•°æ®åˆ—è¡¨
            
        Returns:
            tuple: (æ–°å¢æ•°é‡, æ›´æ–°æ•°é‡)
        """
        try:
            # æ‰¹é‡æŸ¥è¯¢å·²å­˜åœ¨çš„å¬å”¤å…½ï¼ˆä¸€æ¬¡æŸ¥è¯¢ä»£æ›¿Næ¬¡æŸ¥è¯¢ï¼‰
            equip_sns = [pet.get('equip_sn') for pet in pets if pet.get('equip_sn')]
            
            existing_pets = {}
            if equip_sns:
                existing_list = db.session.query(Pet).filter(
                    Pet.equip_sn.in_(equip_sns)
                ).all()
                existing_pets = {pet.equip_sn: pet for pet in existing_list}
            
            new_pets = []
            updated_count = 0
            
            # éå†å¬å”¤å…½æ•°æ®ï¼Œåˆ†ç±»ä¸ºæ–°å¢å’Œæ›´æ–°
            # å…ˆå¯¹åŒä¸€æ‰¹æ¬¡ä¸­çš„é‡å¤æ•°æ®è¿›è¡Œå»é‡ï¼ˆä¿ç•™æœ€åä¸€ä¸ªï¼‰
            seen_equip_sns = set()
            unique_pets = []
            for pet_data in pets:
                equip_sn = pet_data.get('equip_sn')
                if equip_sn:
                    if equip_sn in seen_equip_sns:
                        # æ‰¾åˆ°å·²å­˜åœ¨çš„è®°å½•å¹¶æ›¿æ¢
                        for i, existing_pet in enumerate(unique_pets):
                            if existing_pet.get('equip_sn') == equip_sn:
                                unique_pets[i] = pet_data
                                break
                    else:
                        seen_equip_sns.add(equip_sn)
                        unique_pets.append(pet_data)
                else:
                    # æ²¡æœ‰equip_snçš„è®°å½•ç›´æ¥æ·»åŠ 
                    unique_pets.append(pet_data)
            
            for pet_data in unique_pets:
                try:
                    equip_sn = pet_data.get('equip_sn')
                    
                    if equip_sn and equip_sn in existing_pets:
                        # æ›´æ–°ç°æœ‰è®°å½•
                        existing = existing_pets[equip_sn]
                        for key, value in pet_data.items():
                            if hasattr(existing, key):
                                setattr(existing, key, value)
                        existing.update_time = datetime.now()
                        updated_count += 1
                    else:
                        # å‡†å¤‡æ–°è®°å½•
                        new_pets.append(Pet(**pet_data))
                        
                except Exception as e:
                    self.logger.error(f"å¤„ç†å•ä¸ªå¬å”¤å…½æ•°æ®å¤±è´¥: {e}")
                    continue
            
            # æ‰¹é‡æ’å…¥æ–°è®°å½•
            if new_pets:
                db.session.bulk_save_objects(new_pets)
            
            # æäº¤äº‹åŠ¡
            db.session.commit()
            
            if len(new_pets) > 0:
                self.logger.info(f"âœ… æˆåŠŸä¿å­˜ {len(new_pets)} æ¡æ–°å¬å”¤å…½æ•°æ®åˆ°MySQLæ•°æ®åº“")
            if updated_count > 0:
                self.logger.info(f"âœ… æ›´æ–° {updated_count} æ¡å·²å­˜åœ¨çš„å¬å”¤å…½æ•°æ®")
            
            return len(new_pets), updated_count
            
        except Exception as e:
            db.session.rollback()
            self.logger.error(f"æ‰¹é‡ä¿å­˜MySQLå¤±è´¥: {e}")
            raise
    
    def _sync_to_redis_cache(self, new_data_df):
        """
        åŒæ­¥æ–°æ•°æ®åˆ°Redisç¼“å­˜
        
        Args:
            new_data_df: æ–°æ•°æ®çš„DataFrame
            
        Returns:
            bool: åŒæ­¥æ˜¯å¦æˆåŠŸ
        """
        try:
            from src.evaluator.market_anchor.pet.pet_market_data_collector import PetMarketDataCollector
            
            collector = PetMarketDataCollector()
            redis_success = collector._sync_new_data_to_redis(new_data_df)
            
            if redis_success:
                self.logger.info("âœ… æ–°æ•°æ®å·²åŒæ­¥åˆ°Redisç¼“å­˜")
            else:
                self.logger.warning("âš ï¸ æ–°æ•°æ®åŒæ­¥åˆ°Rediså¤±è´¥")
            
            return redis_success
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ åŒæ­¥æ–°æ•°æ®åˆ°Rediså¤±è´¥: {e}")
            return False
    
    def _save_pet_data_with_context(self, pets):
        """åœ¨Flaskåº”ç”¨ä¸Šä¸‹æ–‡ä¸­ä¿å­˜å¬å”¤å…½æ•°æ® - å†…å­˜ç¼“å­˜ä¼˜å…ˆå¿«é€Ÿå“åº” â†’ MySQL â†’ Redis"""
        try:
            # åœ¨å­çº¿ç¨‹ä¸­é‡æ–°å¯¼å…¥pandasï¼Œç¡®ä¿å¯ç”¨
            import pandas as pd
            
            if not pets:
                self.logger.info("æ²¡æœ‰å¬å”¤å…½æ•°æ®éœ€è¦ä¿å­˜")
                return 0
            
            self.logger.info(f"å¼€å§‹ä¿å­˜ {len(pets)} æ¡å¬å”¤å…½æ•°æ®...")
            
            # ä¼˜åŒ–ï¼šåªè¿‡æ»¤ä¸€æ¬¡ï¼Œé¿å…é‡å¤ä»£ç 
            filtered_pets = self._filter_pet_fields(pets)
            new_data_df = pd.DataFrame(filtered_pets)
            
            # ç¬¬ä¸€æ­¥ï¼šç«‹å³å‘å¸ƒDataFrameæ¶ˆæ¯ï¼Œå¿«é€Ÿæ›´æ–°å†…å­˜ç¼“å­˜ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼Œè¶…å¿«å“åº”ï¼‰
            publish_success = self._publish_dataframe_message(new_data_df, len(pets))
            
            if publish_success:
                self.logger.info(f"âœ… å·²ç«‹å³é€šçŸ¥å†…å­˜ç¼“å­˜æ›´æ–° {len(pets)} æ¡æ•°æ®ï¼ˆå¿«é€Ÿå“åº”ï¼‰")
            else:
                self.logger.warning(f"âš ï¸ å†…å­˜ç¼“å­˜é€šçŸ¥å‘é€å¤±è´¥ï¼Œä½†ç»§ç»­ä¿å­˜åˆ°æ•°æ®åº“")
            
            # ç¬¬äºŒæ­¥ï¼šå¼‚æ­¥æ‰¹é‡ä¿å­˜åˆ°MySQLå’ŒRedisï¼ˆä¸é˜»å¡ä¸»æµç¨‹ï¼‰
            self._submit_async_save_task(pets, new_data_df)
            
            self.logger.info(f"ğŸ‰ å¬å”¤å…½æ•°æ®ä¿å­˜æµç¨‹: å†…å­˜ç¼“å­˜(âœ…å·²é€šçŸ¥) â†’ MySQL(å¤„ç†ä¸­) â†’ Redis(å¤„ç†ä¸­)")
            
            # è¿”å›é¢„ä¼°çš„æ–°å¢æ•°é‡ï¼ˆå®é™…æ•°é‡ç”±å¼‚æ­¥ä»»åŠ¡è®¡ç®—ï¼‰
            return len(pets)
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜å¬å”¤å…½æ•°æ®å¤±è´¥: {e}")
            return 0
    
    def _submit_async_save_task(self, pets, new_data_df):
        """
        æäº¤å¼‚æ­¥ä¿å­˜ä»»åŠ¡åˆ°çº¿ç¨‹æ± 
        
        Args:
            pets: å¬å”¤å…½æ•°æ®åˆ—è¡¨
            new_data_df: è¿‡æ»¤åçš„DataFrame
        """
        try:
            # ä½¿ç”¨çº¿ç¨‹æ± å¼‚æ­¥æ‰§è¡Œä¿å­˜ä»»åŠ¡
            future = self._executor.submit(
                self._async_batch_save_worker,
                pets,
                new_data_df
            )
            
            # æ·»åŠ å®Œæˆå›è°ƒï¼Œè®°å½•ç»“æœ
            future.add_done_callback(self._async_save_callback)
            
        except Exception as e:
            self.logger.error(f"âŒ æäº¤å¼‚æ­¥ä¿å­˜ä»»åŠ¡å¤±è´¥: {e}")
    
    def _async_save_callback(self, future):
        """å¼‚æ­¥ä¿å­˜ä»»åŠ¡å®Œæˆå›è°ƒ - è®°å½•MySQLå’ŒRedisä¿å­˜ç»“æœ"""
        try:
            result = future.result()
            if result:
                saved_count, updated_count, saved_dataframe, redis_synced = result
                self.logger.info(
                    f"âœ… å¼‚æ­¥ä¿å­˜å®Œæˆ: æ–°å¢ {saved_count} æ¡, æ›´æ–° {updated_count} æ¡, RedisåŒæ­¥: {'æˆåŠŸ' if redis_synced else 'å¤±è´¥'}"
                )
                
                # å†…å­˜ç¼“å­˜å·²åœ¨ä¸»çº¿ç¨‹ä¸­ç«‹å³æ›´æ–°ï¼Œè¿™é‡Œä¸å†å‘é€æ¶ˆæ¯
                if saved_count > 0 and redis_synced:
                    self.logger.info(f"âœ… MySQLå’ŒRedisä¿å­˜æˆåŠŸï¼Œæ•°æ®ä¸€è‡´æ€§å·²ä¿è¯")
                elif saved_count > 0 and not redis_synced:
                    self.logger.warning(f"âš ï¸ MySQLä¿å­˜æˆåŠŸä½†RedisåŒæ­¥å¤±è´¥ï¼Œå†…å­˜å’ŒMySQLå·²æœ‰æ•°æ®ï¼Œéœ€æ‰‹åŠ¨åŒæ­¥Redis")
                elif updated_count > 0:
                    self.logger.info(f"ğŸ“ æ— æ–°å¢æ•°æ®ï¼Œæ›´æ–°äº† {updated_count} æ¡ç°æœ‰æ•°æ®")
                
        except Exception as e:
            self.logger.error(f"âŒ å¼‚æ­¥ä¿å­˜ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
    
    def _async_batch_save_worker(self, pets, new_data_df):
        """
        å¼‚æ­¥æ‰¹é‡ä¿å­˜å·¥ä½œçº¿ç¨‹ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰- MySQL â†’ Redis â†’ é€šçŸ¥å†…å­˜ç¼“å­˜
        
        Args:
            pets: å¬å”¤å…½æ•°æ®åˆ—è¡¨
            new_data_df: è¿‡æ»¤åçš„DataFrame
            
        Returns:
            tuple: (æ–°å¢æ•°é‡, æ›´æ–°æ•°é‡, å®é™…ä¿å­˜çš„DataFrame, RedisåŒæ­¥çŠ¶æ€)
        """
        try:
            # åœ¨Flaskåº”ç”¨ä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œæ•°æ®åº“æ“ä½œ
            from src.app import create_app
            app = create_app()
            
            with app.app_context():
                self.logger.info(f"ğŸ”„ å¼‚æ­¥ä»»åŠ¡å¼€å§‹å¤„ç† {len(pets)} æ¡å¬å”¤å…½æ•°æ®...")
                
                # ç¬¬ä¸€æ­¥ï¼šæ‰¹é‡ä¿å­˜åˆ°MySQL
                saved_count, updated_count = self._batch_save_to_mysql(pets)
                
                # ç¬¬äºŒæ­¥ï¼šåŒæ­¥åˆ°Redisï¼ˆåªåœ¨æœ‰æ–°æ•°æ®æ—¶ï¼‰
                redis_synced = False
                saved_dataframe = None
                
                if saved_count > 0:
                    # åŒæ­¥åˆ°Redisç¼“å­˜
                    redis_synced = self._sync_to_redis_cache(new_data_df)
                    
                    if redis_synced:
                        # è¿”å›å®é™…æ–°å¢çš„æ•°æ®ï¼ˆç”¨äºé€šçŸ¥å†…å­˜ç¼“å­˜æ›´æ–°ï¼‰
                        saved_dataframe = new_data_df.copy()
                        self.logger.info(f"âœ… MySQLå’ŒRedisä¿å­˜æˆåŠŸï¼Œå‡†å¤‡é€šçŸ¥å†…å­˜ç¼“å­˜æ›´æ–°")
                    else:
                        self.logger.warning(f"âš ï¸ MySQLä¿å­˜æˆåŠŸä½†RedisåŒæ­¥å¤±è´¥ï¼Œä¸æ›´æ–°å†…å­˜ç¼“å­˜")
                
                return saved_count, updated_count, saved_dataframe, redis_synced
                
        except Exception as e:
            self.logger.error(f"âŒ å¼‚æ­¥æ‰¹é‡ä¿å­˜å·¥ä½œçº¿ç¨‹å¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return 0, 0, None, False
    
    def __del__(self):
        """ææ„æ–¹æ³•ï¼Œç¡®ä¿çº¿ç¨‹æ± æ­£ç¡®å…³é—­"""
        try:
            if hasattr(self, '_executor'):
                self.logger.info("æ­£åœ¨å…³é—­çº¿ç¨‹æ± ...")
                self._executor.shutdown(wait=True, cancel_futures=False)
                self.logger.info("çº¿ç¨‹æ± å·²å…³é—­")
        except Exception as e:
            # ææ„æ—¶å¯èƒ½loggerå·²ç»è¢«å›æ”¶
            print(f"å…³é—­çº¿ç¨‹æ± æ—¶å‡ºé”™: {e}")

    async def fetch_page(self, page=1, search_params=None, search_type='overall_search_pet'):
        """
        è·å–å•é¡µå¬å”¤å…½æ•°æ®
        
        Args:
            page: é¡µç 
            search_params: æœç´¢å‚æ•°
            search_type: æœç´¢ç±»å‹
            
        Returns:
            list: è§£æåçš„å¬å”¤å…½æ•°æ®
        """
        try:
            # ç¡®ä¿search_paramsä¸ä¸ºNone
            if search_params is None:
                # æä¾›ä¸€ä¸ªåŸºç¡€çš„é»˜è®¤å€¼
                search_params = { 'level_min': 0, 'level_max': 180, 'server_type': 3, 'evol_skill_mode': 0 }

            # æ„å»ºè¯·æ±‚å‚æ•°
            params = {
                **search_params,  # æ·»åŠ æœç´¢å‚æ•°
                'callback': 'Request.JSONP.request_map.request_0',
                '_': str(int(time.time() * 1000)),
                'act': 'recommd_by_role',
                'page': page,
                'count': 15,
                'search_type': search_type,
                'view_loc': 'overall_search',
                'server_type': 3,# é»˜è®¤3å¹´å¤–æœåŠ¡å™¨
            }
            
            # æ„å»ºå®Œæ•´URL
            url = f"{self.base_url}?{urlencode(params)}"
            
            # ä½¿ç”¨Playwrightå‘é€è¯·æ±‚
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context()
                
                # è®¾ç½®cookies
                cookies = get_playwright_cookies_for_context(self.logger)
                if cookies:
                    await context.add_cookies(cookies)
                
                page_obj = await context.new_page()
                response = await page_obj.goto(url)
                
                if response:
                    text = await response.text()
                    await browser.close()
                    
                    # è§£æå“åº”
                    parsed_result = self.parse_jsonp_response(text)
                    return parsed_result
                
                await browser.close()
                return None
                
        except Exception as e:
            self.logger.error(f"è·å–å¬å”¤å…½ç¬¬{page}é¡µæ•°æ®æ—¶å‡ºé”™: {e}")
            return None

    async def crawl_all_pages_async(self, max_pages=10, delay_range=None, use_browser=False, cached_params=None):
        """
        å¼‚æ­¥çˆ¬å–æ‰€æœ‰å¬å”¤å…½é¡µé¢
        """
        # é¦–å…ˆéªŒè¯Cookieæœ‰æ•ˆæ€§
        self.logger.info("æ­£åœ¨éªŒè¯Cookieæœ‰æ•ˆæ€§...")
        from src.utils.cookie_manager import verify_cookie_validity_async
        if not await verify_cookie_validity_async(self.logger):
            self.logger.warning("CookieéªŒè¯å¤±è´¥ï¼Œæ­£åœ¨æ›´æ–°Cookie...")
            from src.utils.cookie_manager import _update_cookies_internal
            if not await _update_cookies_internal():
                self.logger.error("Cookieæ›´æ–°å¤±è´¥ï¼Œæ— æ³•ç»§ç»­çˆ¬å–")
                return
            else:
                self.logger.info("Cookieæ›´æ–°æˆåŠŸï¼Œé‡æ–°è®¾ç½®ä¼šè¯")
                # é‡æ–°è®¾ç½®ä¼šè¯
                self.setup_session()
        else:
            self.logger.info("CookieéªŒè¯é€šè¿‡")

        search_type = 'overall_search_pet'

        self.logger.info(f" å¼€å§‹å¬å”¤å…½çˆ¬å–ï¼Œæœ€å¤§é¡µæ•°: {max_pages}")

        # è·å–å‚æ•°
        params_file = 'config/pet_params.json'
        
        # å¼ºåˆ¶æµè§ˆå™¨æ¨¡å¼ï¼šå¦‚æœuse_browserä¸ºTrueï¼Œåˆ™åˆ é™¤æ—§å‚æ•°æ–‡ä»¶
        if use_browser and os.path.exists(params_file):
            self.logger.info(f"å¼ºåˆ¶æµè§ˆå™¨æ¨¡å¼ï¼Œåˆ é™¤æ—§çš„å‚æ•°æ–‡ä»¶: {params_file}")
            os.remove(params_file)
        
        # ä½¿ç”¨ä¼ å…¥çš„ç¼“å­˜å‚æ•°æˆ–è·å–æ–°å‚æ•°
        if cached_params and not use_browser:
            if 'server_id' in cached_params:
                search_type = 'search_pet'
            search_params = cached_params
            self.logger.info(f" ä½¿ç”¨ä¼ å…¥çš„ç¼“å­˜å‚æ•°: {len(search_params)} ä¸ª")
        else:
            search_params = await get_pet_search_params_async(use_browser=use_browser)
            if search_params:
                self.logger.info(f" ä½¿ç”¨æœç´¢å‚æ•°: {search_params}")

        if not search_params:
            self.logger.error(f"æ— æ³•è·å–å¬å”¤å…½çš„æœç´¢å‚æ•°ï¼Œçˆ¬å–ä¸­æ­¢")
            return
            
        total_saved_count = 0
        successful_pages = 0
        
        for page_num in range(1, max_pages + 1):
            try:
                # ç¡®ä¿æ—¥å¿—ç«‹å³è¾“å‡º
                self.logger.info(f"ğŸ“„ æ­£åœ¨çˆ¬å–å¬å”¤å…½ç¬¬ {page_num} é¡µ...")
                # å¼ºåˆ¶åˆ·æ–°æ—¥å¿—ç¼“å†²
                import sys
                sys.stdout.flush()
                
                pets = await self.fetch_page(page_num, search_params, search_type)
                
                if pets is None:
                    self.logger.warning(f" ç¬¬ {page_num} é¡µæ•°æ®è·å–å¤±è´¥ï¼Œå°è¯•é‡è¯•...")
                    await asyncio.sleep(5) # ç­‰å¾…5ç§’é‡è¯•
                    pets = await self.fetch_page(page_num, search_params, search_type)

                if pets:
                    saved_count = self.save_pet_data(pets)
                    total_saved_count += saved_count
                    successful_pages += 1
                    
                    # æ‰“å°æ¯æ¡å¬å”¤å…½çš„ç®€è¦ä¿¡æ¯
                    for pet in pets:
                        price = pet.get('price_desc', pet.get('price', 'æœªçŸ¥'))
                        pet_name = pet.get('equip_name', 'æœªçŸ¥å¬å”¤å…½')
                        level = pet.get('level', 'æœªçŸ¥')
                        server_name = pet.get('server_name', 'æœªçŸ¥æœåŠ¡å™¨')
                        seller_nickname = pet.get('seller_nickname', 'æœªçŸ¥å–å®¶')
                        desc_sumup_short = pet.get('desc_sumup_short', 'æ— æè¿°')
                        self.logger.info(f" ï¿¥{price} - {pet_name}({level}çº§) - {desc_sumup_short} - {server_name} - {seller_nickname}")
                    
                    self.logger.info(f" ç¬¬ {page_num} é¡µå®Œæˆï¼Œè·å– {len(pets)} æ¡å¬å”¤å…½ï¼Œä¿å­˜ {saved_count} æ¡")
                else:
                    self.logger.info(f"ğŸ“„ ç¬¬ {page_num} é¡µæ²¡æœ‰æ•°æ®ï¼Œçˆ¬å–ç»“æŸ")
                    break 

                # æ·»åŠ å»¶è¿Ÿ
                if delay_range and page_num < max_pages:  # æœ€åä¸€é¡µä¸éœ€è¦å»¶è¿Ÿ
                    delay = random.uniform(delay_range[0], delay_range[1])
                    self.logger.info(f"â³ ç­‰å¾… {delay:.2f} ç§’åç»§ç»­...")
                    await asyncio.sleep(delay)
                    
            except Exception as e:
                self.logger.error(f"å¤„ç†ç¬¬ {page_num} é¡µæ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
                import traceback
                traceback.print_exc()
                break

        self.logger.info(f" å¬å”¤å…½çˆ¬å–å®Œæˆï¼æˆåŠŸé¡µæ•°: {successful_pages}/{max_pages}, æ€»å¬å”¤å…½æ•°: {total_saved_count}")
        
        # å¼ºåˆ¶åˆ·æ–°æ‰€æœ‰æ—¥å¿—ç¼“å†²åŒºï¼Œç¡®ä¿æ—¥å¿—è¢«å®Œæ•´å†™å…¥æ–‡ä»¶
        import sys
        sys.stdout.flush()
        sys.stderr.flush()
        
        # åˆ·æ–°æ—¥å¿—å¤„ç†å™¨ç¼“å†²åŒº
        for handler in self.logger.handlers:
            if hasattr(handler, 'flush'):
                handler.flush()

    def crawl_all_pages(self, max_pages=10, delay_range=None, use_browser=False, cached_params=None):
        """
        åŒæ­¥å¯åŠ¨å¼‚æ­¥å¬å”¤å…½çˆ¬è™«çš„å…¥å£
        """
        try:
            asyncio.run(self.crawl_all_pages_async(
                max_pages=max_pages,
                delay_range=delay_range,
                use_browser=use_browser,
                cached_params=cached_params
            ))
        except Exception as e:
            self.logger.error(f"å¯åŠ¨å¬å”¤å…½çˆ¬è™«å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()