#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¢¦å¹»è¥¿æ¸¸è—å®é˜å¬å”¤å…½çˆ¬è™«æ¨¡å—
ä¸“é—¨ç”¨äºçˆ¬å–å¬å”¤å…½ï¼ˆå® ç‰©ï¼‰æ•°æ®
"""

import os
import sys
import json
import sqlite3
import time
import random
import logging
from datetime import datetime
from urllib.parse import urlencode
import asyncio
from playwright.async_api import async_playwright

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
sys.path.insert(0, project_root)

from src.tools.setup_requests_session import setup_session
from src.utils.smart_db_helper import CBGSmartDB
from src.cbg_config import DB_TABLE_SCHEMAS, DB_TABLE_ORDER
from src.tools.search_form_helper import (
    get_pet_search_params_sync,
    get_pet_search_params_async,
    verify_cookie_validity,
)


class CBGPetSpider:
    def __init__(self):
        self.session = setup_session()
        self.base_url = 'https://xyq.cbg.163.com/cgi-bin/recommend.py'
        self.output_dir = self.create_output_dir()
        
        # ä½¿ç”¨æŒ‰æœˆåˆ†å‰²çš„æ•°æ®åº“æ–‡ä»¶è·¯å¾„
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        current_month = datetime.now().strftime('%Y%m')
        
        # å® ç‰©æ•°æ®åº“è·¯å¾„
        db_filename = f"cbg_pets_{current_month}.db"
        self.db_path = os.path.join(project_root, 'data', db_filename)
        
        # ç¡®ä¿dataç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        
        # åˆå§‹åŒ–æ™ºèƒ½æ•°æ®åº“åŠ©æ‰‹
        self.smart_db = CBGSmartDB(self.db_path)
        
        # é…ç½®ä¸“ç”¨çš„æ—¥å¿—å™¨ï¼Œé¿å…ä¸å…¶ä»–æ¨¡å—å†²çª
        self.logger = self._setup_logger()
        
        # åˆå§‹åŒ–å…¶ä»–ç»„ä»¶
        self.setup_session()
        self.init_database()
        self.retry_attempts = 1

    def _setup_logger(self):
        """è®¾ç½®ä¸“ç”¨çš„æ—¥å¿—å™¨"""
        # åˆ›å»ºä¸“ç”¨çš„æ—¥å¿—å™¨
        logger = logging.getLogger(f'CBGPetSpider_{id(self)}')
        logger.setLevel(logging.INFO)
        
        # æ¸…é™¤å¯èƒ½å­˜åœ¨çš„å¤„ç†å™¨ï¼Œé¿å…é‡å¤æ—¥å¿—
        if logger.handlers:
            logger.handlers.clear()
        
        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_file = os.path.join(self.output_dir, f'cbg_pet_spider_{timestamp}.log')
        file_handler = logging.FileHandler(log_file, encoding='utf-8', mode='w')
        file_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºæ ¼å¼å™¨
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        # æ·»åŠ å¤„ç†å™¨åˆ°æ—¥å¿—å™¨
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        
        # é˜²æ­¢æ—¥å¿—ä¼ æ’­åˆ°æ ¹æ—¥å¿—å™¨ï¼Œé¿å…é‡å¤è¾“å‡º
        logger.propagate = False
        
        # æµ‹è¯•æ—¥å¿—å†™å…¥
        logger.info("ğŸ‰ CBGå® ç‰©çˆ¬è™«æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶è·¯å¾„: {log_file}")
        
        return logger

    def create_output_dir(self):
        """åˆ›å»ºè¾“å‡ºç›®å½• - æŒ‰å¹´æœˆåˆ†ç»„"""
        current_date = datetime.now()
        year_month = current_date.strftime('%Y%m')  # 202506
        output_dir = os.path.join('output', year_month)
        os.makedirs(output_dir, exist_ok=True)
        return output_dir
    
    def setup_session(self):
        """è®¾ç½®è¯·æ±‚ä¼šè¯"""
        # ä»cookies.txtæ–‡ä»¶è¯»å–Cookie
        cookie_content = None
        try:
            # è·å–é¡¹ç›®æ ¹ç›®å½•
            project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            cookies_path = os.path.join(project_root, 'config/cookies.txt')
            
            with open(cookies_path, 'r', encoding='utf-8') as f:
                cookie_content = f.read().strip()
            if cookie_content:
                self.session.headers.update({'Cookie': cookie_content})
                self.logger.info("æˆåŠŸä»config/cookies.txtæ–‡ä»¶åŠ è½½Cookie")
            else:
                self.logger.warning("config/cookies.txtæ–‡ä»¶ä¸ºç©º")
                
        except FileNotFoundError:
            self.logger.error("æœªæ‰¾åˆ°config/cookies.txtæ–‡ä»¶ï¼Œè¯·åˆ›å»ºè¯¥æ–‡ä»¶å¹¶æ·»åŠ æœ‰æ•ˆçš„Cookie")
        except Exception as e:
            self.logger.error(f"è¯»å–config/cookies.txtæ–‡ä»¶å¤±è´¥: {e}")
        
        # è®¾ç½®è¯·æ±‚å¤´
        headers = {
            'accept': '*/*',
            'accept-language': 'zh-CN,zh;q=0.9',
            'sec-ch-ua': '"Google Chrome";v="137", "Chromium";v="137", "Not/A)Brand";v="24"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"',
            'sec-fetch-dest': 'script',
            'sec-fetch-mode': 'no-cors',
            'sec-fetch-site': 'same-origin',
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36',
            'referer': 'https://xyq.cbg.163.com/cgi-bin/equipquery.py?act=show_overall_search_pet'
        }
        
        # å¦‚æœæœ‰Cookieï¼Œæ·»åŠ åˆ°è¯·æ±‚å¤´ä¸­
        if cookie_content:
            headers['Cookie'] = cookie_content
            self.logger.info("Cookieå·²æ·»åŠ åˆ°è¯·æ±‚å¤´")
        else:
            self.logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„Cookieï¼Œå¯èƒ½å½±å“æ•°æ®è·å–")
        
        self.session.headers.update(headers)
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“å’Œè¡¨ç»“æ„"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # åªåˆ›å»ºå® ç‰©ç›¸å…³çš„è¡¨
            cursor.execute(DB_TABLE_SCHEMAS['pets'])
            self.logger.debug("å® ç‰©æ•°æ®åº“åˆ›å»ºè¡¨: pets")
            
            conn.commit()
            self.logger.info(f"å® ç‰©æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ: {os.path.basename(self.db_path)}")
            
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–å® ç‰©æ•°æ®åº“å¤±è´¥: {e}")
            raise
        finally:
            conn.close()

    def parse_jsonp_response(self, text):
        """è§£æJSONPå“åº”ï¼Œæå–å® ç‰©æ•°æ®"""
        try:
            # æå–JSONéƒ¨åˆ†
            start = text.find('(') + 1
            end = text.rfind(')')
            if start <= 0 or end <= 0:
                self.logger.error("å“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONPæ ¼å¼")
                return None
                
            json_str = text[start:end]
            data = json.loads(json_str)
            
            if not isinstance(data, dict):
                self.logger.error("è§£æJSONPå“åº”å¤±è´¥ï¼šå“åº”ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„JSONå¯¹è±¡")
                return None

            equip_list = data.get('equip_list', [])
            
            if not equip_list:
                self.logger.warning(text)
                self.logger.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å® ç‰©æ•°æ®")
                return []
                
            pets = []
            for pet in equip_list:
                try:
                    # ç›´æ¥ä¿å­˜æ‰€æœ‰åŸå§‹å­—æ®µï¼Œä¸åšè§£æ
                    pet_data = {
                        # åŸºæœ¬å­—æ®µç›´æ¥æ˜ å°„
                        'eid': pet.get('eid'),
                        'equipid': pet.get('equipid'),
                        'equip_sn': pet.get('equip_sn'),
                        'server_name': pet.get('server_name'),
                        'serverid': pet.get('serverid'),
                        'equip_server_sn': pet.get('equip_server_sn'),
                        'seller_nickname': pet.get('seller_nickname'),
                        'seller_roleid': pet.get('seller_roleid'),
                        'area_name': pet.get('area_name'),
                        'equip_name': pet.get('equip_name'),
                        'equip_type': pet.get('equip_type'),
                        'equip_type_name': pet.get('equip_type_name'),
                        'equip_type_desc': pet.get('equip_type_desc'),
                        'level': pet.get('level'),
                        'equip_level': pet.get('equip_level'),
                        'equip_level_desc': pet.get('equip_level_desc'),
                        'level_desc': pet.get('level_desc'),
                        'subtitle': pet.get('subtitle'),
                        'equip_pos': pet.get('equip_pos'),
                        'position': pet.get('position'),
                        'school': pet.get('school'),
                        'role_grade_limit': pet.get('role_grade_limit'),
                        'min_buyer_level': pet.get('min_buyer_level'),
                        'equip_count': pet.get('equip_count'),
                        'price': pet.get('price'),
                        'price_desc': pet.get('price_desc'),
                        'unit_price_desc': pet.get('unit_price_desc'),
                        'min_unit_price': pet.get('min_unit_price'),
                        'accept_bargain': 1 if pet.get('accept_bargain') else 0,
                        'equip_status': pet.get('equip_status'),
                        'equip_status_desc': pet.get('equip_status_desc'),
                        'status_desc': pet.get('status_desc'),
                        'onsale_expire_time_desc': pet.get('onsale_expire_time_desc'),
                        'time_left': pet.get('time_left'),
                        'expire_time': pet.get('expire_time'),
                        'create_time_equip': pet.get('create_time'),
                        'selling_time': pet.get('selling_time'),
                        'selling_time_ago_desc': pet.get('selling_time_ago_desc'),
                        'first_onsale_time': pet.get('first_onsale_time'),
                        'pass_fair_show': pet.get('pass_fair_show'),
                        'fair_show_time': pet.get('fair_show_time'),
                        'fair_show_end_time': pet.get('fair_show_end_time'),
                        'fair_show_end_time_left': pet.get('fair_show_end_time_left'),
                        'fair_show_poundage': pet.get('fair_show_poundage'),
                        
                        # å® ç‰©å±æ€§
                        'hp': pet.get('hp'),
                        'qixue': pet.get('qixue'),
                        'init_hp': pet.get('init_hp'),
                        'mofa': pet.get('mofa'),
                        'init_wakan': pet.get('init_wakan'),
                        'mingzhong': pet.get('mingzhong'),
                        'fangyu': pet.get('fangyu'),
                        'init_defense': pet.get('init_defense'),
                        'defense': pet.get('defense'),
                        'speed': pet.get('speed'),
                        'minjie': pet.get('minjie'),
                        'init_dex': pet.get('init_dex'),
                        'shanghai': pet.get('shanghai'),
                        'damage': pet.get('damage'),
                        'init_damage': pet.get('init_damage'),
                        'init_damage_raw': pet.get('init_damage_raw'),
                        'all_damage': pet.get('all_damage'),
                        'magic_damage': pet.get('magic_damage'),
                        'magic_defense': pet.get('magic_defense'),
                        'lingli': pet.get('lingli'),
                        'fengyin': pet.get('fengyin'),
                        'anti_fengyin': pet.get('anti_fengyin'),
                        'zongshang': pet.get('zongshang'),
                        
                        # ä¿®ç‚¼ç›¸å…³
                        'expt_gongji': pet.get('expt_gongji'),
                        'expt_fangyu': pet.get('expt_fangyu'),
                        'expt_fashu': pet.get('expt_fashu'),
                        'expt_kangfa': pet.get('expt_kangfa'),
                        'max_expt_gongji': pet.get('max_expt_gongji'),
                        'max_expt_fangyu': pet.get('max_expt_fangyu'),
                        'max_expt_fashu': pet.get('max_expt_fashu'),
                        'max_expt_kangfa': pet.get('max_expt_kangfa'),
                        'sum_exp': pet.get('sum_exp'),
                        
                        # å®å®ä¿®ç‚¼
                        'bb_expt_gongji': pet.get('bb_expt_gongji'),
                        'bb_expt_fangyu': pet.get('bb_expt_fangyu'),
                        'bb_expt_fashu': pet.get('bb_expt_fashu'),
                        'bb_expt_kangfa': pet.get('bb_expt_kangfa'),
                        
                        # é™„åŠ å±æ€§
                        'addon_tizhi': pet.get('addon_tizhi'),
                        'addon_liliang': pet.get('addon_liliang'),
                        'addon_naili': pet.get('addon_naili'),
                        'addon_minjie': pet.get('addon_minjie'),
                        'addon_fali': pet.get('addon_fali'),
                        'addon_lingli': pet.get('addon_lingli'),
                        'addon_total': pet.get('addon_total'),
                        'addon_status': pet.get('addon_status'),
                        'addon_skill_chance': pet.get('addon_skill_chance'),
                        'addon_effect_chance': pet.get('addon_effect_chance'),
                        
                        # å®çŸ³ç›¸å…³
                        'gem_level': pet.get('gem_level'),
                        'xiang_qian_level': pet.get('xiang_qian_level'),
                        'gem_value': pet.get('gem_value'),
                        
                        # å¼ºåŒ–ç›¸å…³
                        'jinglian_level': pet.get('jinglian_level'),
                        
                        # ç‰¹æŠ€å’Œå¥—è£…
                        'special_skill': pet.get('special_skill'),
                        'special_effect': pet.get('special_effect'),
                        'suit_skill': pet.get('suit_skill'),
                        'suit_effect': pet.get('suit_effect'),
                        
                        # å…¶ä»–ä¿¡æ¯
                        'collect_num': pet.get('collect_num'),
                        'has_collect': 1 if pet.get('has_collect') else 0,
                        'score': pet.get('score'),
                        'icon_index': pet.get('icon_index'),
                        'icon': pet.get('icon'),
                        'equip_face_img': pet.get('equip_face_img'),
                        'kindid': pet.get('kindid'),
                        'game_channel': pet.get('game_channel'),
                        
                        # è®¢å•ç›¸å…³
                        'game_ordersn': pet.get('game_ordersn'),
                        'whole_game_ordersn': pet.get('whole_game_ordersn'),
                        
                        # è·¨æœç›¸å…³
                        'allow_cross_buy': pet.get('allow_cross_buy'),
                        'cross_server_poundage': pet.get('cross_server_poundage'),
                        'cross_server_poundage_origin': pet.get('cross_server_poundage_origin'),
                        'cross_server_poundage_discount': pet.get('cross_server_poundage_discount'),
                        'cross_server_poundage_discount_label': pet.get('cross_server_poundage_discount_label'),
                        'cross_server_poundage_display_mode': pet.get('cross_server_poundage_display_mode'),
                        'cross_server_activity_conf_discount': pet.get('cross_server_activity_conf_discount'),
                        
                        # æ´»åŠ¨ç›¸å…³
                        'activity_type': pet.get('activity_type'),
                        'joined_seller_activity': 1 if pet.get('joined_seller_activity') else 0,
                        
                        # æ‹†åˆ†ç›¸å…³
                        'is_split_sale': 1 if pet.get('is_split_sale') else 0,
                        'is_split_main_role': 1 if pet.get('is_split_main_role') else 0,
                        'is_split_independent_role': 1 if pet.get('is_split_independent_role') else 0,
                        'is_split_independent_equip': 1 if pet.get('is_split_independent_equip') else 0,
                        'split_equip_sold_happen': 1 if pet.get('split_equip_sold_happen') else 0,
                        'show_split_equip_sold_remind': 1 if pet.get('show_split_equip_sold_remind') else 0,
                        
                        # ä¿æŠ¤ç›¸å…³
                        'is_onsale_protection_period': 1 if pet.get('is_onsale_protection_period') else 0,
                        'onsale_protection_end_time': pet.get('onsale_protection_end_time'),
                        'is_vip_protection': 1 if pet.get('is_vip_protection') else 0,
                        'is_time_lock': 1 if pet.get('is_time_lock') else 0,
                        
                        # æµ‹è¯•æœç›¸å…³
                        'equip_in_test_server': 1 if pet.get('equip_in_test_server') else 0,
                        'buyer_in_test_server': 1 if pet.get('buyer_in_test_server') else 0,
                        'equip_in_allow_take_away_server': 1 if pet.get('equip_in_allow_take_away_server') else 0,
                        
                        # å…¶ä»–æ ‡è¯†
                        'is_weijianding': 1 if pet.get('is_weijianding') else 0,
                        'is_show_alipay_privilege': 1 if pet.get('is_show_alipay_privilege') else 0,
                        'is_seller_redpacket_flag': 1 if pet.get('is_seller_redpacket_flag') else 0,
                        'is_show_expert_desc': pet.get('is_show_expert_desc'),
                        'is_show_special_highlight': 1 if pet.get('is_show_special_highlight') else 0,
                        'is_xyq_game_role_kunpeng_reach_limit': 1 if pet.get('is_xyq_game_role_kunpeng_reach_limit') else 0,
                        
                        # ç‰ˆæœ¬å’Œå­˜å‚¨ç›¸å…³
                        'equip_onsale_version': pet.get('equip_onsale_version'),
                        'storage_type': pet.get('storage_type'),
                        'agent_trans_time': pet.get('agent_trans_time'),
                        
                        # KOLç›¸å…³
                        'kol_article_id': pet.get('kol_article_id'),
                        'kol_share_id': pet.get('kol_share_id'),
                        'kol_share_time': pet.get('kol_share_time'),
                        'kol_share_status': pet.get('kol_share_status'),
                        
                        # æ¨èç›¸å…³
                        'reco_request_id': pet.get('reco_request_id'),
                        'appointed_roleid': pet.get('appointed_roleid'),
                        
                        # å›¢é˜Ÿç›¸å…³
                        'play_team_cnt': pet.get('play_team_cnt'),
                        
                        # éšæœºæŠ½å¥–ç›¸å…³
                        'random_draw_finish_time': pet.get('random_draw_finish_time'),
                        
                        # è¯¦ç»†æè¿°
                        'desc': pet.get('desc'),
                        'large_equip_desc': pet.get('large_equip_desc'),
                        'desc_sumup': pet.get('desc_sumup'),
                        'desc_sumup_short': pet.get('desc_sumup_short'),
                        'diy_desc': pet.get('diy_desc'),
                        'rec_desc': pet.get('rec_desc'),
                        
                        # æœç´¢ç›¸å…³
                        'search_type': pet.get('search_type'),
                        'tag': pet.get('tag'),
                        
                        # JSONæ ¼å¼å­—æ®µ
                        'price_explanation': json.dumps(pet.get('price_explanation'), ensure_ascii=False) if pet.get('price_explanation') else '',
                        'bargain_info': json.dumps(pet.get('bargain_info'), ensure_ascii=False) if pet.get('bargain_info') else '',
                        'diy_desc_pay_info': json.dumps(pet.get('diy_desc_pay_info'), ensure_ascii=False) if pet.get('diy_desc_pay_info') else '',
                        'other_info': pet.get('other_info', ''),
                        'video_info': json.dumps(pet.get('video_info'), ensure_ascii=False) if pet.get('video_info') else '',
                        'agg_added_attrs': json.dumps(pet.get('agg_added_attrs'), ensure_ascii=False) if pet.get('agg_added_attrs') else '',
                        'dynamic_tags': json.dumps(pet.get('dynamic_tags'), ensure_ascii=False) if pet.get('dynamic_tags') else '',
                        'highlight': json.dumps(pet.get('highlight'), ensure_ascii=False) if pet.get('highlight') else '',
                        'tag_key': pet.get('tag_key', ''),
                        
                        # åŸå§‹æ•°æ®
                        'raw_data_json': json.dumps(pet, ensure_ascii=False)
                    }
                    
                    pets.append(pet_data)
                    
                except Exception as e:
                    self.logger.error(f"è§£æå•ä¸ªå® ç‰©æ•°æ®æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            return pets
            
        except Exception as e:
            self.logger.error(f"è§£æJSONPå“åº”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None

    def get_search_params(self, use_browser=False):
        """
        è·å–å® ç‰©æœç´¢å‚æ•°
        - use_browser=True: å¯åŠ¨æµè§ˆå™¨æ‰‹åŠ¨è®¾ç½®å‚æ•°
        - use_browser=False: ä»æœ¬åœ°æ–‡ä»¶æˆ–é»˜è®¤é…ç½®åŠ è½½å‚æ•°
        """
        params_file = 'config/pet_params.json'
        
        # å¼ºåˆ¶æµè§ˆå™¨æ¨¡å¼ï¼šå¦‚æœuse_browserä¸ºTrueï¼Œåˆ™åˆ é™¤æ—§å‚æ•°æ–‡ä»¶
        if use_browser and os.path.exists(params_file):
            self.logger.info(f"å¼ºåˆ¶æµè§ˆå™¨æ¨¡å¼ï¼Œåˆ é™¤æ—§çš„å‚æ•°æ–‡ä»¶: {params_file}")
            os.remove(params_file)

        # ä½¿ç”¨åŒæ­¥çš„å‚æ•°è·å–å‡½æ•°
        return get_pet_search_params_sync(use_browser=use_browser)

    def fetch_page_sync(self, page=1, search_params=None, search_type='overall_search_pet'):
        """
        åŒæ­¥è·å–å•é¡µå® ç‰©æ•°æ®
        """
        if search_params is None:
            search_params = {}
        
        # åŸºç¡€å‚æ•°
        params = {
            'act': search_type,
            'page': page
        }
        
        # åˆå¹¶æœç´¢å‚æ•°
        params.update(search_params)
        
        # æ„å»ºURL
        url = 'https://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?' + urlencode(params)
        self.logger.info(f"æ­£åœ¨è¯·æ±‚URL: {url}")
        
        try:
            response = self.session.get(url, timeout=20)
            response.raise_for_status()
            
            # è§£æJSONPå“åº”
            pets = self.parse_jsonp_response(response.text)
            return pets

        except Exception as e:
            self.logger.error(f"è·å–å® ç‰©æ•°æ®å¤±è´¥ (é¡µç : {page}): {e}")
            return None

    def save_pet_data(self, pets):
        """ä¿å­˜å® ç‰©æ•°æ®åˆ°æ•°æ®åº“"""
        if not pets:
            self.logger.info("æ²¡æœ‰å® ç‰©æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        try:
            # ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•åï¼šsave_pets_batch
            success = self.smart_db.save_pets_batch(pets)
            if success:
                self.logger.info(f"æˆåŠŸä¿å­˜ {len(pets)} æ¡å® ç‰©æ•°æ®åˆ°æ•°æ®åº“")
                return len(pets)
            else:
                self.logger.error("ä¿å­˜å® ç‰©æ•°æ®åˆ°æ•°æ®åº“å¤±è´¥")
                return 0
        except Exception as e:
            self.logger.error(f"ä¿å­˜å® ç‰©æ•°æ®åˆ°æ•°æ®åº“å¤±è´¥: {e}")
            return 0

    async def fetch_page(self, page=1, search_params=None, search_type='overall_search_pet'):
        """
        è·å–å•é¡µå® ç‰©æ•°æ®
        
        Args:
            page: é¡µç 
            search_params: æœç´¢å‚æ•°
            search_type: æœç´¢ç±»å‹
            
        Returns:
            list: è§£æåçš„å® ç‰©æ•°æ®
        """
        try:
            # ç¡®ä¿search_paramsä¸ä¸ºNone
            if search_params is None:
                # æä¾›ä¸€ä¸ªåŸºç¡€çš„é»˜è®¤å€¼
                search_params = { 'level_min': 0, 'level_max': 180, 'server_type': 3, 'evol_skill_mode': 0 }

            # æ„å»ºè¯·æ±‚å‚æ•°
            params = {
                **search_params,  # æ·»åŠ æœç´¢å‚æ•°
                'callback': 'Request.JSONP.request_map.request_0',
                '_': str(int(time.time() * 1000)),
                'act': 'recommd_by_role',
                'page': page,
                'count': 15,
                'search_type': search_type,
                'view_loc': 'overall_search',
            }
            
            # æ„å»ºå®Œæ•´URL
            url = f"{self.base_url}?{urlencode(params)}"
            
            # ä½¿ç”¨Playwrightå‘é€è¯·æ±‚
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context()
                
                # è®¾ç½®cookies
                cookie_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), 'config', 'cookies.txt')
                if os.path.exists(cookie_path):
                    with open(cookie_path, 'r', encoding='utf-8') as f:
                        cookie_str = f.read().strip()
                        cookies = []
                        for cookie in cookie_str.split('; '):
                            if '=' in cookie:
                                name, value = cookie.split('=', 1)
                                cookies.append({
                                    'name': name,
                                    'value': value,
                                    'domain': '.163.com',
                                    'path': '/'
                                })
                        await context.add_cookies(cookies)
                
                page_obj = await context.new_page()
                response = await page_obj.goto(url)
                
                if response:
                    text = await response.text()
                    await browser.close()
                    
                    # è§£æå“åº”
                    parsed_result = self.parse_jsonp_response(text)
                    return parsed_result
                
                await browser.close()
                return None
                
        except Exception as e:
            self.logger.error(f"è·å–å® ç‰©ç¬¬{page}é¡µæ•°æ®æ—¶å‡ºé”™: {e}")
            return None

    async def crawl_all_pages_async(self, max_pages=10, delay_range=None, use_browser=False):
        """
        å¼‚æ­¥çˆ¬å–æ‰€æœ‰å® ç‰©é¡µé¢
        """
        # é¦–å…ˆéªŒè¯Cookieæœ‰æ•ˆæ€§
        self.logger.info("æ­£åœ¨éªŒè¯Cookieæœ‰æ•ˆæ€§...")
        if not verify_cookie_validity():
            self.logger.warning("CookieéªŒè¯å¤±è´¥ï¼Œæ­£åœ¨æ›´æ–°Cookie...")
            from src.utils.cookie_updater import _update_cookies_internal
            if not await _update_cookies_internal():
                self.logger.error("Cookieæ›´æ–°å¤±è´¥ï¼Œæ— æ³•ç»§ç»­çˆ¬å–")
                return
            else:
                self.logger.info("Cookieæ›´æ–°æˆåŠŸï¼Œé‡æ–°è®¾ç½®ä¼šè¯")
                # é‡æ–°è®¾ç½®ä¼šè¯
                self.setup_session()
        else:
            self.logger.info("CookieéªŒè¯é€šè¿‡")

        search_type = 'overall_search_pet'

        self.logger.info(f"ğŸš€ å¼€å§‹å® ç‰©çˆ¬å–ï¼Œæœ€å¤§é¡µæ•°: {max_pages}")

        # è·å–å‚æ•°
        params_file = 'config/pet_params.json'
        
        # å¼ºåˆ¶æµè§ˆå™¨æ¨¡å¼ï¼šå¦‚æœuse_browserä¸ºTrueï¼Œåˆ™åˆ é™¤æ—§å‚æ•°æ–‡ä»¶
        if use_browser and os.path.exists(params_file):
            self.logger.info(f"å¼ºåˆ¶æµè§ˆå™¨æ¨¡å¼ï¼Œåˆ é™¤æ—§çš„å‚æ•°æ–‡ä»¶: {params_file}")
            os.remove(params_file)
            
        search_params = await get_pet_search_params_async(use_browser=use_browser)

        if not search_params:
            self.logger.error(f"æ— æ³•è·å–å® ç‰©çš„æœç´¢å‚æ•°ï¼Œçˆ¬å–ä¸­æ­¢")
            return
            
        self.logger.info(f"ğŸ“Š ä½¿ç”¨æœç´¢å‚æ•°: {len(search_params)} ä¸ª")
        
        total_saved_count = 0
        successful_pages = 0
        
        for page_num in range(1, max_pages + 1):
            try:
                # ç¡®ä¿æ—¥å¿—ç«‹å³è¾“å‡º
                self.logger.info(f"ğŸ“„ æ­£åœ¨çˆ¬å–å® ç‰©ç¬¬ {page_num} é¡µ...")
                # å¼ºåˆ¶åˆ·æ–°æ—¥å¿—ç¼“å†²
                import sys
                sys.stdout.flush()
                
                pets = await self.fetch_page(page_num, search_params, search_type)
                
                if pets is None:
                    self.logger.warning(f"âŒ ç¬¬ {page_num} é¡µæ•°æ®è·å–å¤±è´¥ï¼Œå°è¯•é‡è¯•...")
                    await asyncio.sleep(5) # ç­‰å¾…5ç§’é‡è¯•
                    pets = await self.fetch_page(page_num, search_params, search_type)

                if pets:
                    saved_count = self.save_pet_data(pets)
                    total_saved_count += saved_count
                    successful_pages += 1
                    
                    # æ‰“å°æ¯æ¡å® ç‰©çš„ç®€è¦ä¿¡æ¯
                    for pet in pets:
                        price = pet.get('price_desc', pet.get('price', 'æœªçŸ¥'))
                        pet_name = pet.get('equip_name', 'æœªçŸ¥å® ç‰©')
                        level = pet.get('level', 'æœªçŸ¥')
                        server_name = pet.get('server_name', 'æœªçŸ¥æœåŠ¡å™¨')
                        seller_nickname = pet.get('seller_nickname', 'æœªçŸ¥å–å®¶')
                        desc_sumup_short = pet.get('desc_sumup_short', 'æ— æè¿°')
                        self.logger.info(f"è¯†åˆ«å¹¶ä¿å­˜å® ç‰©: ï¿¥{price} - {pet_name}({level}çº§) - {desc_sumup_short} - {server_name} - {seller_nickname}")
                    
                    self.logger.info(f"âœ… ç¬¬ {page_num} é¡µå®Œæˆï¼Œè·å– {len(pets)} æ¡å® ç‰©ï¼Œä¿å­˜ {saved_count} æ¡")
                else:
                    self.logger.info(f"ğŸ“„ ç¬¬ {page_num} é¡µæ²¡æœ‰æ•°æ®ï¼Œçˆ¬å–ç»“æŸ")
                    break 

                # æ·»åŠ å»¶è¿Ÿ
                if delay_range and page_num < max_pages:  # æœ€åä¸€é¡µä¸éœ€è¦å»¶è¿Ÿ
                    delay = random.uniform(delay_range[0], delay_range[1])
                    self.logger.info(f"â³ ç­‰å¾… {delay:.2f} ç§’åç»§ç»­...")
                    await asyncio.sleep(delay)
                    
            except Exception as e:
                self.logger.error(f"å¤„ç†ç¬¬ {page_num} é¡µæ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
                import traceback
                traceback.print_exc()
                break

        self.logger.info(f"ğŸ‰ å® ç‰©çˆ¬å–å®Œæˆï¼æˆåŠŸé¡µæ•°: {successful_pages}/{max_pages}, æ€»å® ç‰©æ•°: {total_saved_count}")

    def crawl_all_pages(self, max_pages=10, delay_range=None, use_browser=False):
        """
        åŒæ­¥å¯åŠ¨å¼‚æ­¥å® ç‰©çˆ¬è™«çš„å…¥å£
        """
        try:
            asyncio.run(self.crawl_all_pages_async(
                max_pages=max_pages,
                delay_range=delay_range,
                use_browser=use_browser
            ))
        except Exception as e:
            self.logger.error(f"å¯åŠ¨å® ç‰©çˆ¬è™«å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()


def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•"""
    async def run_test():
        spider = CBGPetSpider()
        
        # --- æµ‹è¯•é…ç½® ---
        use_browser_for_test = True   # æ˜¯å¦ä½¿ç”¨æµè§ˆå™¨è·å–å‚æ•°
        max_pages_to_crawl = 2
        # ----------------
        
        print(f"\n--- æ­£åœ¨æµ‹è¯•: å® ç‰©çˆ¬è™« ---")
        
        try:
            await spider.crawl_all_pages_async(
                max_pages=max_pages_to_crawl, 
                delay_range=(1, 3), 
                use_browser=use_browser_for_test
            )
            print(f"--- âœ… å® ç‰©çˆ¬è™«æµ‹è¯•å®Œæˆ ---")
        except Exception as e:
            print(f"--- âŒ å® ç‰©çˆ¬è™«æµ‹è¯•å¤±è´¥: {e} ---")
            import traceback
            traceback.print_exc()

    asyncio.run(run_test())

if __name__ == '__main__':
    main()
