#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¢¦å¹»è¥¿æ¸¸è—å®é˜å¬å”¤å…½çˆ¬è™«æ¨¡å—
ä¸“é—¨ç”¨äºçˆ¬å–å¬å”¤å…½ï¼ˆå® ç‰©ï¼‰æ•°æ®
"""

import os
import sys
import json
import sqlite3
import time
import random
import logging
from datetime import datetime
from urllib.parse import urlencode
import asyncio
from playwright.async_api import async_playwright

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
from src.utils.project_path import get_project_root
project_root = get_project_root()
sys.path.insert(0, project_root)

from src.tools.setup_requests_session import setup_session
from src.utils.smart_db_helper import CBGSmartDB
from src.cbg_config import DB_SCHEMA_CONFIG
from src.tools.search_form_helper import (
    get_pet_search_params_sync,
    get_pet_search_params_async
)
from src.utils.cookie_manager import (
    setup_session_with_cookies, 
    get_playwright_cookies_for_context,
    verify_cookie_validity
)

# å¯¼å…¥å® ç‰©æè¿°è§£æç›¸å…³æ¨¡å—
from src.spider.helper.decode_desc import parse_pet_info


class CBGPetSpider:
    def __init__(self):
        self.session = setup_session()
        self.base_url = 'https://xyq.cbg.163.com/cgi-bin/recommend.py'
        self.output_dir = self.create_output_dir()
        
        # ä¸éœ€è¦åˆå§‹åŒ–è§£æå™¨ï¼Œç›´æ¥ä½¿ç”¨parse_pet_infoå‡½æ•°
        
        # ä½¿ç”¨æŒ‰æœˆåˆ†å‰²çš„æ•°æ®åº“æ–‡ä»¶è·¯å¾„
        from src.utils.project_path import get_data_path
        current_month = datetime.now().strftime('%Y%m')
        
        # å® ç‰©æ•°æ®åº“è·¯å¾„
        db_filename = f"cbg_pets_{current_month}.db"
        self.db_path = os.path.join(get_data_path(), current_month, db_filename)

        # ç¡®ä¿dataç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        
        # åˆå§‹åŒ–æ™ºèƒ½æ•°æ®åº“åŠ©æ‰‹
        self.smart_db = CBGSmartDB(self.db_path)
        
        # é…ç½®ä¸“ç”¨çš„æ—¥å¿—å™¨ï¼Œé¿å…ä¸å…¶ä»–æ¨¡å—å†²çª
        self.logger = self._setup_logger()
        
        # åˆå§‹åŒ–å…¶ä»–ç»„ä»¶
        self.setup_session()
        # å»¶è¿Ÿåˆå§‹åŒ–æ•°æ®åº“ï¼Œé¿å…åœ¨å¯¼å…¥æ—¶åˆ›å»ºæ–‡ä»¶
        # self.init_database()
        self.retry_attempts = 1

    def _setup_logger(self):
        """è®¾ç½®ä¸“ç”¨çš„æ—¥å¿—å™¨"""
        # åˆ›å»ºä¸“ç”¨çš„æ—¥å¿—å™¨
        logger = logging.getLogger(f'CBGPetSpider_{id(self)}')
        logger.setLevel(logging.INFO)
        
        # æ¸…é™¤å¯èƒ½å­˜åœ¨çš„å¤„ç†å™¨ï¼Œé¿å…é‡å¤æ—¥å¿—
        if logger.handlers:
            logger.handlers.clear()
        
        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_file = os.path.join(self.output_dir, f'cbg_pet_spider_{timestamp}.log')
        file_handler = logging.FileHandler(log_file, encoding='utf-8', mode='w')
        file_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºæ ¼å¼å™¨
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        # æ·»åŠ å¤„ç†å™¨åˆ°æ—¥å¿—å™¨
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        
        # é˜²æ­¢æ—¥å¿—ä¼ æ’­åˆ°æ ¹æ—¥å¿—å™¨ï¼Œé¿å…é‡å¤è¾“å‡º
        logger.propagate = False
        
        # æµ‹è¯•æ—¥å¿—å†™å…¥
        logger.info("ğŸ‰ CBGå® ç‰©çˆ¬è™«æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶è·¯å¾„: {log_file}")
        
        return logger

    def create_output_dir(self):
        """åˆ›å»ºè¾“å‡ºç›®å½• - æŒ‰å¹´æœˆåˆ†ç»„"""
        current_date = datetime.now()
        year_month = current_date.strftime('%Y%m')  # 202506
        output_dir = os.path.join('output', year_month)
        os.makedirs(output_dir, exist_ok=True)
        return output_dir
    
    def setup_session(self):
        """è®¾ç½®è¯·æ±‚ä¼šè¯"""
        # ä½¿ç”¨ç»Ÿä¸€çš„Cookieç®¡ç†
        setup_session_with_cookies(
            self.session, 
            referer='https://xyq.cbg.163.com/cgi-bin/equipquery.py?act=show_overall_search_pet',
            logger=self.logger
        )
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“å’Œè¡¨ç»“æ„"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # åªåˆ›å»ºå® ç‰©ç›¸å…³çš„è¡¨
            cursor.execute(DB_SCHEMA_CONFIG['pets'])
            self.logger.debug("å® ç‰©æ•°æ®åº“åˆ›å»ºè¡¨: pets")
            
            conn.commit()
            self.logger.info(f"å® ç‰©æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ: {os.path.basename(self.db_path)}")
            
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–å® ç‰©æ•°æ®åº“å¤±è´¥: {e}")
            raise
        finally:
            conn.close()
    
    def _ensure_database_initialized(self):
        """ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–ï¼Œå¦‚æœæœªåˆå§‹åŒ–åˆ™è¿›è¡Œåˆå§‹åŒ–"""
        try:
            # æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(self.db_path):
                self.logger.info("æ£€æµ‹åˆ°æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¼€å§‹åˆå§‹åŒ–...")
                self.init_database()
            else:
                # æ£€æŸ¥è¡¨ç»“æ„æ˜¯å¦å®Œæ•´
                try:
                    conn = sqlite3.connect(self.db_path)
                    cursor = conn.cursor()
                    cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
                    tables = [row[0] for row in cursor.fetchall()]
                    conn.close()
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„è¡¨
                    if 'pets' not in tables:
                        self.logger.info("æ£€æµ‹åˆ°ç¼ºå¤±çš„è¡¨: petsï¼Œé‡æ–°åˆå§‹åŒ–æ•°æ®åº“...")
                        self.init_database()
                        
                except Exception as e:
                    self.logger.warning(f"æ£€æŸ¥æ•°æ®åº“è¡¨ç»“æ„å¤±è´¥: {e}ï¼Œé‡æ–°åˆå§‹åŒ–...")
                    self.init_database()
                    
        except Exception as e:
            self.logger.error(f"ç¡®ä¿æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def parse_jsonp_response(self, text):
        """è§£æJSONPå“åº”ï¼Œæå–å® ç‰©æ•°æ®"""
        try:
            # æå–JSONéƒ¨åˆ†
            start = text.find('(') + 1
            end = text.rfind(')')
            if start <= 0 or end <= 0:
                self.logger.error("å“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONPæ ¼å¼")
                return None
                
            json_str = text[start:end]
            data = json.loads(json_str)
            
            if not isinstance(data, dict):
                self.logger.error("è§£æJSONPå“åº”å¤±è´¥ï¼šå“åº”ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„JSONå¯¹è±¡")
                return None

            equip_list = data.get('equip_list', [])
            
            if not equip_list:
                self.logger.warning(text)
                self.logger.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å® ç‰©æ•°æ®")
                return []
                
            pets = []
            for pet in equip_list:
                try:
                    # å¼•å…¥decode_desc.pyä¸­çš„decode_descå‡½æ•°ï¼Œè§£ædescå­—æ®µï¼ŒæŠŠè§£æçš„å­—æ®µå­˜å…¥æ•°æ®åº“ 
                    
                    # è·å–å¹¶è§£æå® ç‰©æè¿°å­—æ®µ
                    raw_desc = pet.get('desc', '')
                    parsed_pet_attrs = {}
                    
                    # å¦‚æœå­˜åœ¨descå­—æ®µï¼Œåˆ™è§£æå® ç‰©å±æ€§
                    if raw_desc:
                        try:
                            parsed_pet_attrs = parse_pet_info(raw_desc)
                            self.logger.debug(f"æˆåŠŸè§£æå® ç‰©æè¿°ï¼Œè·å¾—{len(parsed_pet_attrs)}ä¸ªå±æ€§å­—æ®µ")
                        except Exception as e:
                            self.logger.warning(f"è§£æå® ç‰©æè¿°å¤±è´¥: {e}")
                            parsed_pet_attrs = {}
                    
                    # ç›´æ¥ä¿å­˜æ‰€æœ‰åŸå§‹å­—æ®µï¼Œä¸åšè§£æ
                    pet_data = {
                        # åŸºæœ¬å­—æ®µç›´æ¥æ˜ å°„
                        'eid': pet.get('eid'),
                        'equipid': pet.get('equipid'),
                        'equip_sn': pet.get('equip_sn'),
                        'server_name': pet.get('server_name'),
                        'serverid': pet.get('serverid'),
                        'equip_server_sn': pet.get('equip_server_sn'),
                        'seller_nickname': pet.get('seller_nickname'),
                        'seller_roleid': pet.get('seller_roleid'),
                        'area_name': pet.get('area_name'),
                        'equip_name': pet.get('equip_name'),
                        'equip_type': pet.get('equip_type'),
                        'equip_type_name': pet.get('equip_type_name'),
                        'equip_type_desc': pet.get('equip_type_desc'),
                        'level': pet.get('level'),
                        'equip_level': pet.get('equip_level'),
                        'equip_level_desc': pet.get('equip_level_desc'),
                        'level_desc': pet.get('level_desc'),
                        'subtitle': pet.get('subtitle'),
                        'equip_pos': pet.get('equip_pos'),
                        'position': pet.get('position'),
                        'school': pet.get('school'),
                        'role_grade_limit': pet.get('role_grade_limit'),
                        'min_buyer_level': pet.get('min_buyer_level'),
                        'equip_count': pet.get('equip_count'),
                        'price': pet.get('price'),
                        'price_desc': pet.get('price_desc'),
                        'unit_price_desc': pet.get('unit_price_desc'),
                        'min_unit_price': pet.get('min_unit_price'),
                        'accept_bargain': 1 if pet.get('accept_bargain') else 0,
                        'equip_status': pet.get('equip_status'),
                        'equip_status_desc': pet.get('equip_status_desc'),
                        'status_desc': pet.get('status_desc'),
                        'onsale_expire_time_desc': pet.get('onsale_expire_time_desc'),
                        'time_left': pet.get('time_left'),
                        'expire_time': pet.get('expire_time'),
                        'create_time_equip': pet.get('create_time'),
                        'selling_time': pet.get('selling_time'),
                        'selling_time_ago_desc': pet.get('selling_time_ago_desc'),
                        'first_onsale_time': pet.get('first_onsale_time'),
                        'pass_fair_show': pet.get('pass_fair_show'),
                        'fair_show_time': pet.get('fair_show_time'),
                        'fair_show_end_time': pet.get('fair_show_end_time'),
                        'fair_show_end_time_left': pet.get('fair_show_end_time_left'),
                        'fair_show_poundage': pet.get('fair_show_poundage'),
                  
                        # å…¶ä»–ä¿¡æ¯
                        'collect_num': pet.get('collect_num'),
                        'has_collect': 1 if pet.get('has_collect') else 0,
                        'score': pet.get('score'),
                        'icon_index': pet.get('icon_index'),
                        'icon': pet.get('icon'),
                        'equip_face_img': pet.get('equip_face_img'),
                        'kindid': pet.get('kindid'),
                        'game_channel': pet.get('game_channel'),
                        
                        # è®¢å•ç›¸å…³
                        'game_ordersn': pet.get('game_ordersn'),
                        'whole_game_ordersn': pet.get('whole_game_ordersn'),
                        
                        # è·¨æœç›¸å…³
                        'allow_cross_buy': pet.get('allow_cross_buy'),
                        'cross_server_poundage': pet.get('cross_server_poundage'),
                        'cross_server_poundage_origin': pet.get('cross_server_poundage_origin'),
                        'cross_server_poundage_discount': pet.get('cross_server_poundage_discount'),
                        'cross_server_poundage_discount_label': pet.get('cross_server_poundage_discount_label'),
                        'cross_server_poundage_display_mode': pet.get('cross_server_poundage_display_mode'),
                        'cross_server_activity_conf_discount': pet.get('cross_server_activity_conf_discount'),
                        
                        # æ´»åŠ¨ç›¸å…³
                        'activity_type': pet.get('activity_type'),
                        'joined_seller_activity': 1 if pet.get('joined_seller_activity') else 0,
                        
                        # æ‹†åˆ†ç›¸å…³
                        'is_split_sale': 1 if pet.get('is_split_sale') else 0,
                        'is_split_main_role': 1 if pet.get('is_split_main_role') else 0,
                        'is_split_independent_role': 1 if pet.get('is_split_independent_role') else 0,
                        'is_split_independent_equip': 1 if pet.get('is_split_independent_equip') else 0,
                        'split_equip_sold_happen': 1 if pet.get('split_equip_sold_happen') else 0,
                        'show_split_equip_sold_remind': 1 if pet.get('show_split_equip_sold_remind') else 0,
                        
                        # ä¿æŠ¤ç›¸å…³
                        'is_onsale_protection_period': 1 if pet.get('is_onsale_protection_period') else 0,
                        'onsale_protection_end_time': pet.get('onsale_protection_end_time'),
                        'is_vip_protection': 1 if pet.get('is_vip_protection') else 0,
                        'is_time_lock': 1 if pet.get('is_time_lock') else 0,
                        
                        # æµ‹è¯•æœç›¸å…³
                        'equip_in_test_server': 1 if pet.get('equip_in_test_server') else 0,
                        'buyer_in_test_server': 1 if pet.get('buyer_in_test_server') else 0,
                        'equip_in_allow_take_away_server': 1 if pet.get('equip_in_allow_take_away_server') else 0,
                        
                        # å…¶ä»–æ ‡è¯†
                        'is_weijianding': 1 if pet.get('is_weijianding') else 0,
                        'is_show_alipay_privilege': 1 if pet.get('is_show_alipay_privilege') else 0,
                        'is_seller_redpacket_flag': 1 if pet.get('is_seller_redpacket_flag') else 0,
                        'is_show_expert_desc': pet.get('is_show_expert_desc'),
                        'is_show_special_highlight': 1 if pet.get('is_show_special_highlight') else 0,
                        'is_xyq_game_role_kunpeng_reach_limit': 1 if pet.get('is_xyq_game_role_kunpeng_reach_limit') else 0,
                        
                        # ç‰ˆæœ¬å’Œå­˜å‚¨ç›¸å…³
                        'equip_onsale_version': pet.get('equip_onsale_version'),
                        'storage_type': pet.get('storage_type'),
                        'agent_trans_time': pet.get('agent_trans_time'),
                        
                        # KOLç›¸å…³
                        'kol_article_id': pet.get('kol_article_id'),
                        'kol_share_id': pet.get('kol_share_id'),
                        'kol_share_time': pet.get('kol_share_time'),
                        'kol_share_status': pet.get('kol_share_status'),
                        
                        # æ¨èç›¸å…³
                        'reco_request_id': pet.get('reco_request_id'),
                        'appointed_roleid': pet.get('appointed_roleid'),
                        
                        # å›¢é˜Ÿç›¸å…³
                        'play_team_cnt': pet.get('play_team_cnt'),
                        
                        # éšæœºæŠ½å¥–ç›¸å…³
                        'random_draw_finish_time': pet.get('random_draw_finish_time'),
                        
                        # è¯¦ç»†æè¿°
                        'desc': raw_desc,  # ä½¿ç”¨è§£æè·å–çš„åŸå§‹descå­—æ®µ
                        'large_equip_desc': pet.get('large_equip_desc'),
                        'desc_sumup': pet.get('desc_sumup'),
                        'desc_sumup_short': pet.get('desc_sumup_short'),
                        'diy_desc': pet.get('diy_desc'),
                        'rec_desc': pet.get('rec_desc'),
                        
                        # æœç´¢ç›¸å…³
                        'search_type': pet.get('search_type'),
                        'tag': pet.get('tag'),
                        
                        # JSONæ ¼å¼å­—æ®µ
                        'price_explanation': json.dumps(pet.get('price_explanation'), ensure_ascii=False) if pet.get('price_explanation') else '',
                        'bargain_info': json.dumps(pet.get('bargain_info'), ensure_ascii=False) if pet.get('bargain_info') else '',
                        'diy_desc_pay_info': json.dumps(pet.get('diy_desc_pay_info'), ensure_ascii=False) if pet.get('diy_desc_pay_info') else '',
                        'other_info': pet.get('other_info', ''),
                        'video_info': json.dumps(pet.get('video_info'), ensure_ascii=False) if pet.get('video_info') else '',
                        'agg_added_attrs': json.dumps(pet.get('agg_added_attrs'), ensure_ascii=False) if pet.get('agg_added_attrs') else '',
                        'dynamic_tags': json.dumps(pet.get('dynamic_tags'), ensure_ascii=False) if pet.get('dynamic_tags') else '',
                        'highlight': json.dumps(pet.get('highlight'), ensure_ascii=False) if pet.get('highlight') else '',
                        'tag_key': pet.get('tag_key', ''),
                        
                        # åŸå§‹æ•°æ®
                        'raw_data_json': json.dumps(pet, ensure_ascii=False)
                    }
                    
                    # æ·»åŠ è§£æåçš„å® ç‰©å±æ€§å­—æ®µ - ç›´æ¥ä½¿ç”¨åŸå§‹å­—æ®µå
                    if parsed_pet_attrs:
                        # ç›´æ¥å°†æ‰€æœ‰è§£æå‡ºçš„å­—æ®µæ·»åŠ åˆ°pet_dataä¸­
                        for field_name, field_value in parsed_pet_attrs.items():
                            # å¯¹äºå¤æ‚æ•°æ®ç±»å‹ï¼ˆåˆ—è¡¨å’Œå­—å…¸ï¼‰ï¼Œè½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
                            if isinstance(field_value, (list, dict)):
                                pet_data[field_name] = json.dumps(field_value, ensure_ascii=False)
                            else:
                                pet_data[field_name] = field_value
                    
                    pets.append(pet_data)
                    
                except Exception as e:
                    self.logger.error(f"è§£æå•ä¸ªå® ç‰©æ•°æ®æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            return pets
            
        except Exception as e:
            self.logger.error(f"è§£æJSONPå“åº”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None

    def get_search_params(self, use_browser=False):
        """
        è·å–å® ç‰©æœç´¢å‚æ•°
        - use_browser=True: å¯åŠ¨æµè§ˆå™¨æ‰‹åŠ¨è®¾ç½®å‚æ•°
        - use_browser=False: ä»æœ¬åœ°æ–‡ä»¶æˆ–é»˜è®¤é…ç½®åŠ è½½å‚æ•°
        """
        params_file = 'config/equip_params_pet.json'
        
        # å¼ºåˆ¶æµè§ˆå™¨æ¨¡å¼ï¼šå¦‚æœuse_browserä¸ºTrueï¼Œåˆ™åˆ é™¤æ—§å‚æ•°æ–‡ä»¶
        if use_browser and os.path.exists(params_file):
            self.logger.info(f"å¼ºåˆ¶æµè§ˆå™¨æ¨¡å¼ï¼Œåˆ é™¤æ—§çš„å‚æ•°æ–‡ä»¶: {params_file}")
            os.remove(params_file)

        # ä½¿ç”¨åŒæ­¥çš„å‚æ•°è·å–å‡½æ•°
        return get_pet_search_params_sync(use_browser=use_browser)

    def fetch_page_sync(self, page=1, search_params=None, search_type='overall_search_pet'):
        """
        åŒæ­¥è·å–å•é¡µå® ç‰©æ•°æ®
        """
        if search_params is None:
            search_params = {}
        
        # åŸºç¡€å‚æ•°
        params = {
            'act': search_type,
            'page': page
        }
        
        # åˆå¹¶æœç´¢å‚æ•°
        params.update(search_params)
        
        # æ„å»ºURL
        url = 'https://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?' + urlencode(params)
        self.logger.info(f"æ­£åœ¨è¯·æ±‚URL: {url}")
        
        try:
            response = self.session.get(url, timeout=20)
            response.raise_for_status()
            
            # è§£æJSONPå“åº”
            pets = self.parse_jsonp_response(response.text)
            return pets

        except Exception as e:
            self.logger.error(f"è·å–å® ç‰©æ•°æ®å¤±è´¥ (é¡µç : {page}): {e}")
            return None

    def save_pet_data(self, pets):
        """ä¿å­˜å® ç‰©æ•°æ®åˆ°æ•°æ®åº“"""
        if not pets:
            self.logger.info("æ²¡æœ‰å® ç‰©æ•°æ®éœ€è¦ä¿å­˜")
            return 0
        
        # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
        self._ensure_database_initialized()
        
        try:
            self.logger.info(f"å¼€å§‹ä¿å­˜ {len(pets)} æ¡å® ç‰©æ•°æ®åˆ°æ•°æ®åº“")
            # ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•åï¼šsave_pets_batch
            success = self.smart_db.save_pets_batch(pets)
            if success:
                self.logger.info(f"æˆåŠŸä¿å­˜ {len(pets)} æ¡å® ç‰©æ•°æ®åˆ°æ•°æ®åº“")
                return len(pets)
            else:
                self.logger.error("ä¿å­˜å® ç‰©æ•°æ®åˆ°æ•°æ®åº“å¤±è´¥")
                return 0
        except Exception as e:
            self.logger.error(f"ä¿å­˜å® ç‰©æ•°æ®åˆ°æ•°æ®åº“å¤±è´¥: {e}")
            return 0

    async def fetch_page(self, page=1, search_params=None, search_type='overall_search_pet'):
        """
        è·å–å•é¡µå® ç‰©æ•°æ®
        
        Args:
            page: é¡µç 
            search_params: æœç´¢å‚æ•°
            search_type: æœç´¢ç±»å‹
            
        Returns:
            list: è§£æåçš„å® ç‰©æ•°æ®
        """
        try:
            # ç¡®ä¿search_paramsä¸ä¸ºNone
            if search_params is None:
                # æä¾›ä¸€ä¸ªåŸºç¡€çš„é»˜è®¤å€¼
                search_params = { 'level_min': 0, 'level_max': 180, 'server_type': 3, 'evol_skill_mode': 0 }

            # æ„å»ºè¯·æ±‚å‚æ•°
            params = {
                **search_params,  # æ·»åŠ æœç´¢å‚æ•°
                'callback': 'Request.JSONP.request_map.request_0',
                '_': str(int(time.time() * 1000)),
                'act': 'recommd_by_role',
                'page': page,
                'count': 15,
                'search_type': search_type,
                'view_loc': 'overall_search',
            }
            
            # æ„å»ºå®Œæ•´URL
            url = f"{self.base_url}?{urlencode(params)}"
            
            # ä½¿ç”¨Playwrightå‘é€è¯·æ±‚
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context()
                
                # è®¾ç½®cookies
                cookies = get_playwright_cookies_for_context(self.logger)
                if cookies:
                    await context.add_cookies(cookies)
                
                page_obj = await context.new_page()
                response = await page_obj.goto(url)
                
                if response:
                    text = await response.text()
                    await browser.close()
                    
                    # è§£æå“åº”
                    parsed_result = self.parse_jsonp_response(text)
                    return parsed_result
                
                await browser.close()
                return None
                
        except Exception as e:
            self.logger.error(f"è·å–å® ç‰©ç¬¬{page}é¡µæ•°æ®æ—¶å‡ºé”™: {e}")
            return None

    async def crawl_all_pages_async(self, max_pages=10, delay_range=None, use_browser=False, cached_params=None):
        """
        å¼‚æ­¥çˆ¬å–æ‰€æœ‰å® ç‰©é¡µé¢
        """
        # é¦–å…ˆéªŒè¯Cookieæœ‰æ•ˆæ€§
        self.logger.info("æ­£åœ¨éªŒè¯Cookieæœ‰æ•ˆæ€§...")
        from src.utils.cookie_manager import verify_cookie_validity_async
        if not await verify_cookie_validity_async(self.logger):
            self.logger.warning("CookieéªŒè¯å¤±è´¥ï¼Œæ­£åœ¨æ›´æ–°Cookie...")
            from src.utils.cookie_manager import _update_cookies_internal
            if not await _update_cookies_internal():
                self.logger.error("Cookieæ›´æ–°å¤±è´¥ï¼Œæ— æ³•ç»§ç»­çˆ¬å–")
                return
            else:
                self.logger.info("Cookieæ›´æ–°æˆåŠŸï¼Œé‡æ–°è®¾ç½®ä¼šè¯")
                # é‡æ–°è®¾ç½®ä¼šè¯
                self.setup_session()
        else:
            self.logger.info("CookieéªŒè¯é€šè¿‡")

        search_type = 'overall_search_pet'

        self.logger.info(f"ğŸš€ å¼€å§‹å® ç‰©çˆ¬å–ï¼Œæœ€å¤§é¡µæ•°: {max_pages}")

        # è·å–å‚æ•°
        params_file = 'config/pet_params.json'
        
        # å¼ºåˆ¶æµè§ˆå™¨æ¨¡å¼ï¼šå¦‚æœuse_browserä¸ºTrueï¼Œåˆ™åˆ é™¤æ—§å‚æ•°æ–‡ä»¶
        if use_browser and os.path.exists(params_file):
            self.logger.info(f"å¼ºåˆ¶æµè§ˆå™¨æ¨¡å¼ï¼Œåˆ é™¤æ—§çš„å‚æ•°æ–‡ä»¶: {params_file}")
            os.remove(params_file)
        
        # ä½¿ç”¨ä¼ å…¥çš„ç¼“å­˜å‚æ•°æˆ–è·å–æ–°å‚æ•°
        if cached_params and not use_browser:
            search_params = cached_params
            self.logger.info(f"ğŸ“Š ä½¿ç”¨ä¼ å…¥çš„ç¼“å­˜å‚æ•°: {len(search_params)} ä¸ª")
        else:
            search_params = await get_pet_search_params_async(use_browser=use_browser)
            if search_params:
                self.logger.info(f"ğŸ“Š ä½¿ç”¨æœç´¢å‚æ•°: {search_params}")

        if not search_params:
            self.logger.error(f"æ— æ³•è·å–å® ç‰©çš„æœç´¢å‚æ•°ï¼Œçˆ¬å–ä¸­æ­¢")
            return
            
        total_saved_count = 0
        successful_pages = 0
        
        for page_num in range(1, max_pages + 1):
            try:
                # ç¡®ä¿æ—¥å¿—ç«‹å³è¾“å‡º
                self.logger.info(f"ğŸ“„ æ­£åœ¨çˆ¬å–å® ç‰©ç¬¬ {page_num} é¡µ...")
                # å¼ºåˆ¶åˆ·æ–°æ—¥å¿—ç¼“å†²
                import sys
                sys.stdout.flush()
                
                pets = await self.fetch_page(page_num, search_params, search_type)
                
                if pets is None:
                    self.logger.warning(f"âŒ ç¬¬ {page_num} é¡µæ•°æ®è·å–å¤±è´¥ï¼Œå°è¯•é‡è¯•...")
                    await asyncio.sleep(5) # ç­‰å¾…5ç§’é‡è¯•
                    pets = await self.fetch_page(page_num, search_params, search_type)

                if pets:
                    saved_count = self.save_pet_data(pets)
                    total_saved_count += saved_count
                    successful_pages += 1
                    
                    # æ‰“å°æ¯æ¡å® ç‰©çš„ç®€è¦ä¿¡æ¯
                    for pet in pets:
                        price = pet.get('price_desc', pet.get('price', 'æœªçŸ¥'))
                        pet_name = pet.get('equip_name', 'æœªçŸ¥å® ç‰©')
                        level = pet.get('level', 'æœªçŸ¥')
                        server_name = pet.get('server_name', 'æœªçŸ¥æœåŠ¡å™¨')
                        seller_nickname = pet.get('seller_nickname', 'æœªçŸ¥å–å®¶')
                        desc_sumup_short = pet.get('desc_sumup_short', 'æ— æè¿°')
                        self.logger.info(f" ï¿¥{price} - {pet_name}({level}çº§) - {desc_sumup_short} - {server_name} - {seller_nickname}")
                    
                    self.logger.info(f"âœ… ç¬¬ {page_num} é¡µå®Œæˆï¼Œè·å– {len(pets)} æ¡å® ç‰©ï¼Œä¿å­˜ {saved_count} æ¡")
                else:
                    self.logger.info(f"ğŸ“„ ç¬¬ {page_num} é¡µæ²¡æœ‰æ•°æ®ï¼Œçˆ¬å–ç»“æŸ")
                    break 

                # æ·»åŠ å»¶è¿Ÿ
                if delay_range and page_num < max_pages:  # æœ€åä¸€é¡µä¸éœ€è¦å»¶è¿Ÿ
                    delay = random.uniform(delay_range[0], delay_range[1])
                    self.logger.info(f"â³ ç­‰å¾… {delay:.2f} ç§’åç»§ç»­...")
                    await asyncio.sleep(delay)
                    
            except Exception as e:
                self.logger.error(f"å¤„ç†ç¬¬ {page_num} é¡µæ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
                import traceback
                traceback.print_exc()
                break

        self.logger.info(f"ğŸ‰ å® ç‰©çˆ¬å–å®Œæˆï¼æˆåŠŸé¡µæ•°: {successful_pages}/{max_pages}, æ€»å® ç‰©æ•°: {total_saved_count}")

    def crawl_all_pages(self, max_pages=10, delay_range=None, use_browser=False, cached_params=None):
        """
        åŒæ­¥å¯åŠ¨å¼‚æ­¥å® ç‰©çˆ¬è™«çš„å…¥å£
        """
        try:
            asyncio.run(self.crawl_all_pages_async(
                max_pages=max_pages,
                delay_range=delay_range,
                use_browser=use_browser,
                cached_params=cached_params
            ))
        except Exception as e:
            self.logger.error(f"å¯åŠ¨å® ç‰©çˆ¬è™«å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()


def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•"""
    async def run_test():
        spider = CBGPetSpider()
        
        # --- æµ‹è¯•é…ç½® ---
        use_browser_for_test = True   # æ˜¯å¦ä½¿ç”¨æµè§ˆå™¨è·å–å‚æ•°
        max_pages_to_crawl = 2
        # ----------------
        
        print(f"\n--- æ­£åœ¨æµ‹è¯•: å® ç‰©çˆ¬è™« ---")
        
        try:
            await spider.crawl_all_pages_async(
                max_pages=max_pages_to_crawl, 
                delay_range=(1, 3), 
                use_browser=use_browser_for_test
            )
            print(f"--- âœ… å® ç‰©çˆ¬è™«æµ‹è¯•å®Œæˆ ---")
        except Exception as e:
            print(f"--- âŒ å® ç‰©çˆ¬è™«æµ‹è¯•å¤±è´¥: {e} ---")
            import traceback
            traceback.print_exc()

    asyncio.run(run_test())

if __name__ == '__main__':
    main()
