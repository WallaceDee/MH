#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ•´åˆä»£ç†IPè½®æ¢çš„å®Œæ•´CBGçˆ¬è™«ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•åœ¨å®é™…çˆ¬è™«ä¸­ä½¿ç”¨ä»£ç†IPè½®æ¢ç³»ç»Ÿ
"""

import sys
import os
import time
import json
import sqlite3
from datetime import datetime
import pandas as pd

# å¯¼å…¥æˆ‘ä»¬çš„ä»£ç†ç³»ç»Ÿ
from proxy_rotation_system import ProxyRotationManager, CBGProxyCrawler
from proxy_source_manager import ProxySourceManager

class EnhancedCBGCrawler:
    """å¢å¼ºç‰ˆCBGçˆ¬è™«ï¼Œé›†æˆä»£ç†IPè½®æ¢"""
    
    def __init__(self, use_proxy=True, auto_fetch_proxies=False):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆCBGçˆ¬è™«
        
        Args:
            use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†IP
            auto_fetch_proxies: æ˜¯å¦è‡ªåŠ¨è·å–å…è´¹ä»£ç†IP
        """
        self.use_proxy = use_proxy
        self.proxy_manager = None
        self.crawler = None
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        self.output_dir = f"output/{timestamp}"
        os.makedirs(self.output_dir, exist_ok=True)
        
        # åˆå§‹åŒ–ä»£ç†ç³»ç»Ÿ
        if use_proxy:
            self._setup_proxy_system(auto_fetch_proxies)
        
        print(f"ğŸš€ CBGçˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        if use_proxy:
            print(f"ğŸŒ ä»£ç†æ¨¡å¼: {'å·²å¯ç”¨' if self.proxy_manager else 'å¯ç”¨å¤±è´¥'}")
        else:
            print("ğŸ”— ç›´è¿æ¨¡å¼: å·²å¯ç”¨")
    
    def _setup_proxy_system(self, auto_fetch=False):
        """è®¾ç½®ä»£ç†ç³»ç»Ÿ"""
        try:
            # å¦‚æœéœ€è¦è‡ªåŠ¨è·å–ä»£ç†
            if auto_fetch:
                print("ğŸ”„ è‡ªåŠ¨è·å–å…è´¹ä»£ç†IP...")
                source_manager = ProxySourceManager()
                proxies = source_manager.get_proxies_from_all_sources()
                
                if proxies:
                    source_manager.save_proxies_to_file(proxies)
                    print(f"âœ… è·å–åˆ° {len(proxies)} ä¸ªä»£ç†IP")
                else:
                    print("âŒ æœªèƒ½è·å–åˆ°å…è´¹ä»£ç†IPï¼Œå°†å°è¯•ä½¿ç”¨æœ¬åœ°ä»£ç†æ–‡ä»¶")
            
            # åˆå§‹åŒ–ä»£ç†ç®¡ç†å™¨
            self.proxy_manager = ProxyRotationManager()
            
            if not self.proxy_manager.proxy_pool:
                print("âŒ æ²¡æœ‰å¯ç”¨çš„ä»£ç†IPï¼Œåˆ‡æ¢åˆ°ç›´è¿æ¨¡å¼")
                self.use_proxy = False
                return
            
            # æµ‹è¯•ä»£ç†å¯ç”¨æ€§
            print("ğŸ” æµ‹è¯•ä»£ç†IPå¯ç”¨æ€§...")
            working_proxies, failed_proxies = self.proxy_manager.test_all_proxies()
            
            if not working_proxies:
                print("âŒ æ‰€æœ‰ä»£ç†IPéƒ½ä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°ç›´è¿æ¨¡å¼")
                self.use_proxy = False
                return
            
            print(f"âœ… å‘ç° {len(working_proxies)} ä¸ªå¯ç”¨ä»£ç†")
            
            # åˆ›å»ºä»£ç†çˆ¬è™«
            self.crawler = CBGProxyCrawler(self.proxy_manager)
            
        except Exception as e:
            print(f"âŒ è®¾ç½®ä»£ç†ç³»ç»Ÿå¤±è´¥: {e}")
            self.use_proxy = False
    
    def create_database(self):
        """åˆ›å»ºæ•°æ®åº“å’Œè¡¨ç»“æ„"""
        db_path = os.path.join(self.output_dir, "cbg_data.db")
        
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºè§’è‰²è¡¨ï¼ˆç®€åŒ–ç‰ˆï¼ŒåŒ…å«ä¸»è¦å­—æ®µï¼‰
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS characters (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            equip_id TEXT UNIQUE,
            character_name TEXT,
            price REAL,
            server_name TEXT,
            level INTEGER,
            profession TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            proxy_used TEXT,
            response_time REAL
        )
        """)
        
        conn.commit()
        conn.close()
        
        return db_path
    
    def fetch_page_data(self, page):
        """è·å–å•é¡µæ•°æ®"""
        if self.use_proxy and self.crawler:
            # ä½¿ç”¨ä»£ç†çˆ¬å–
            result = self.crawler.fetch_page_with_proxy_retry(page)
            return result
        else:
            # ç›´è¿çˆ¬å–ï¼ˆè¿™é‡Œå¯ä»¥é›†æˆä½ åŸæœ‰çš„çˆ¬è™«é€»è¾‘ï¼‰
            print(f"âš¡ ç›´è¿æ¨¡å¼è·å–ç¬¬{page}é¡µæ•°æ®")
            time.sleep(2)  # æ¨¡æ‹Ÿè¯·æ±‚å»¶æ—¶
            return None  # è¿™é‡Œåº”è¯¥è¿”å›å®é™…çš„æ•°æ®
    
    def parse_character_data(self, raw_data, proxy_info=None):
        """è§£æè§’è‰²æ•°æ®"""
        characters = []
        
        try:
            if not raw_data or 'result' not in raw_data:
                return characters
            
            for item in raw_data['result']:
                character = {
                    'equip_id': item.get('equip_id'),
                    'character_name': item.get('equip_name'),
                    'price': float(item.get('price', 0)) / 100,  # åˆ†è½¬å…ƒ
                    'server_name': item.get('server_name'),
                    'level': int(item.get('level', 0)),
                    'profession': item.get('school_name'),
                    'proxy_used': f"{proxy_info['ip']}:{proxy_info['port']}" if proxy_info else 'direct',
                    'response_time': proxy_info.get('response_time', 0) if proxy_info else 0
                }
                characters.append(character)
        
        except Exception as e:
            print(f"âŒ è§£æè§’è‰²æ•°æ®å¤±è´¥: {e}")
        
        return characters
    
    def save_to_database(self, characters, db_path):
        """ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“"""
        if not characters:
            return
        
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            for char in characters:
                cursor.execute("""
                INSERT OR REPLACE INTO characters 
                (equip_id, character_name, price, server_name, level, profession, proxy_used, response_time)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    char['equip_id'], char['character_name'], char['price'],
                    char['server_name'], char['level'], char['profession'],
                    char['proxy_used'], char['response_time']
                ))
            
            conn.commit()
            conn.close()
            
            print(f"ğŸ’¾ å·²ä¿å­˜ {len(characters)} æ¡è§’è‰²æ•°æ®")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ•°æ®å¤±è´¥: {e}")
    
    def crawl_multiple_pages(self, start_page=1, end_page=10):
        """çˆ¬å–å¤šé¡µæ•°æ®"""
        print(f"ğŸš€ å¼€å§‹çˆ¬å–ç¬¬{start_page}-{end_page}é¡µæ•°æ®")
        
        # åˆ›å»ºæ•°æ®åº“
        db_path = self.create_database()
        
        total_characters = 0
        successful_pages = 0
        
        for page in range(start_page, end_page + 1):
            try:
                print(f"\nğŸ“„ æ­£åœ¨å¤„ç†ç¬¬{page}é¡µ...")
                
                # è·å–é¡µé¢æ•°æ®
                start_time = time.time()
                raw_data = self.fetch_page_data(page)
                fetch_time = time.time() - start_time
                
                if raw_data:
                    # è·å–å½“å‰ä½¿ç”¨çš„ä»£ç†ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                    proxy_info = None
                    if self.use_proxy and self.proxy_manager:
                        # è¿™é‡Œå¯ä»¥è·å–æœ€åä½¿ç”¨çš„ä»£ç†ä¿¡æ¯
                        active_proxies = [p for p in self.proxy_manager.proxy_pool if p['status'] == 'active']
                        if active_proxies:
                            proxy_info = active_proxies[0]  # ç®€åŒ–å¤„ç†
                    
                    # è§£ææ•°æ®
                    characters = self.parse_character_data(raw_data, proxy_info)
                    
                    # ä¿å­˜åˆ°æ•°æ®åº“
                    self.save_to_database(characters, db_path)
                    
                    total_characters += len(characters)
                    successful_pages += 1
                    
                    print(f"âœ… ç¬¬{page}é¡µå®Œæˆï¼Œè·å–{len(characters)}æ¡æ•°æ®ï¼Œè€—æ—¶{fetch_time:.2f}ç§’")
                
                else:
                    print(f"âŒ ç¬¬{page}é¡µè·å–å¤±è´¥")
                
                # é¡µé¢é—´å»¶æ—¶
                if page < end_page:
                    delay = 3 if not self.use_proxy else 1
                    print(f"â° ç­‰å¾…{delay}ç§’...")
                    time.sleep(delay)
                
            except Exception as e:
                print(f"âŒ ç¬¬{page}é¡µå¤„ç†å¼‚å¸¸: {e}")
                continue
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        self._show_crawl_summary(successful_pages, end_page - start_page + 1, total_characters, db_path)
        
        # å¦‚æœä½¿ç”¨äº†ä»£ç†ï¼Œæ˜¾ç¤ºä»£ç†ç»Ÿè®¡
        if self.use_proxy and self.proxy_manager:
            print("\nğŸ“Š ä»£ç†ä½¿ç”¨ç»Ÿè®¡:")
            self.proxy_manager.show_proxy_status()
        
        return db_path
    
    def _show_crawl_summary(self, successful_pages, total_pages, total_characters, db_path):
        """æ˜¾ç¤ºçˆ¬å–æ±‡æ€»"""
        print("\n" + "="*60)
        print("ğŸ‰ çˆ¬å–ä»»åŠ¡å®Œæˆï¼")
        print(f"ğŸ“Š æˆåŠŸé¡µé¢: {successful_pages}/{total_pages}")
        print(f"ğŸ‘¥ æ€»è§’è‰²æ•°: {total_characters}")
        print(f"ğŸ’¾ æ•°æ®åº“æ–‡ä»¶: {db_path}")
        
        if total_characters > 0:
            success_rate = (successful_pages / total_pages) * 100
            print(f"âœ… æˆåŠŸç‡: {success_rate:.1f}%")
            
            # å¿«é€Ÿç»Ÿè®¡
            try:
                conn = sqlite3.connect(db_path)
                df = pd.read_sql_query("SELECT * FROM characters", conn)
                conn.close()
                
                if len(df) > 0:
                    print(f"ğŸ’° ä»·æ ¼èŒƒå›´: {df['price'].min():.2f}å…ƒ - {df['price'].max():.2f}å…ƒ")
                    print(f"ğŸ“ˆ å¹³å‡ä»·æ ¼: {df['price'].mean():.2f}å…ƒ")
                    
                    if self.use_proxy:
                        proxy_stats = df['proxy_used'].value_counts()
                        print(f"ğŸŒ ä½¿ç”¨ä»£ç†æ•°: {len(proxy_stats)}")
            
            except Exception as e:
                print(f"âŒ ç»Ÿè®¡ä¿¡æ¯ç”Ÿæˆå¤±è´¥: {e}")
    
    def export_to_excel(self, db_path):
        """å¯¼å‡ºExcelæ–‡ä»¶"""
        try:
            conn = sqlite3.connect(db_path)
            df = pd.read_sql_query("SELECT * FROM characters", conn)
            conn.close()
            
            if len(df) == 0:
                print("âŒ æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")
                return
            
            # Excelæ–‡ä»¶è·¯å¾„
            excel_path = os.path.join(self.output_dir, "cbg_characters.xlsx")
            
            # å¯¼å‡ºExcel
            with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
                df.to_excel(writer, sheet_name='è§’è‰²æ•°æ®', index=False)
            
            print(f"ğŸ“Š Excelæ–‡ä»¶å·²å¯¼å‡º: {excel_path}")
            return excel_path
            
        except Exception as e:
            print(f"âŒ Excelå¯¼å‡ºå¤±è´¥: {e}")
            return None

def demo_proxy_crawling():
    """æ¼”ç¤ºä»£ç†IPè½®æ¢çˆ¬è™«"""
    print("ğŸ¯ CBGä»£ç†IPè½®æ¢çˆ¬è™«æ¼”ç¤º")
    print("="*60)
    
    # é€‰æ‹©æ¨¡å¼
    print("\nè¯·é€‰æ‹©è¿è¡Œæ¨¡å¼:")
    print("1. ä½¿ç”¨ç°æœ‰ä»£ç†æ–‡ä»¶")
    print("2. è‡ªåŠ¨è·å–å…è´¹ä»£ç†")
    print("3. ç›´è¿æ¨¡å¼ï¼ˆæ— ä»£ç†ï¼‰")
    
    try:
        choice = input("è¯·è¾“å…¥é€‰æ‹© (1-3): ").strip()
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
        return
    
    # åˆå§‹åŒ–çˆ¬è™«
    if choice == "1":
        crawler = EnhancedCBGCrawler(use_proxy=True, auto_fetch_proxies=False)
    elif choice == "2":
        crawler = EnhancedCBGCrawler(use_proxy=True, auto_fetch_proxies=True)
    else:
        crawler = EnhancedCBGCrawler(use_proxy=False)
    
    # çˆ¬å–é¡µé¢æ•°
    try:
        pages = int(input("è¯·è¾“å…¥è¦çˆ¬å–çš„é¡µé¢æ•° (é»˜è®¤5é¡µ): ") or "5")
    except (ValueError, KeyboardInterrupt):
        pages = 5
    
    print(f"\nğŸš€ å¼€å§‹çˆ¬å–{pages}é¡µæ•°æ®...")
    
    # æ‰§è¡Œçˆ¬å–
    db_path = crawler.crawl_multiple_pages(1, pages)
    
    # å¯¼å‡ºExcel
    excel_path = crawler.export_to_excel(db_path)
    
    print("\nğŸ‰ ä»»åŠ¡å®Œæˆï¼")
    if excel_path:
        print(f"ğŸ“Š å¯ä»¥æŸ¥çœ‹Excelæ–‡ä»¶: {excel_path}")

if __name__ == "__main__":
    demo_proxy_crawling() 