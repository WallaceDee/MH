#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç³»ç»ŸAPIè“å›¾
"""

from flask import Blueprint, send_file, request, jsonify, Response, current_app
from ....controllers.system_controller import SystemController
from ....utils.response import success_response, error_response
import os
import json
import logging
import time

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
import sys
import os
current_dir = os.path.dirname(os.path.abspath(__file__))
src_path = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))  # å‘ä¸Šä¸‰çº§åˆ°srcç›®å½•
if src_path not in sys.path:
    sys.path.insert(0, src_path)

try:
    from src.utils.project_path import get_relative_path, get_config_path
except ImportError:
    # å¦‚æœæ— æ³•å¯¼å…¥ï¼Œç›´æ¥ä½¿ç”¨ç›¸å¯¹è·¯å¾„è®¡ç®—
    def get_relative_path(path):
        return os.path.join(src_path, path)
    
    def get_config_path():
        return os.path.join(src_path, 'config')

# è·å–logger
logger = logging.getLogger(__name__)

system_bp = Blueprint('system', __name__)
controller = SystemController()


@system_bp.route('/info', methods=['GET'])
def get_system_info():
    """è·å–ç³»ç»Ÿä¿¡æ¯"""
    try:
        info = controller.get_system_info()
        return success_response(data=info)
    except Exception as e:
        return error_response(f"è·å–ç³»ç»Ÿä¿¡æ¯å¤±è´¥: {str(e)}")


@system_bp.route('/files', methods=['GET'])
def list_files():
    """åˆ—å‡ºè¾“å‡ºæ–‡ä»¶"""
    try:
        files = controller.list_output_files()
        return success_response(data=files)
    except Exception as e:
        return error_response(f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")


@system_bp.route('/files/<filename>/download', methods=['GET'])
def download_file(filename):
    """ä¸‹è½½æ–‡ä»¶"""
    try:
        file_path = controller.get_file_path(filename)
        return send_file(file_path, as_attachment=True)
    except FileNotFoundError:
        return error_response("æ–‡ä»¶ä¸å­˜åœ¨", code=404, http_code=404)
    except Exception as e:
        return error_response(f"ä¸‹è½½å¤±è´¥: {str(e)}", http_code=500)


@system_bp.route('/config-file/<filename>', methods=['GET'])
def get_config_file(filename):
    """è·å–é…ç½®æ–‡ä»¶å†…å®¹"""
    try:
        # ä½¿ç”¨é¡¹ç›®è·¯å¾„å·¥å…·è·å–é…ç½®æ–‡ä»¶è·¯å¾„
        config_path = get_relative_path(f'src/constant/{filename}')
        
        if not os.path.exists(config_path):
            return "é…ç½®æ–‡ä»¶ä¸å­˜åœ¨", 404
        
        with open(config_path, 'r', encoding='utf-8') as f:
            js_content = f.read()
        # å°†jsoncéœ€è¦å»é™¤æ³¨é‡Š
        # ç®€å•å¤„ç†JSONCæ³¨é‡Šï¼ˆç§»é™¤//æ³¨é‡Šï¼‰
        lines = js_content.split('\n')
        cleaned_lines = []
        for line in lines:
            # ç§»é™¤è¡Œå†…æ³¨é‡Š
            if '//' in line:
                line = line[:line.index('//')]
            cleaned_lines.append(line)
        
        js_content = '\n'.join(cleaned_lines)
        # ç›´æ¥è¿”å›æ–‡ä»¶å†…å®¹
        return Response(js_content, mimetype='text/plain')
        
    except Exception as e:
        return f"è·å–é…ç½®å¤±è´¥: {str(e)}", 500


@system_bp.route('/config/search-params', methods=['GET'])
def get_search_params():
    """
    è·å–æœç´¢å‚æ•°é…ç½®
    
    Returns:
        JSON: åŒ…å«æ‰€æœ‰æœç´¢å‚æ•°é…ç½®çš„å“åº”
    """
    try:
        config_dir = get_config_path()
        
        # å®šä¹‰éœ€è¦è¯»å–çš„é…ç½®æ–‡ä»¶
        config_files = {
            'equip_params_normal': 'equip_params_normal.json',
            'equip_params_lingshi': 'equip_params_lingshi.json',
            'equip_params_pet': 'equip_params_pet.json',
            'equip_params_pet_equip': 'equip_params_pet_equip.json'
        }
        
        result = {}
        
        for key, filename in config_files.items():
            file_path = os.path.join(config_dir, filename)
            
            if os.path.exists(file_path):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = json.load(f)
                        result[key] = content
                except json.JSONDecodeError as e:
                    logger.error(f"è§£æé…ç½®æ–‡ä»¶ {filename} å¤±è´¥: {e}")
                    result[key] = {"error": f"JSONè§£æå¤±è´¥: {str(e)}"}
                except Exception as e:
                    logger.error(f"è¯»å–é…ç½®æ–‡ä»¶ {filename} å¤±è´¥: {e}")
                    result[key] = {"error": f"æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}"}
            else:
                logger.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
                result[key] = {"error": "æ–‡ä»¶ä¸å­˜åœ¨"}
        
        # å¤„ç†è§’è‰²æœç´¢å‚æ•°ï¼ˆä»search_paramsç›®å½•è¯»å–ï¼‰
        role_params = {}
        if os.path.exists(config_dir):
            try:
                # è¯»å–109ç­‰çº§çš„é…ç½®ä½œä¸ºé»˜è®¤è§’è‰²é…ç½®
                role_file = os.path.join(config_dir, '109.json')
                if os.path.exists(role_file):
                    with open(role_file, 'r', encoding='utf-8') as f:
                        role_params = json.load(f)
                else:
                    # å¦‚æœæ²¡æœ‰109.jsonï¼Œè¯»å–ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ–‡ä»¶
                    for filename in os.listdir(config_dir):
                        if filename.endswith('.json'):
                            role_file = os.path.join(config_dir, filename)
                            with open(role_file, 'r', encoding='utf-8') as f:
                                role_params = json.load(f)
                            break
            except Exception as e:
                logger.error(f"è¯»å–è§’è‰²æœç´¢å‚æ•°å¤±è´¥: {e}")
        
        # å¤„ç†å¬å”¤å…½æœç´¢å‚æ•°ï¼ˆä½¿ç”¨equip_params_petä½œä¸ºåŸºç¡€ï¼‰
        pet_params = result.get('equip_params_pet', {})
        
        # æ„å»ºå‰ç«¯éœ€è¦çš„æ ¼å¼
        frontend_params = {
            "role": role_params,
            "equip_normal": result.get('equip_params_normal', {}),
            "equip_lingshi": result.get('equip_params_lingshi', {}),
            "equip_pet": result.get('equip_params_pet', {}),
            "equip_pet_equip": result.get('equip_params_pet_equip', {}),
            "pet": pet_params
        }
        
        return success_response(data=frontend_params, message="è·å–æœç´¢å‚æ•°é…ç½®æˆåŠŸ")
        
    except Exception as e:
        logger.error(f"è·å–æœç´¢å‚æ•°é…ç½®å¤±è´¥: {e}")
        return error_response(f"è·å–æœç´¢å‚æ•°é…ç½®å¤±è´¥: {str(e)}")


@system_bp.route('/config/search-params/<param_type>', methods=['GET'])
def get_search_param_by_type(param_type: str):
    """
    æ ¹æ®ç±»å‹è·å–ç‰¹å®šçš„æœç´¢å‚æ•°é…ç½®
    
    Args:
        param_type: å‚æ•°ç±»å‹ (role, equip_normal, equip_lingshi, equip_pet, equip_pet_equip, pet)
    
    Returns:
        JSON: åŒ…å«ç‰¹å®šæœç´¢å‚æ•°é…ç½®çš„å“åº”
    """
    try:
        config_dir = get_config_path()
        
        # å‚æ•°ç±»å‹åˆ°æ–‡ä»¶åçš„æ˜ å°„
        type_to_file = {
            'equip_normal': 'equip_params_normal.json',
            'equip_lingshi': 'equip_params_lingshi.json',
            'equip_pet': 'equip_params_pet.json',
            'equip_pet_equip': 'equip_params_pet_equip.json'
        }
        
        # ç‰¹æ®Šå¤„ç†è§’è‰²å’Œå¬å”¤å…½å‚æ•°
        if param_type == 'role':
            if os.path.exists(config_dir):
                try:
                    # è¯»å–109ç­‰çº§çš„é…ç½®ä½œä¸ºé»˜è®¤è§’è‰²é…ç½®
                    role_file = os.path.join(config_dir, '109.json')
                    if os.path.exists(role_file):
                        with open(role_file, 'r', encoding='utf-8') as f:
                            content = json.load(f)
                            return success_response(data=content, message=f"è·å–{param_type}å‚æ•°é…ç½®æˆåŠŸ")
                    else:
                        return error_response("è§’è‰²é…ç½®æ–‡ä»¶ä¸å­˜åœ¨")
                except Exception as e:
                    logger.error(f"è¯»å–è§’è‰²é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
                    return error_response(f"æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}")
            else:
                return error_response("è§’è‰²é…ç½®ç›®å½•ä¸å­˜åœ¨")
        elif param_type == 'pet':
            # å¬å”¤å…½å‚æ•°ä½¿ç”¨equip_params_pet
            filename = 'equip_params_pet.json'
            file_path = os.path.join(config_dir, filename)
        elif param_type not in type_to_file:
            return error_response(f"ä¸æ”¯æŒçš„å‚æ•°ç±»å‹: {param_type}")
        else:
            filename = type_to_file[param_type]
            file_path = os.path.join(config_dir, filename)
        
        if not os.path.exists(file_path):
            return error_response(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = json.load(f)
                return success_response(data=content, message=f"è·å–{param_type}å‚æ•°é…ç½®æˆåŠŸ")
        except json.JSONDecodeError as e:
            logger.error(f"è§£æé…ç½®æ–‡ä»¶ {filename} å¤±è´¥: {e}")
            return error_response(f"JSONè§£æå¤±è´¥: {str(e)}")
        except Exception as e:
            logger.error(f"è¯»å–é…ç½®æ–‡ä»¶ {filename} å¤±è´¥: {e}")
            return error_response(f"æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}")
        
    except Exception as e:
        logger.error(f"è·å–{param_type}å‚æ•°é…ç½®å¤±è´¥: {e}")
        return error_response(f"è·å–{param_type}å‚æ•°é…ç½®å¤±è´¥: {str(e)}")


@system_bp.route('/config/search-params/<param_type>', methods=['POST'])
def update_search_param(param_type: str):
    """
    æ›´æ–°ç‰¹å®šç±»å‹çš„æœç´¢å‚æ•°é…ç½®
    
    Args:
        param_type: å‚æ•°ç±»å‹
    
    Returns:
        JSON: æ›´æ–°ç»“æœ
    """
    try:
        config_dir = get_config_path()
        
        # å‚æ•°ç±»å‹åˆ°æ–‡ä»¶åçš„æ˜ å°„
        type_to_file = {
            'equip_normal': 'equip_params_normal.json',
            'equip_lingshi': 'equip_params_lingshi.json',
            'equip_pet': 'equip_params_pet.json',
            'equip_pet_equip': 'equip_params_pet_equip.json'
        }
        
        # è·å–è¯·æ±‚æ•°æ®
        data = request.get_json()
        if not data:
            return error_response("è¯·æä¾›è¦æ›´æ–°çš„å‚æ•°æ•°æ®")
        
        # ç‰¹æ®Šå¤„ç†è§’è‰²å’Œå¬å”¤å…½å‚æ•°
        if param_type == 'role':
            if not os.path.exists(config_dir):
                os.makedirs(config_dir, exist_ok=True)
            filename = '109.json'
            file_path = os.path.join(config_dir, filename)
        elif param_type == 'pet':
            # å¬å”¤å…½å‚æ•°ä½¿ç”¨equip_params_pet
            filename = 'equip_params_pet.json'
            file_path = os.path.join(config_dir, filename)
        elif param_type not in type_to_file:
            return error_response(f"ä¸æ”¯æŒçš„å‚æ•°ç±»å‹: {param_type}")
        else:
            filename = type_to_file[param_type]
            file_path = os.path.join(config_dir, filename)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(config_dir, exist_ok=True)
        
        # å†™å…¥æ–‡ä»¶
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æˆåŠŸæ›´æ–°{param_type}å‚æ•°é…ç½®")
            return success_response(message=f"æ›´æ–°{param_type}å‚æ•°é…ç½®æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"å†™å…¥é…ç½®æ–‡ä»¶ {filename} å¤±è´¥: {e}")
            return error_response(f"æ–‡ä»¶å†™å…¥å¤±è´¥: {str(e)}")
        
    except Exception as e:
        logger.error(f"æ›´æ–°{param_type}å‚æ•°é…ç½®å¤±è´¥: {e}")
        return error_response(f"æ›´æ–°{param_type}å‚æ•°é…ç½®å¤±è´¥: {str(e)}") 
    
@system_bp.route('/market-data/status', methods=['GET'])
def get_market_data_status():
    """è·å–å¸‚åœºæ•°æ®çŠ¶æ€"""
    try:
        from src.evaluator.market_data_collector import MarketDataCollector
        
        # è·å–å¸‚åœºæ•°æ®æ”¶é›†å™¨å®ä¾‹
        collector = MarketDataCollector()
        
        # è·å–MySQLæ•°æ®æ€»æ•°
        mysql_count = collector._get_empty_roles_count()
        
        # è·å–åŸºæœ¬çŠ¶æ€ä¿¡æ¯
        status_info = {
            "data_loaded": collector._data_loaded,
            "last_refresh_time": collector._last_refresh_time.isoformat() if collector._last_refresh_time else None,
            "cache_expiry_hours": collector._cache_expiry_hours,
            "data_count": len(collector.market_data) if not collector.market_data.empty else 0,
            "data_columns": list(collector.market_data.columns) if not collector.market_data.empty else [],
            "memory_usage_mb": collector.market_data.memory_usage(deep=True).sum() / 1024 / 1024 if not collector.market_data.empty else 0,
            "mysql_data_count": mysql_count
        }
        
        # æ·»åŠ åˆ·æ–°è¿›åº¦ä¿¡æ¯
        refresh_status = collector.get_refresh_status()
        status_info.update({
            "refresh_status": refresh_status["status"],
            "refresh_progress": refresh_status["progress"],
            "refresh_message": refresh_status["message"],
            "refresh_processed_records": refresh_status["processed_records"],
            "refresh_total_records": refresh_status["total_records"],
            "refresh_current_batch": refresh_status["current_batch"],
            "refresh_total_batches": refresh_status["total_batches"],
            "refresh_start_time": refresh_status["start_time"],
            "refresh_elapsed_seconds": refresh_status["elapsed_seconds"]
        })
        
        # å¦‚æœæœ‰æ•°æ®ï¼Œæ·»åŠ æ•°æ®ç»Ÿè®¡ä¿¡æ¯
        if not collector.market_data.empty:
            try:
                # ä»·æ ¼ç»Ÿè®¡
                price_stats = {
                    "min_price": float(collector.market_data['price'].min()),
                    "max_price": float(collector.market_data['price'].max()),
                    "avg_price": float(collector.market_data['price'].mean()),
                    "median_price": float(collector.market_data['price'].median())
                }
                status_info["price_statistics"] = price_stats
                
                # è§’è‰²ç±»å‹åˆ†å¸ƒ
                if 'role_type' in collector.market_data.columns:
                    role_type_counts = collector.market_data['role_type'].value_counts().to_dict()
                    status_info["role_type_distribution"] = role_type_counts
                
                # ç­‰çº§åˆ†å¸ƒï¼ˆå¦‚æœå­˜åœ¨levelå­—æ®µï¼‰
                level_fields = [col for col in collector.market_data.columns if 'level' in col.lower()]
                if level_fields:
                    level_field = level_fields[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„ç­‰çº§å­—æ®µ
                    level_stats = {
                        "min_level": int(collector.market_data[level_field].min()),
                        "max_level": int(collector.market_data[level_field].max()),
                        "avg_level": float(collector.market_data[level_field].mean())
                    }
                    status_info["level_statistics"] = level_stats
                    
            except Exception as e:
                logger.warning(f"è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {e}")
                status_info["statistics_error"] = str(e)
        
        # è®¡ç®—ç¼“å­˜æ˜¯å¦è¿‡æœŸ
        if collector._last_refresh_time:
            from datetime import datetime, timedelta
            cache_expiry_time = collector._last_refresh_time + timedelta(hours=collector._cache_expiry_hours)
            status_info["cache_expired"] = datetime.now() > cache_expiry_time
            status_info["cache_expiry_time"] = cache_expiry_time.isoformat()
        else:
            status_info["cache_expired"] = True
            status_info["cache_expiry_time"] = None
        
        # æ·»åŠ Flaskç¼“å­˜ä¿¡æ¯
        try:
            cache_info = collector.get_cache_info()
            status_info["flask_cache"] = cache_info
        except Exception as e:
            logger.warning(f"è·å–Flaskç¼“å­˜ä¿¡æ¯å¤±è´¥: {e}")
            status_info["flask_cache"] = {"available": False, "error": str(e)}
        
        return success_response(data=status_info, message="è·å–å¸‚åœºæ•°æ®çŠ¶æ€æˆåŠŸ")
        
    except Exception as e:
        logger.error(f"è·å–å¸‚åœºæ•°æ®çŠ¶æ€å¤±è´¥: {e}")
        return error_response(f"è·å–å¸‚åœºæ•°æ®çŠ¶æ€å¤±è´¥: {str(e)}")


@system_bp.route('/market-data/cache-status', methods=['GET'])
def get_cache_status():
    """è·å–ç¼“å­˜çŠ¶æ€"""
    try:
        from src.evaluator.market_data_collector import MarketDataCollector
        
        # è·å–å¸‚åœºæ•°æ®æ”¶é›†å™¨å®ä¾‹
        collector = MarketDataCollector()
        
        # è·å–ç¼“å­˜çŠ¶æ€
        cache_status = collector.get_cache_status()
        
        return success_response(data=cache_status, message="è·å–ç¼“å­˜çŠ¶æ€æˆåŠŸ")
        
    except Exception as e:
        logger.error(f"è·å–ç¼“å­˜çŠ¶æ€å¤±è´¥: {e}")
        return error_response(f"è·å–ç¼“å­˜çŠ¶æ€å¤±è´¥: {str(e)}")


@system_bp.route('/market-data/equipment/cache-status', methods=['GET'])
def get_equipment_cache_status():
    """è·å–è£…å¤‡ç¼“å­˜çŠ¶æ€"""
    try:
        from src.evaluator.market_anchor.equip.equip_market_data_collector import EquipMarketDataCollector
        
        # ä½¿ç”¨ç±»æ–¹æ³•è·å–å•ä¾‹å®ä¾‹çš„ç¼“å­˜çŠ¶æ€
        cache_status = EquipMarketDataCollector.get_cache_status_static()
        
        return success_response(data=cache_status, message="è·å–è£…å¤‡ç¼“å­˜çŠ¶æ€æˆåŠŸ")
        
    except Exception as e:
        logger.error(f"è·å–è£…å¤‡ç¼“å­˜çŠ¶æ€å¤±è´¥: {e}")
        return error_response(f"è·å–è£…å¤‡ç¼“å­˜çŠ¶æ€å¤±è´¥: {str(e)}")


@system_bp.route('/market-data/equipment/refresh', methods=['POST'])
def refresh_equipment_data():
    """å¯åŠ¨è£…å¤‡æ•°æ®åˆ·æ–°"""
    try:
        from src.evaluator.market_anchor.equip.equip_market_data_collector import EquipMarketDataCollector
        import threading
        
        # è·å–è¯·æ±‚å‚æ•°
        data = request.get_json() or {}
        
        # è·å–è£…å¤‡æ•°æ®é‡‡é›†å™¨å®ä¾‹ï¼ˆä½¿ç”¨å•ä¾‹æ¨¡å¼ï¼‰
        collector = EquipMarketDataCollector.get_instance()
        
        # æ£€æŸ¥æ˜¯å¦æ­£åœ¨åˆ·æ–°
        if collector._refresh_status == "running":
            return error_response("è£…å¤‡æ•°æ®åˆ·æ–°æ­£åœ¨è¿›è¡Œä¸­ï¼Œè¯·ç­‰å¾…å®Œæˆåå†è¯•")
        
        # è®¾ç½®åˆ·æ–°å‚æ•°
        use_cache = data.get('use_cache', True)
        force_refresh = data.get('force_refresh', False)
        
        # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œåˆ·æ–°
        def background_refresh():
            try:
                if force_refresh:
                    # å¼ºåˆ¶åˆ·æ–°ï¼Œå®Œå…¨é‡æ–°åŠ è½½
                    collector.refresh_full_cache()
                else:
                    # ä½¿ç”¨ç¼“å­˜ï¼Œå¦‚æœç¼“å­˜ä¸å­˜åœ¨åˆ™åŠ è½½
                    collector._load_full_data_to_redis(force_refresh=False)
            except Exception as e:
                logger.error(f"åå°åˆ·æ–°è£…å¤‡æ•°æ®å¤±è´¥: {e}")
                collector._refresh_status = "error"
                collector._refresh_message = f"åˆ·æ–°å¤±è´¥: {str(e)}"
        
        # å¯åŠ¨åå°çº¿ç¨‹
        refresh_thread = threading.Thread(target=background_refresh)
        refresh_thread.daemon = True
        refresh_thread.start()
        
        # ç«‹å³è¿”å›å¯åŠ¨æˆåŠŸçš„å“åº”
        return success_response(data={
            "refresh_started": True,
            "message": "è£…å¤‡æ•°æ®åˆ·æ–°å·²å¯åŠ¨ï¼Œè¯·ä½¿ç”¨çŠ¶æ€æ¥å£æŸ¥è¯¢è¿›åº¦",
            "force_refresh": force_refresh,
            "use_cache": use_cache
        }, message="è£…å¤‡æ•°æ®åˆ·æ–°å·²å¯åŠ¨")
        
    except Exception as e:
        logger.error(f"å¯åŠ¨è£…å¤‡æ•°æ®åˆ·æ–°å¤±è´¥: {e}")
        return error_response(f"å¯åŠ¨è£…å¤‡æ•°æ®åˆ·æ–°å¤±è´¥: {str(e)}")

 
@system_bp.route('/market-data/equipment/status', methods=['GET'])
def get_equipment_market_data_status():
    """è·å–è£…å¤‡å¸‚åœºæ•°æ®çŠ¶æ€"""
    try:
        from src.evaluator.market_anchor.equip.equip_market_data_collector import EquipMarketDataCollector
        
        import time
        
        # è·å–è£…å¤‡æ•°æ®é‡‡é›†å™¨å®ä¾‹ï¼ˆä½¿ç”¨å•ä¾‹æ¨¡å¼ï¼‰
        start_time = time.time()
        collector = EquipMarketDataCollector.get_instance()
        instance_time = (time.time() - start_time) * 1000
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        print(f"ğŸ” è£…å¤‡å¸‚åœºæ•°æ®çŠ¶æ€ - å®ä¾‹ID: {id(collector)} (è€—æ—¶: {instance_time:.2f}ms)")
        print(f"ğŸ” å†…å­˜ç¼“å­˜çŠ¶æ€: {collector._full_data_cache is not None and not collector._full_data_cache.empty if collector._full_data_cache is not None else False}")
        print(f"ğŸ” åˆ·æ–°çŠ¶æ€: {collector._refresh_status}")
        
        # è·å–MySQLè£…å¤‡æ•°æ®æ€»æ•°
        mysql_start = time.time()
        mysql_count = collector._get_mysql_equipments_count()
        mysql_time = (time.time() - mysql_start) * 1000
        print(f"ğŸ” MySQLæŸ¥è¯¢è€—æ—¶: {mysql_time:.2f}ms")
        
        # è·å–Redisæ•°æ®æ€»æ•°ï¼ˆä»Hashå…ƒæ•°æ®è·å–ï¼Œä¸åŠ è½½å®é™…æ•°æ®ï¼‰
        redis_count = 0
        redis_start = time.time()
        try:
            if collector.redis_cache and collector.redis_cache.is_available():
                # ä»Hashç»“æ„å…ƒæ•°æ®è·å–æ•°æ®æ€»æ•°
                meta_key = f"{collector._full_cache_key}:meta"
                print(f"ğŸ” å°è¯•è·å–Rediså…ƒæ•°æ®é”®: {meta_key}")
                
                # å…ˆæ£€æŸ¥é”®æ˜¯å¦å­˜åœ¨
                full_meta_key = collector.redis_cache._make_key(meta_key)
                print(f"ğŸ” å®Œæ•´å…ƒæ•°æ®é”®: {full_meta_key}")
                
                # æ£€æŸ¥é”®æ˜¯å¦å­˜åœ¨
                key_exists = collector.redis_cache.client.exists(full_meta_key)
                print(f"ğŸ” å…ƒæ•°æ®é”®æ˜¯å¦å­˜åœ¨: {key_exists}")
                
                if key_exists:
                    # å…ƒæ•°æ®æ˜¯ä½œä¸ºæ™®é€šå­—ç¬¦ä¸²å­˜å‚¨çš„ï¼Œéœ€è¦ååºåˆ—åŒ–
                    import pickle
                    try:
                        metadata_bytes = collector.redis_cache.client.get(full_meta_key)
                        if metadata_bytes:
                            hash_metadata = pickle.loads(metadata_bytes)
                            print(f"ğŸ” Rediså…ƒæ•°æ®å†…å®¹: {hash_metadata}")
                            if hash_metadata and 'total_count' in hash_metadata:
                                redis_count = hash_metadata.get('total_count', 0)
                                print(f"ğŸ” Redisç¼“å­˜æ•°æ®é‡: {redis_count} æ¡")
                            else:
                                print("ğŸ” Rediså…ƒæ•°æ®ç¼ºå°‘total_countå­—æ®µ")
                        else:
                            print("ğŸ” Rediså…ƒæ•°æ®ä¸ºç©º")
                    except Exception as e:
                        print(f"ğŸ” ååºåˆ—åŒ–Rediså…ƒæ•°æ®å¤±è´¥: {e}")
                        # å°è¯•ä½œä¸ºHashç»“æ„è·å–ï¼ˆå‘åå…¼å®¹ï¼‰
                        hash_metadata = collector.redis_cache.get(meta_key)
                        print(f"ğŸ” Rediså…ƒæ•°æ®å†…å®¹(Hash): {hash_metadata}")
                        if hash_metadata and 'total_count' in hash_metadata:
                            redis_count = hash_metadata.get('total_count', 0)
                            print(f"ğŸ” Redisç¼“å­˜æ•°æ®é‡: {redis_count} æ¡")
                        else:
                            print("ğŸ” Redis Hashå…ƒæ•°æ®ç¼ºå°‘total_countå­—æ®µ")
                else:
                    print("ğŸ” Redis Hashå…ƒæ•°æ®é”®ä¸å­˜åœ¨")
                    
                    # é¿å…è°ƒç”¨hlen()ï¼Œå› ä¸ºè¿™ä¼šè§¦å‘Redisæ‰«æï¼Œå½±å“æ€§èƒ½
                    # å¦‚æœå…ƒæ•°æ®ä¸å­˜åœ¨ï¼Œè¯´æ˜å¯èƒ½æ²¡æœ‰æ•°æ®æˆ–è€…æ•°æ®æ­£åœ¨åŠ è½½ä¸­
                    print("ğŸ” å…ƒæ•°æ®ä¸å­˜åœ¨ï¼Œè·³è¿‡Redisæ•°æ®é‡æŸ¥è¯¢ä»¥é¿å…æ€§èƒ½å½±å“")
                    redis_count = 0
            else:
                print("ğŸ” Redisä¸å¯ç”¨")
        except Exception as e:
            print(f"ğŸ” è·å–Redisæ•°æ®æ€»æ•°å¤±è´¥: {e}")
        finally:
            redis_time = (time.time() - redis_start) * 1000
            print(f"ğŸ” RedisæŸ¥è¯¢è€—æ—¶: {redis_time:.2f}ms")
        
        # è·å–åŸºæœ¬çŠ¶æ€ä¿¡æ¯
        status_info = {
            "data_loaded": collector._full_data_cache is not None and not collector._full_data_cache.empty,
            "last_refresh_time": collector._refresh_start_time.isoformat() if collector._refresh_start_time else None,
            "cache_ttl_hours": collector._cache_ttl_hours,
            "data_count": len(collector._full_data_cache) if collector._full_data_cache is not None and not collector._full_data_cache.empty else 0,
            "data_columns": list(collector._full_data_cache.columns) if collector._full_data_cache is not None and not collector._full_data_cache.empty else [],
            "memory_usage_mb": collector._full_data_cache.memory_usage(deep=True).sum() / 1024 / 1024 if collector._full_data_cache is not None and not collector._full_data_cache.empty else 0,
            "mysql_data_count": mysql_count,
            "redis_data_count": redis_count
        }
        
        # æ·»åŠ åˆ·æ–°è¿›åº¦ä¿¡æ¯
        refresh_status = collector.get_refresh_status()
        status_info.update({
            "refresh_status": refresh_status["status"],
            "refresh_progress": refresh_status["progress"],
            "refresh_message": refresh_status["message"],
            "refresh_processed_records": refresh_status["processed_records"],
            "refresh_total_records": refresh_status["total_records"],
            "refresh_current_batch": refresh_status["current_batch"],
            "refresh_total_batches": refresh_status["total_batches"],
            "refresh_start_time": refresh_status["start_time"],
            "refresh_elapsed_seconds": refresh_status["elapsed_seconds"]
        })
        
        # å¦‚æœæœ‰æ•°æ®ï¼Œæ·»åŠ æ•°æ®ç»Ÿè®¡ä¿¡æ¯
        if collector._full_data_cache is not None and not collector._full_data_cache.empty:
            try:
                # ä»·æ ¼ç»Ÿè®¡
                if 'price' in collector._full_data_cache.columns:
                    price_stats = {
                        "min_price": float(collector._full_data_cache['price'].min()),
                        "max_price": float(collector._full_data_cache['price'].max()),
                        "avg_price": float(collector._full_data_cache['price'].mean()),
                        "median_price": float(collector._full_data_cache['price'].median())
                    }
                    status_info["price_statistics"] = price_stats
                
                # è£…å¤‡ç±»å‹åˆ†å¸ƒ
                if 'kindid' in collector._full_data_cache.columns:
                    kindid_counts = collector._full_data_cache['kindid'].value_counts().to_dict()
                    status_info["kindid_distribution"] = kindid_counts
                
                # ç­‰çº§åˆ†å¸ƒ
                if 'equip_level' in collector._full_data_cache.columns:
                    level_stats = {
                        "min_level": int(collector._full_data_cache['equip_level'].min()),
                        "max_level": int(collector._full_data_cache['equip_level'].max()),
                        "avg_level": float(collector._full_data_cache['equip_level'].mean())
                    }
                    status_info["level_statistics"] = level_stats
                    
            except Exception as e:
                logger.warning(f"è·å–è£…å¤‡æ•°æ®ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {e}")
                status_info["statistics_error"] = str(e)
        
        # è®¡ç®—ç¼“å­˜æ˜¯å¦è¿‡æœŸ
        if collector._refresh_start_time:
            from datetime import datetime, timedelta
            if collector._cache_ttl_hours == -1:
                status_info["cache_expired"] = False
                status_info["cache_expiry_time"] = None
            else:
                cache_expiry_time = collector._refresh_start_time + timedelta(hours=collector._cache_ttl_hours)
                status_info["cache_expired"] = datetime.now() > cache_expiry_time
                status_info["cache_expiry_time"] = cache_expiry_time.isoformat()
        else:
            status_info["cache_expired"] = True
            status_info["cache_expiry_time"] = None
        
        # æ·»åŠ Rediså…¨é‡ç¼“å­˜çŠ¶æ€ä¿¡æ¯
        try:
            redis_cache_status = collector.get_cache_status()
            status_info["redis_full_cache"] = redis_cache_status
        except Exception as e:
            logger.warning(f"è·å–Rediså…¨é‡ç¼“å­˜çŠ¶æ€å¤±è´¥: {e}")
            status_info["redis_full_cache"] = {"available": False, "error": str(e)}
        
        return success_response(data=status_info, message="è·å–è£…å¤‡å¸‚åœºæ•°æ®çŠ¶æ€æˆåŠŸ")
        
    except Exception as e:
        logger.error(f"è·å–è£…å¤‡å¸‚åœºæ•°æ®çŠ¶æ€å¤±è´¥: {e}")
        return error_response(f"è·å–è£…å¤‡å¸‚åœºæ•°æ®çŠ¶æ€å¤±è´¥: {str(e)}")


@system_bp.route('/market-data/pet/status', methods=['GET'])
def get_pet_market_data_status():
    """è·å–å¬å”¤å…½å¸‚åœºæ•°æ®çŠ¶æ€"""
    try:
        from src.evaluator.market_anchor.pet.pet_market_data_collector import PetMarketDataCollector
        
        # è·å–å¬å”¤å…½æ•°æ®é‡‡é›†å™¨å®ä¾‹
        collector = PetMarketDataCollector()
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        print(f"ğŸ” å¬å”¤å…½å¸‚åœºæ•°æ®çŠ¶æ€ - å®ä¾‹ID: {id(collector)}")
        print(f"ğŸ” å†…å­˜ç¼“å­˜çŠ¶æ€: {collector._full_data_cache is not None and not collector._full_data_cache.empty if collector._full_data_cache is not None else False}")
        print(f"ğŸ” åˆ·æ–°çŠ¶æ€: {collector._refresh_status}")
        
        # è·å–MySQLå¬å”¤å…½æ•°æ®æ€»æ•°
        mysql_count = collector._get_mysql_pets_count()
        
        # è·å–åŸºæœ¬çŠ¶æ€ä¿¡æ¯
        status_info = {
            "data_loaded": collector._full_data_cache is not None and not collector._full_data_cache.empty,
            "last_refresh_time": collector._refresh_start_time.isoformat() if collector._refresh_start_time else None,
            "cache_ttl_hours": collector._cache_ttl_hours,
            "data_count": len(collector._full_data_cache) if collector._full_data_cache is not None and not collector._full_data_cache.empty else 0,
            "data_columns": list(collector._full_data_cache.columns) if collector._full_data_cache is not None and not collector._full_data_cache.empty else [],
            "memory_usage_mb": collector._full_data_cache.memory_usage(deep=True).sum() / 1024 / 1024 if collector._full_data_cache is not None and not collector._full_data_cache.empty else 0,
            "mysql_data_count": mysql_count
        }
        
        # æ·»åŠ åˆ·æ–°è¿›åº¦ä¿¡æ¯
        refresh_status = collector.get_refresh_status()
        status_info.update({
            "refresh_status": refresh_status["status"],
            "refresh_progress": refresh_status["progress"],
            "refresh_message": refresh_status["message"],
            "refresh_processed_records": refresh_status["processed_records"],
            "refresh_total_records": refresh_status["total_records"],
            "refresh_current_batch": refresh_status["current_batch"],
            "refresh_total_batches": refresh_status["total_batches"],
            "refresh_start_time": refresh_status["start_time"],
            "refresh_elapsed_seconds": refresh_status["elapsed_seconds"]
        })
        
        # å¦‚æœæœ‰æ•°æ®ï¼Œæ·»åŠ æ•°æ®ç»Ÿè®¡ä¿¡æ¯
        if collector._full_data_cache is not None and not collector._full_data_cache.empty:
            try:
                df = collector._full_data_cache
                
                # ä»·æ ¼ç»Ÿè®¡
                if 'price' in df.columns:
                    price_stats = {
                        "min_price": float(df['price'].min()),
                        "max_price": float(df['price'].max()),
                        "avg_price": float(df['price'].mean()),
                        "median_price": float(df['price'].median())
                    }
                    status_info["price_statistics"] = price_stats
                
                # ç­‰çº§ç»Ÿè®¡
                if 'equip_level' in df.columns:
                    level_stats = {
                        "min_level": int(df['equip_level'].min()),
                        "max_level": int(df['equip_level'].max())
                    }
                    status_info["level_statistics"] = level_stats
                
                # æºå¸¦ç­‰çº§ç»Ÿè®¡
                if 'role_grade_limit' in df.columns:
                    role_grade_limit_stats = {
                        "min_role_grade_limit": int(df['role_grade_limit'].min()),
                        "max_role_grade_limit": int(df['role_grade_limit'].max())
                    }
                    status_info["role_grade_limit_statistics"] = role_grade_limit_stats
                
                # æŠ€èƒ½åˆ†å¸ƒç»Ÿè®¡
                if 'all_skill' in df.columns:
                    # ç»Ÿè®¡æ‰€æœ‰æŠ€èƒ½çš„å‡ºç°æ¬¡æ•°
                    all_skills = []
                    for skills_str in df['all_skill'].dropna():
                        if skills_str and str(skills_str).strip():
                            skills = str(skills_str).split('|')
                            all_skills.extend([s.strip() for s in skills if s.strip()])
                    
                    from collections import Counter
                    skill_counts = Counter(all_skills)
                    status_info["skill_distribution"] = dict(skill_counts.most_common(20))  # å–å‰20ä¸ªæœ€å¸¸è§çš„æŠ€èƒ½
                
            except Exception as e:
                logger.warning(f"è®¡ç®—å¬å”¤å…½æ•°æ®ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
                status_info["statistics_error"] = str(e)
        
        # è®¡ç®—ç¼“å­˜æ˜¯å¦è¿‡æœŸ
        if collector._refresh_start_time:
            from datetime import datetime, timedelta
            if collector._cache_ttl_hours == -1:
                status_info["cache_expired"] = False
                status_info["cache_expiry_time"] = None
            else:
                cache_expiry_time = collector._refresh_start_time + timedelta(hours=collector._cache_ttl_hours)
                status_info["cache_expired"] = datetime.now() > cache_expiry_time
                status_info["cache_expiry_time"] = cache_expiry_time.isoformat()
        else:
            status_info["cache_expired"] = True
            status_info["cache_expiry_time"] = None
        
        # æ·»åŠ Rediså…¨é‡ç¼“å­˜çŠ¶æ€ä¿¡æ¯
        try:
            redis_cache_status = collector.get_cache_status()
            status_info["redis_full_cache"] = redis_cache_status
        except Exception as e:
            logger.warning(f"è·å–Rediså…¨é‡ç¼“å­˜çŠ¶æ€å¤±è´¥: {e}")
            status_info["redis_full_cache"] = {"available": False, "error": str(e)}
        
        return success_response(data=status_info, message="è·å–å¬å”¤å…½å¸‚åœºæ•°æ®çŠ¶æ€æˆåŠŸ")
        
    except Exception as e:
        logger.error(f"è·å–å¬å”¤å…½å¸‚åœºæ•°æ®çŠ¶æ€å¤±è´¥: {e}")
        return error_response(f"è·å–å¬å”¤å…½å¸‚åœºæ•°æ®çŠ¶æ€å¤±è´¥: {str(e)}")


@system_bp.route('/market-data/pet/refresh', methods=['POST'])
def refresh_pet_data():
    """åˆ·æ–°å¬å”¤å…½æ•°æ®"""
    try:
        from src.evaluator.market_anchor.pet.pet_market_data_collector import PetMarketDataCollector
        
        # è·å–å¬å”¤å…½æ•°æ®é‡‡é›†å™¨å®ä¾‹
        collector = PetMarketDataCollector()
        
        # å¯åŠ¨åå°æ•°æ®åˆ·æ–°
        import threading
        
        def refresh_task():
            try:
                # åŠ è½½å¬å”¤å…½æ•°æ®åˆ°Redisç¼“å­˜
                collector._load_full_data_to_redis()
            except Exception as e:
                logger.error(f"å¬å”¤å…½æ•°æ®åˆ·æ–°ä»»åŠ¡å¤±è´¥: {e}")
        
        # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œåˆ·æ–°ä»»åŠ¡
        thread = threading.Thread(target=refresh_task)
        thread.daemon = True
        thread.start()
        
        return success_response(message="å¬å”¤å…½æ•°æ®åˆ·æ–°å·²å¯åŠ¨ï¼Œæ­£åœ¨åå°å¤„ç†...")
        
    except Exception as e:
        logger.error(f"å¯åŠ¨å¬å”¤å…½æ•°æ®åˆ·æ–°å¤±è´¥: {e}")
        return error_response(f"å¯åŠ¨å¬å”¤å…½æ•°æ®åˆ·æ–°å¤±è´¥: {str(e)}")


@system_bp.route('/market-data/pet/refresh-full-cache', methods=['POST'])
def refresh_pet_full_cache():
    """åˆ·æ–°å¬å”¤å…½å…¨é‡ç¼“å­˜"""
    try:
        from src.evaluator.market_anchor.pet.pet_market_data_collector import PetMarketDataCollector
        
        # è·å–å¬å”¤å…½æ•°æ®é‡‡é›†å™¨å®ä¾‹
        collector = PetMarketDataCollector()
        
        # å¯åŠ¨åå°å…¨é‡ç¼“å­˜åˆ·æ–°
        import threading
        
        def refresh_task():
            try:
                # å¼ºåˆ¶åˆ·æ–°å…¨é‡ç¼“å­˜
                collector.refresh_full_cache()
            except Exception as e:
                logger.error(f"å¬å”¤å…½å…¨é‡ç¼“å­˜åˆ·æ–°ä»»åŠ¡å¤±è´¥: {e}")
        
        # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œåˆ·æ–°ä»»åŠ¡
        thread = threading.Thread(target=refresh_task)
        thread.daemon = True
        thread.start()
        
        return success_response(message="å¬å”¤å…½å…¨é‡ç¼“å­˜åˆ·æ–°å·²å¯åŠ¨ï¼Œæ­£åœ¨åå°å¤„ç†...")
        
    except Exception as e:
        logger.error(f"å¯åŠ¨å¬å”¤å…½å…¨é‡ç¼“å­˜åˆ·æ–°å¤±è´¥: {e}")
        return error_response(f"å¯åŠ¨å¬å”¤å…½å…¨é‡ç¼“å­˜åˆ·æ–°å¤±è´¥: {str(e)}")


@system_bp.route('/market-data/pet/refresh-status', methods=['GET'])
def get_pet_refresh_status():
    """è·å–å¬å”¤å…½æ•°æ®åˆ·æ–°çŠ¶æ€"""
    try:
        from src.evaluator.market_anchor.pet.pet_market_data_collector import PetMarketDataCollector
        
        # è·å–å¬å”¤å…½æ•°æ®é‡‡é›†å™¨å®ä¾‹
        collector = PetMarketDataCollector()
        
        # è·å–åˆ·æ–°çŠ¶æ€
        refresh_status = collector.get_refresh_status()
        
        return success_response(data=refresh_status, message="è·å–å¬å”¤å…½åˆ·æ–°çŠ¶æ€æˆåŠŸ")
        
    except Exception as e:
        logger.error(f"è·å–å¬å”¤å…½åˆ·æ–°çŠ¶æ€å¤±è´¥: {e}")
        return error_response(f"è·å–å¬å”¤å…½åˆ·æ–°çŠ¶æ€å¤±è´¥: {str(e)}")


@system_bp.route('/redis/status', methods=['GET'])
def get_redis_status():
    """è·å–RedisçŠ¶æ€ä¿¡æ¯"""
    try:
        from src.utils.redis_cache import get_redis_cache
        
        redis_cache = get_redis_cache()
        if not redis_cache:
            return error_response("Redisç¼“å­˜æœªåˆå§‹åŒ–")
        
        # åŸºæœ¬è¿æ¥ä¿¡æ¯
        status_info = {
            "available": redis_cache.is_available(),
            "host": redis_cache.host,
            "port": redis_cache.port,
            "db": redis_cache.db,
            "connection_pool_size": redis_cache.pool.max_connections if hasattr(redis_cache, 'pool') else 0
        }
        
        if not status_info["available"]:
            status_info["error"] = "Redisè¿æ¥ä¸å¯ç”¨"
            return success_response(data=status_info, message="RedisçŠ¶æ€è·å–æˆåŠŸ")
        
        try:
            # è·å–RedisæœåŠ¡å™¨ä¿¡æ¯
            redis_info = redis_cache.get_redis_info()
            if redis_info:
                status_info.update({
                    "redis_version": redis_info.get('redis_version'),
                    "used_memory_human": redis_info.get('used_memory_human'),
                    "used_memory_peak_human": redis_info.get('used_memory_peak_human'),
                    "connected_clients": redis_info.get('connected_clients'),
                    "total_commands_processed": redis_info.get('total_commands_processed'),
                    "keyspace_hits": redis_info.get('keyspace_hits'),
                    "keyspace_misses": redis_info.get('keyspace_misses'),
                    "uptime_in_seconds": redis_info.get('uptime_in_seconds'),
                    "cache_keys_count": redis_info.get('db0', {}).get('keys', 0)
                })
                
                # è®¡ç®—å‘½ä¸­ç‡
                hits = redis_info.get('keyspace_hits', 0)
                misses = redis_info.get('keyspace_misses', 0)
                total = hits + misses
                if total > 0:
                    status_info["hit_rate"] = round((hits / total) * 100, 2)
                else:
                    status_info["hit_rate"] = 0
        except Exception as e:
            logger.warning(f"è·å–Redisè¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}")
            status_info["info_error"] = str(e)
        
        # è·å–ç¼“å­˜ç±»å‹ç»Ÿè®¡
        try:
            cache_types = redis_cache.get_cache_types()
            if cache_types:
                status_info["cache_types"] = cache_types
        except Exception as e:
            logger.warning(f"è·å–ç¼“å­˜ç±»å‹ç»Ÿè®¡å¤±è´¥: {e}")
            status_info["cache_types_error"] = str(e)
        
        return success_response(data=status_info, message="RedisçŠ¶æ€è·å–æˆåŠŸ")
        
    except Exception as e:
        logger.error(f"è·å–RedisçŠ¶æ€å¤±è´¥: {e}")
        return error_response(f"è·å–RedisçŠ¶æ€å¤±è´¥: {str(e)}")


@system_bp.route('/market-data/equipment/refresh-status', methods=['GET'])
def get_equipment_refresh_status():
    """è·å–è£…å¤‡æ•°æ®åˆ·æ–°è¿›åº¦çŠ¶æ€"""
    try:
        from src.evaluator.market_anchor.equip.equip_market_data_collector import EquipMarketDataCollector
        
        # ä½¿ç”¨ç±»æ–¹æ³•è·å–å•ä¾‹å®ä¾‹çš„åˆ·æ–°çŠ¶æ€
        refresh_status = EquipMarketDataCollector.get_refresh_status_static()
        
        return success_response(data=refresh_status, message="è·å–è£…å¤‡åˆ·æ–°çŠ¶æ€æˆåŠŸ")
        
    except Exception as e:
        logger.error(f"è·å–è£…å¤‡åˆ·æ–°çŠ¶æ€å¤±è´¥: {e}")
        return error_response(f"è·å–è£…å¤‡åˆ·æ–°çŠ¶æ€å¤±è´¥: {str(e)}")


@system_bp.route('/market-data/analysis', methods=['GET'])
def get_market_data_analysis():
    """è·å–å¸‚åœºæ•°æ®è¯¦ç»†åˆ†æ"""
    try:
        from src.evaluator.market_data_collector import MarketDataCollector
        import pandas as pd
        import numpy as np
        
        # è·å–å¸‚åœºæ•°æ®æ”¶é›†å™¨å®ä¾‹
        collector = MarketDataCollector()
        
        if collector.market_data.empty:
            return error_response("æš‚æ— å¸‚åœºæ•°æ®ï¼Œè¯·å…ˆåˆ·æ–°æ•°æ®")
        
        df = collector.market_data.copy()
        
        analysis_data = {}
        
        # 1. ç­‰çº§åˆ†å¸ƒåˆ†æ
        if 'level' in df.columns:
            level_bins = [109, 120, 130, 140, 150, 160, 175]
            level_labels = ['109-119', '120-129', '130-139', '140-149', '150-159', '160-175']
            df['level_range'] = pd.cut(df['level'], bins=level_bins, labels=level_labels, include_lowest=True)
            level_dist = df['level_range'].value_counts().sort_index()
            
            analysis_data['level_distribution'] = {
                'categories': level_dist.index.tolist(),
                'values': level_dist.values.tolist()
            }
        
        # 2. ä»·æ ¼åˆ†å¸ƒåˆ†æ
        if 'price' in df.columns:
            price_bins = [0, 1000, 5000, 10000, 20000, 50000, float('inf')]
            price_labels = ['<1000', '1000-5000', '5000-10000', '10000-20000', '20000-50000', '>50000']
            df['price_range'] = pd.cut(df['price'], bins=price_bins, labels=price_labels, include_lowest=True)
            price_dist = df['price_range'].value_counts().sort_index()
            
            analysis_data['price_distribution'] = {
                'categories': price_dist.index.tolist(),
                'values': price_dist.values.tolist()
            }
        
        # 3. ç­‰çº§ä¸ä»·æ ¼å…³ç³»ï¼ˆæ•£ç‚¹æ•°æ®ï¼‰
        if 'level' in df.columns and 'price' in df.columns:
            # é‡‡æ ·æ•°æ®ï¼Œé¿å…è¿”å›è¿‡å¤šç‚¹
            sample_size = min(500, len(df))
            sample_df = df.sample(n=sample_size)
            scatter_data = sample_df[['level', 'price']].values.tolist()
            
            analysis_data['level_price_relation'] = {
                'data': scatter_data
            }
            
            # è®¡ç®—ç­‰çº§ä»·æ ¼è¶‹åŠ¿
            level_groups = df.groupby(pd.cut(df['level'], bins=8))['price'].agg(['mean', 'count']).reset_index()
            level_groups['level_mid'] = level_groups['level'].apply(lambda x: x.mid)
            
            analysis_data['price_trend'] = {
                'levels': level_groups['level_mid'].round().astype(int).tolist(),
                'avg_prices': level_groups['mean'].round().tolist(),
                'counts': level_groups['count'].tolist()
            }
        
        # 4. é—¨æ´¾åˆ†å¸ƒåˆ†æ

        if 'school' in df.columns:
            school_dist = df['school'].value_counts()
            school_data = []
            for school_id, count in school_dist.items():
                school_data.append({'name': school_id, 'value': int(count)})
            
            analysis_data['school_distribution'] = school_data
        
        # 5. æœåŠ¡å™¨åˆ†å¸ƒåˆ†æï¼ˆTOP 10ï¼‰
        if 'serverid' in df.columns:
            server_dist = df['serverid'].value_counts().head(10)
            analysis_data['server_distribution'] = {
                'server_ids': server_dist.index.tolist(),
                'counts': server_dist.values.tolist()
            }
        
        # 6. åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
        analysis_data['basic_stats'] = {
            'total_count': len(df),
            'avg_price': float(df['price'].mean()) if 'price' in df.columns else 0,
            'median_price': float(df['price'].median()) if 'price' in df.columns else 0,
            'price_std': float(df['price'].std()) if 'price' in df.columns else 0,
            'avg_level': float(df['level'].mean()) if 'level' in df.columns else 0,
            'level_std': float(df['level'].std()) if 'level' in df.columns else 0
        }
        
        return success_response(data=analysis_data, message="è·å–å¸‚åœºæ•°æ®åˆ†ææˆåŠŸ")
        
    except Exception as e:
        logger.error(f"è·å–å¸‚åœºæ•°æ®åˆ†æå¤±è´¥: {e}")
        return error_response(f"è·å–å¸‚åœºæ•°æ®åˆ†æå¤±è´¥: {str(e)}")


@system_bp.route('/market-data/refresh', methods=['POST'])
def refresh_market_data():
    """å¯åŠ¨åˆ†æ‰¹åˆ·æ–°å¸‚åœºæ•°æ®"""
    try:
        from src.evaluator.market_data_collector import MarketDataCollector
        import threading
        
        # è·å–è¯·æ±‚å‚æ•°
        data = request.get_json() or {}
        
        # è·å–å¸‚åœºæ•°æ®æ”¶é›†å™¨å®ä¾‹
        collector = MarketDataCollector()
        
        # æ£€æŸ¥æ˜¯å¦æ­£åœ¨åˆ·æ–°
        if collector._refresh_status == "running":
            return error_response("æ•°æ®åˆ·æ–°æ­£åœ¨è¿›è¡Œä¸­ï¼Œè¯·ç­‰å¾…å®Œæˆåå†è¯•")
        
        # è®¾ç½®åˆ·æ–°å‚æ•°
        filters = data.get('filters', None)
        use_cache = data.get('use_cache', True)
        force_refresh = data.get('force_refresh', False)
        # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œåˆ·æ–°
        def background_refresh():
            try:
                collector.refresh_market_data(
                    filters=filters,
                    use_cache=use_cache,
                    force_refresh=force_refresh,
                )
            except Exception as e:
                logger.error(f"åå°åˆ·æ–°å¤±è´¥: {e}")
                collector._refresh_status = "error"
                collector._refresh_message = f"åˆ·æ–°å¤±è´¥: {str(e)}"
        
        # å¯åŠ¨åå°çº¿ç¨‹
        refresh_thread = threading.Thread(target=background_refresh)
        refresh_thread.daemon = True
        refresh_thread.start()
        
        # ç«‹å³è¿”å›å¯åŠ¨æˆåŠŸçš„å“åº”
        return success_response(data={
            "refresh_started": True,
            "message": "æ•°æ®åˆ·æ–°å·²å¯åŠ¨ï¼Œè¯·ä½¿ç”¨çŠ¶æ€æ¥å£æŸ¥è¯¢è¿›åº¦",
            "filters_applied": filters
        }, message="æ•°æ®åˆ·æ–°å·²å¯åŠ¨")
        
    except Exception as e:
        logger.error(f"å¯åŠ¨åˆ·æ–°å¤±è´¥: {e}")
        return error_response(f"å¯åŠ¨åˆ·æ–°å¤±è´¥: {str(e)}")


@system_bp.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥"""
    return success_response(data={"status": "healthy"}, message="è§’è‰²APIæœåŠ¡æ­£å¸¸è¿è¡Œ")